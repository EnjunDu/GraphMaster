[
  {
    "node_id": 0,
    "label": 1,
    "text": "Decomposition in Data Mining: An Industrial Case Study Data mining offers tools for discovery of relationships, patterns, and knowledge in large databases. The knowledge extraction process is computationally complex and therefore a subset of all data is normally considered for mining. In this paper, numerous methods for decomposition of data sets are discussed. Decomposition enhances the quality of knowledge extracted from large databases by simplification of the data mining task. The ideas presented are illustrated with examples and an industrial case study. In the case study reported in this paper, a data mining approach is applied to extract knowledge from a data set. The extracted knowledge is used for the prediction and prevention of manufacturing faults in wafers.",
    "neighbors": [
      442
    ],
    "mask": "Test"
  },
  {
    "node_id": 1,
    "label": 2,
    "text": "Exploration versus Exploitation in Topic Driven Crawlers Topic driven crawlers are increasingly seen as a way to address the scalability limitations of universal search engines, by distributing the crawling process across users, queries, or even client computers. The context available to a topic driven crawler allows for informed decisions about how to prioritize the links to be explored, given time and bandwidth constraints. We have developed a framework and a number of methods to evaluate the performance of topic driven crawler algorithms in a fair way, under limited memory resources. Quality metrics are derived from lexical features, link analysis, and a hybrid combination of the two. In this paper we focus on the issue of how greedy a crawler should be. Given noisy quality estimates of links in a frontier, we investigate what is an appropriate balance between a crawler's need to exploit this information to focus on the most promising links, and the need to explore links that appear suboptimal but might lead to more relevant pages. We show that exploration is essential to locate the most relevant pages under a number of quality measures, in spite of a penalty in the early stage of the crawl.",
    "neighbors": [
      53,
      281,
      457,
      649,
      662,
      774,
      968,
      1000,
      1017,
      1264
    ],
    "mask": "Train"
  },
  {
    "node_id": 2,
    "label": 3,
    "text": "Software Engineering and Middleware: A Roadmap The construction of a large class of distributed systems can be simplified by leveraging middleware, which is layered between network operating systems and application components. Middleware resolves heterogeneity, and facilitates communication and coordination of distributed components. State of-the-practice middleware products enable software engineers to build systems that are distributed across a localarea network. State-of-the-art middleware research aims to push this boundary towards Internet-scale distribution, adaptive systems, middleware for dependable and wireless systems. The challenge for software engineering research is to devise notations, techniques, methods and tools for distributed system construction that systematically build and exploit the capabilities that middleware products deliver, now and in the future.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 3,
    "label": 0,
    "text": "Dynamic-Agents for Dynamic Service Provisioning We claim that a dynamic-agent infrastructure can provide a shift from static distributed computing to dynamic distributed computing, and we have developed such an infrastructure to realize such a shift. We shall show its impact on software engineering through a comparison with other distributed object-oriented systems such as CORBA and DCOM, and demonstrate its value in highly dynamic system integration and service provisioning.  The infrastructure is Java-based, light-weight, and extensible. It differs from other agent platforms and client/server infrastructures in its support of dynamic behavior modification of agents. A dynamic-agent is not designed to have a fixed set of predefined functions but instead, to carry application-specific actions, which can be loaded and modified on the fly. This allows a dynamic-agent to adjust its capability for accommodating environment and requirement changes, and play different roles across multiple applications.  The above features are supported b...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 4,
    "label": 0,
    "text": "A Hybrid Mobile Robot Architecture with Integrated Planning and Control Research in the planning and control of mobile robots has received much attention in the past two decades. Two basic approaches have emerged from these research efforts: deliberative vs. reactive. These two approaches can be distinguished by their different usage of sensed data and global knowledge, speed of response, reasoning capability, and complexity of computation. Their strengths are complementary and their weaknesses can be mitigated by combining the two approaches in a hybrid architecture. This paper describes a method for goal-directed, collision-free navigation in unpredictable environments that employs a behavior-based hybrid architecture with asynchronously operating behavioral modules. It differs from existing hybrid architectures in two important ways: (1) the planning module produces a sequence of checkpoints instead of a conventional complete path, and (2) in addition to obstacle avoidance, the reactive module also performs target reaching under the control of a self-organizing neural network. The neural network is trained to perform fine, smooth motor control that moves the robot through the checkpoints. These two aspects facilitate a tight integration between high-level planning and low-level control, which permits real-time performance and easy path modification even when the robot is en route to the goal position.",
    "neighbors": [
      1194
    ],
    "mask": "Test"
  },
  {
    "node_id": 5,
    "label": 3,
    "text": "XM2VTSDB: The Extended M2VTS Database In this paper we describe the acquisition and content of a large multi-modal database intended for training and testing of multi-modal verification systems. The XM2VTSDB database offers synchronised video and speech data as well as image sequences allowing multiple views of the face. It consists of digital video recordings taken of 295 hundred subjects at one month intervals taken over a period of five months. We also describe a protocol for evaluating verification algorithms on the database. The database has been made available to anyone on request to the University of Surrey through http://www.ee.surrey.ac.uk/Research/VSSP/xm2vtsdb.",
    "neighbors": [
      1060
    ],
    "mask": "Validation"
  },
  {
    "node_id": 6,
    "label": 1,
    "text": "Use of Satellite Image Referencing Algorithms to Characterize Asphaltic Concrete Mixtures A natural way to test the structural integrity of a pavement is to send signals with different frequencies through the pavement and compare the results with the signals passing through an ideal pavement. For this comparison, we must determine how, for the corresponding mixture, the elasticity E depends on the frequency f in the range from 0.1 to 10  5  Hz. It is very expensive to perform measurements in high frequency area (above 20 Hz). To avoid these measurements, we can use the fact that for most of these mixtures, when we change a temperature, the new dependence changes simply by scaling. Thus, instead of performing expensive measurements for different frequencies, we can measure the dependence of E on moderate frequencies f for different temperatures, and then combine the resulting curves into a single \"master\" curve. In this paper, we show how fuzzy techniques can help to automate this \"combination\".",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 7,
    "label": 3,
    "text": "A Case for Parallelism in Data Warehousing and OLAP In recent years the database community has experienced a tremendous increase in the availability of new technologies to support efficient storage and retrieval of large volumes of data, namely data warehousing and  On-Line Analytical Processing (OLAP) products. Efficient query processing is critical in such an environment, yet achieving quick response times with OLAP queries is still largely an open issue. In this paper we propose a solution approach to this problem by applying  parallel processing techniques to a warehouse environment. We suggest an efficient partitioning strategy based on the relational representation of a data warehouse (i.e., star schema). Furthermore, we incorporate a particular indexing strategy, DataIndexes, to further improve query processing times and parallel resource utilization, and propose a preliminary parallel star-join strategy. 1 Introduction  In recent years, there has been an explosive growth in the use of databases for decision support. This phenome...",
    "neighbors": [
      248,
      389
    ],
    "mask": "Validation"
  },
  {
    "node_id": 8,
    "label": 2,
    "text": "A State-of-the-art Review on Multimodal Video Indexing Efficient and effective handling of video documents depends on the availability of indexes. Manual indexing is unfeasible for large video collections. Effective indexing requires a multimodal approach in which either the most appropriate modality is selected or the different modalities are used in collaborative fashion. In this paper we focus on the similarities and differences between the modalities, and survey several methods aiming at automating the time and resource consuming process of video indexing. Furthermore, we put forward a unifying and multimodal framework, which views a video document from the perspective of its author. This framework forms the guiding principle for identifying index types, for which automatic methods are found in literature. It furthermore forms the basis for categorizing these different methods.",
    "neighbors": [
      550,
      605
    ],
    "mask": "Train"
  },
  {
    "node_id": 9,
    "label": 3,
    "text": "Discovering Web Access Patterns and Trends by Applying OLAP and Data Mining Technology on Web Logs As a confluence of data mining and WWW technologies, it is now possible to perform data mining on web log records collected from the Internet web page access history. The behaviour of the web page readers is imprinted in the web server log files. Analyzing and exploring regularities in this behaviour can improve system performance, enhance the quality and delivery of Internet information services to the end user, and identify population of potential customers for electronic commerce. Thus, by observing people using collections of data, data mining can bring considerable contribution to digital library designers. In a joint effort between the TeleLearning-NCE project on Virtual University and NCE-IRIS project on data mining, we have been developing the knowledge discovery tool, WebLogMiner, for mining web server log files. This paper presents the design of the WebLogMiner, reports the current progress, and outlines the future work in this direction.",
    "neighbors": [
      447,
      1120
    ],
    "mask": "Train"
  },
  {
    "node_id": 10,
    "label": 3,
    "text": "Managing the Operator Ordering Problem in Parallel Databases This paper focuses on parallel query optimization. We consider the operator problem and introduce a new class of execution strategies called Linear-Oriented Bushy Trees (LBT). Compared to the related approach of the General Bushy Trees (GBT) a significant complexity reduction of the operator ordering problem can be derived theoretically and demonstrated experimentally (e.g. compared with GBTs, LBTs authorize optimization time improvement that can reach up-to 49%) without loosing quality. Finally we demonstrate that existing commercial parallel query optimizers need little extension modifications in order to handle LBTs.  Key words: Parallel databases, parallel query optimization, linear-oriented bushy  trees, extending existing optimizers.  1 Introduction  Modern database applications, such as data mining and decision support pose several new challenges to query optimization and processing [1]. One of the main issues concerns the processing of complex queries (e.g. recent Teradata rela...",
    "neighbors": [
      788
    ],
    "mask": "Train"
  },
  {
    "node_id": 11,
    "label": 2,
    "text": "SpeechBot: a Speech Recognition based Audio Indexing System for the Web We have developed an audio search engine incorporating speech recognition technology. This allows indexing of spoken documents from the World Wide Web when no transcription is available. This site indexes several talk and news radio shows covering a wide range of topics and speaking styles from a selection of public Web sites with multimedia archives. Our Web site is similar in spirit to normal Web search sites; it contains an index, not the actual multimedia content. The audio from these shows suffers in acoustic quality due to bandwidth limitations, coding, compression, and poor acoustic conditions. The shows are typically sampled at 8 kHz and transmitted, RealAudio compressed, at 6.5 kbps. Our word-error rate results using appropriately trained acoustic models show remarkable resilience to the high compression, though many factors combine to increase the average word-error rates over standard broadcast news benchmarks. We show that, even if the transcription is inaccurate, we can st...",
    "neighbors": [
      763,
      796,
      1161
    ],
    "mask": "Test"
  },
  {
    "node_id": 12,
    "label": 4,
    "text": "Context Awareness by Analysing Accelerometer Data In this paper we describe continuing work being carried out as part of the Bristol Wearable Computing Initiative. We are researching processing techniques for data from accelerometers which enable the wearable computer to determine the user's activity.  We have experimented with, and review, techniques already employed by others; and then propose new methods for analysing the data delivered by these devices. We try to minimise the number of devices needed, and use a single X-Y accelerometer device.  Using our techniques we have adapted our GPS based Tourist Guide wearable Computer application to include a multimedia presentation which gives the user information using different media depending on the user's activity as well as location.  1 Introduction and Background  This is a condensed version of a technical report. [1]  Our interests in wearable computing are centred around determining the context of the user and developing applications which make use of this information. We are expl...",
    "neighbors": [
      124,
      1192
    ],
    "mask": "Validation"
  },
  {
    "node_id": 13,
    "label": 1,
    "text": "Actor-Critic Algorithms We propose and analyze a class of actor-critic algorithms for  simulation-based optimization of a Markov decision process over  a parameterized family of randomized stationary policies. These  are two-time-scale algorithms in which the critic uses TD learning  with a linear approximation architecture and the actor is updated  in an approximate gradient direction based on information provided  by the critic. We show that the features for the critic should  span a subspace prescribed by the choice of parameterization of the  actor. We conclude by discussing convergence properties and some  open problems.  1 Introduction  The vast majority of Reinforcement Learning (RL) [9] and Neuro-Dynamic Programming (NDP) [1] methods fall into one of the following two categories:  (a) Actor-only methods work with a parameterized family of policies. The gradient of the performance, with respect to the actor parameters, is directly estimated by simulation, and the parameters are updated in a direction o...",
    "neighbors": [
      97
    ],
    "mask": "Validation"
  },
  {
    "node_id": 14,
    "label": 3,
    "text": "Computing and Comparing Semantics of Programs in Four-valued Logics The different semantics that can be assigned to a logic program correspond to different assumptions made concerning the atoms whose logical values cannot be inferred from the rules. Thus, the well founded semantics corresponds to the assumption that every such atom is false, while the Kripke-Kleene semantics corresponds to the assumption that every such atom is unknown. In this paper, we propose to unify and extend this assumption-based approach by introducing parameterized semantics for logic programs. The parameter holds the value that one assumes for all atoms whose logical values cannot be inferred from the rules. We work within Belnap's four-valued logic, and we consider the class of logic programs defined by Fitting. Following Fitting's approach, we define a simple operator that allows us to compute the parameterized semantics, and to compare and combine semantics obtained for different values of the parameter. The semantics proposed by Fitting corresponds to the value false. We also show that our approach captures and extends the usual semantics of conventional logic programs thereby unifying their computation.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 15,
    "label": 4,
    "text": "A Pattern Approach to Interaction Design To create successful interactive systems, user interface designers need to cooperate with developers and application domain experts in an interdisciplinary team. These groups, however, usually miss a common terminology to exchange ideas, opinions, and values. This paper presents an approach that uses pattern languages to capture this knowledge in software development, HCI, and the application domain. A formal, domain-independent definition of design patterns allows for computer support without sacrificing readability, and pattern use is integrated into the usability engineering lifecycle. As an example, experience from building an award-winning interactive music exhibit was turned into a pattern language, which was then used to inform follow-up projects and support HCI education.",
    "neighbors": [
      240
    ],
    "mask": "Train"
  },
  {
    "node_id": 16,
    "label": 0,
    "text": "Communication Primitives for Ubiquitous Systems or RPC Considered Harmful RPC is widely used to access and modify remote state. Its procedural call semantics are argued as an efficient unifying paradigm for both local and remote access. Our experience with ubiquitous device control systems has shown otherwise. RPC semantics of a synchronous, blocking invocation on a statically typed interface are overly restrictive, inflexible, and fail to provide an efficient unifying abstraction for accessing and modifying state in ubiquitous systems. This position paper considers other alternatives and proposes the use of comvets (conditional, mobility aware events) as the unifying generic communication paradigm for such systems.  Keywords: RPC, RMI, Events, Comvets, CORBA, Jini  1 Introduction  Ubiquitous environments or active spaces are the next generation of device control networks. A user interacts with an active space by using novel interfaces like speech and gesture input [1] to control her environment, and the system interacts with the user using audio/video outpu...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 17,
    "label": 3,
    "text": "Benchmarking XML Management Systems: The XOO7 Way The effectiveness of existing XML query languages has been studied by many who focused  on the comparison of linguistic features, implicitly reflecting the fact that most XML tools exist  only on paper. In this paper, with a focus on efficiency and concreteness, we propose a pragmatic  first step toward the systematic benchmarking of XML query processing platforms. We begin by  identifying the necessary functionalities an XML data management system should support. We  review existing approaches for managing XML data and the query processing capabilities of these  approaches. We then compare three XML query benchmarks XMach-1, XMark and XOO7 and  discuss the applicability, strengths and limitations of these benchmarks. We highlight the bias of  these benchmarks towards the data centric view of XML and motivate our selection of XOO7 to  extend with document centric queries. We complete XOO7 to capture the information retrieval  capabilities of XML management systems. Finally we summarize our contributions and discuss  future directions.",
    "neighbors": [
      364,
      585,
      1162
    ],
    "mask": "Test"
  },
  {
    "node_id": 18,
    "label": 3,
    "text": "dQUOB: Managing Large Data Flows Using Dynamic Embedded Queries The dQUOB system satisfies client need for specific information from high-volume data streams. The data streams we speak of are the flow of data existing during large-scale visualizations, video streaming to large numbers of distributed users, and high volume business transactions. We introduces the notion of conceptualizing a data stream as a set of relational database tables so that a scientist can request information with an SQL-like query. Transformation or computation that often needs to be performed on the data en-route can be conceptualized ascomputation performed onconsecutive views of the data, with computation associated with each view. The dQUOB system moves the query code into the data stream as a quoblet; as compiled code. The relational database data model has the significant advantage of presenting opportunities for efficient reoptimizations of queries and sets of queries. Using examples from global atmospheric modeling, we illustrate the usefulness of the dQUOB system. We carry the examples through the experiments to establish the viability of the approach for high performance computing with a baseline benchmark. We define a cost-metric of end-to-end latency that can be used to determine realistic cases where optimization should be applied. Finally, we show that end-to-end latency can be controlled through a probability assigned to a query that a query will evaluate to true.",
    "neighbors": [
      309
    ],
    "mask": "Train"
  },
  {
    "node_id": 19,
    "label": 3,
    "text": "Use Case Maps as a Feature Description Notation . We propose Use Case Maps (UCMs) as a notation for describing features. UCMs capture functional requirements in terms of causal scenarios bound to underlying abstract components. This particular view proved very useful in the description of a wide range of reactive and telecommunications systems. This paper presents some of the most interesting constructs and benefits of the notation in relation to a question on a User Requirements Notation recently approved by ITU-T Study Group 10, which will lead to a new Recommendation by 2003. Tool support, current research on UCMs, and related notations are also discussed.  1 Introduction  The modeling of reactive systems requires an early emphasis on behavioral aspects such as interactions between the system and the external world (including the users), on the cause-to-e#ect relationships among these interactions, and on intermediate activities performed by the system. Scenarios are particularly good at representing such aspects so that various ...",
    "neighbors": [
      179
    ],
    "mask": "Train"
  },
  {
    "node_id": 20,
    "label": 2,
    "text": "Hermes - A Notification Service for Digital Libraries The high publication rate of scholarly material makes searching and browsing an inconvenient way to keep oneself up-todate. Instead of being the active part in information access, researchers want to be notified whenever a new paper in one's research area is published.",
    "neighbors": [
      475
    ],
    "mask": "Train"
  },
  {
    "node_id": 21,
    "label": 4,
    "text": "Towards Group Communication for Mobile Participants (Extended Abstract) Group communication will undoubtedly be a useful paradigm for many applications of wireless networking in which reliability and timeliness are requirements. Moreover, location awareness is clearly central to mobile applications such as traffic management and smart spaces. In this paper, we introduce our definition of proximity groups in which group membership depends on location and then discuss some requirements for a group membership management service suitable for proximity groups. We describe a novel approach to efficient coverage estimation, giving applications feedback on the proportion of the area of interest covered by a proximity group, and also discuss our approach to partition anticipation.",
    "neighbors": [
      919,
      1029
    ],
    "mask": "Train"
  },
  {
    "node_id": 22,
    "label": 1,
    "text": "Distinctive Features Should Be Learned Most existing machine vision systems perform recognition based on a xed set  of hand-crafted features, geometric models, or eigen-subspace decomposition.  Drawing from psychology, neuroscience and intuition, we show that certain  aspects of human performance in visual discrimination cannot be explained by  any of these techniques. We argue that many practical recognition tasks for  articial vision systems operating under uncontrolled conditions critically depend  on incremental learning. Loosely motivated by visuocortical processing,  we present feature representations and learning methods that perform biologically  plausible functions. The paper concludes with experimental results  generated by our method.  1 Introduction  How exible are the representations for visual recognition, encoded by the neurons of the human visual cortex? Are they predetermined by a xed developmental schedule, or does their development depend on their stimulation? Does their development cease at some poin...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 23,
    "label": 3,
    "text": "Interaction between Path and Type Constraints XML [7], which is emerging as an important standard for data exchange on the World-Wide Web, highlights the importance of semistructured data. Although the XML standard itself does not require any schema or type system, a number of proposals [6, 17, 19] have been developed that roughly correspond to data definition languages. These allow one to constrain the structure of XML data by imposing a schema on it. These and other proposals also advocate the need for integrity constraints, another form of constraints that should, for example, be capable of expressing inclusion constraints and inverse relationships. The latter have recently been studied as path constraints in the context of semistructured data [4, 9]. It is likely that future XML proposals will involve both forms of constraints, and it is therefore appropriate to understand the interaction between them. This paper investigates that interaction. In particular it studies constraint implication problems, which are important both i...",
    "neighbors": [
      585,
      681
    ],
    "mask": "Test"
  },
  {
    "node_id": 24,
    "label": 1,
    "text": "Robustness of Case-Initialized Genetic Algorithms We investigate the robustness of Case Initialized Genetic AlgoRithm (CIGAR) systems with respect to problem indexing. When confronted with a series of similar problems CIGAR stores potential solutions in a case-base or an associative memory and retrieves and uses these solutions to help improve a genetic algorithm 's performance over time. Defining similarity among the problems, or indexing, is key to performance improvement. We study four indexing schemes on a class of simple problems and provide empirical evidence of CIGAR's robustness to imperfect indexing.",
    "neighbors": [
      837
    ],
    "mask": "Train"
  },
  {
    "node_id": 25,
    "label": 0,
    "text": "Rapid Concurrent Software Engineering in Competitive Situations This article is an experience report on the evolutionary development process of AT Humboldt, a multi agent system which has become World Champion 1997 and Vice World Champion 1998 of RoboCup simulator league. It details why the artifical soccer initiative RoboCup is a tempting domain for rapid concurrent software engineering. Both the development processes in 1997 and 1998 are described, compared and evaluated. Lessons learned for development projects in distributed control conclude this report. 1 Introduction  In this article the project managers describe the evolutionary development process of the software project AT (AgentTeam) Humboldt, which has become World Champion 1997 and Vice World Champion 1998 in the simulator league of the artifical soccer contest RoboCup ([10]). The RoboCup initiative recently gets more and more popular among scientists in robotics, distributed systems and distributed artificial intelligence because of its strong competitive character and tight resource b...",
    "neighbors": [
      141,
      155,
      396,
      964
    ],
    "mask": "Test"
  },
  {
    "node_id": 26,
    "label": 0,
    "text": "Graphical Models for Recognizing Human Interactions We describe a real-time computer vision and machine learning system for modeling and recognizing human actions and interactions. Two different domains are explored: recognition of two-handed motions in the martial art 'Tai Chi', and multiple-person interactions in a visual surveillance task. Our system combines top-down with bottom-up information using a feedback loop, and is formulated with a Bayesian framework. Two different graphical models (HMMs and Coupled HMMs) are used for modeling both individual actions and multiple-agent interactions, and CHMMs are shown to work more efficiently and accurately for a given amount of training. Finally, to overcome the limited amounts of training data, we demonstrate that `synthetic agents' (Alife-style agents) can be used to develop flexible prior models of the person-to-person interactions.   1 INTRODUCTION  We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in two different scenari...",
    "neighbors": [
      319
    ],
    "mask": "Train"
  },
  {
    "node_id": 27,
    "label": 3,
    "text": "A Geometric Framework for Specifying Spatiotemporal Objects We present a framework for specifying spatiotemporal objects using spatial and temporal objects, and a geometric transformation. We define a number of classes of spatiotemporal objects and study their closure properties.  1 Introduction  Many natural or man-made phenomena have both a spatial and a temporal extent. Consider for example, a forest fire or property histories in a city. To store information about such phenomena in a database one needs appropriate data modeling constructs. We claim that a new concept, spatiotemporal object, is necessary. In this paper, we introduce a very general framework for specifying spatiotemporal objects. To define a spatiotemporal object we need a spatial object, a temporal object, and a continuous geometric transformation (specified using a parametric representation) that determines the image of the spatial object at different time instants belonging to the temporal object. In this framework, a number of classes of spatiotemporal objects arise quite ...",
    "neighbors": [
      56,
      129,
      147,
      929,
      1012
    ],
    "mask": "Train"
  },
  {
    "node_id": 28,
    "label": 3,
    "text": "SI-Designer: a tool for intelligent integration of information SI-Designer (Source Integrator Designer) is a designer support tool for semi \ufffd automatic integration of heterogeneous sources schemata (relational, object and semi \ufffd structured sources); it has been implemented within the MOMIS project and it carries out integration following a semantic approach which uses intelligent Description Logics-based techniques, clustering techniques and an extended ODMG-ODL language, \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd , to represent schemata, extracted, integrated information. Starting from the sources \u2019 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd descriptions (local schemata) SI-Designer supports the designer in the creation of an integrated view of all the sources (global schema) which is expressed in the same \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd language. We propose SI-Designer as a tool to build virtual catalogs in the E-Commerce environment. 1.",
    "neighbors": [
      294,
      766
    ],
    "mask": "Test"
  },
  {
    "node_id": 29,
    "label": 0,
    "text": "Error-Tolerant Agents . The use of agents in today's Internet world is expanding rapidly. Yet, agent developers",
    "neighbors": [
      342,
      659
    ],
    "mask": "Train"
  },
  {
    "node_id": 30,
    "label": 1,
    "text": "DirectSVM: A Fast And Simple Support Vector Machine Perceptron .  We propose a simple implementation of the Support Vector Machine (SVM) for pattern recognition, that is not based on solving a complex quadratic optimization problem. Instead we propose a simple, iterative algorithm that is based on a few simple heuristics. The proposed algorithm nds high-quality solutions in a fast and intuitively-simple way. In experiments on the COIL database, on the extended COIL database and on the Sonar database of the UCI Irvine repository, DirectSVM is able to nd solutions that are similar to these found by the original SVM. However DirectSVM is able to nd these solutions substantially faster, while requiring less computational resources than the original SVM.  INTRODUCTION  Support Vector Machines (SVMs) belong to the best-performing learning algorithms available. They have produced remarkable performance in a number of dicult learning tasks without requiring prior knowledge. We mention amongst others the following examples in pattern recognition: handwr...",
    "neighbors": [
      973
    ],
    "mask": "Train"
  },
  {
    "node_id": 31,
    "label": 3,
    "text": "Schema Evolution in Heterogeneous Database Architectures, A Schema Transformation Approach In previous work we have a developed general framework to support schema transformation and integration in heterogeneous database architectures. The framework consists of a hypergraph-based common data model and a set of primitive schema transformations defined for this model. Higher-level common data models and primitive schema transformations for them can be defined in terms of this lower-level model. A key feature of the framework is that both primitive and composite schema transformations are automatically reversible. We have shown in earlier work how this allows automatic query translation from a global schema to a set of source schemas. In this paper we show how our framework also readily supports  evolution of source schemas, allowing the global schema and the query translation pathways to be easily repaired, as opposed to having to be regenerated, after changes to source schemas. 1",
    "neighbors": [
      844
    ],
    "mask": "Train"
  },
  {
    "node_id": 32,
    "label": 4,
    "text": "Dynamic Connection of Wearable Computers to Companion Devices Using Near-Field Radio Hewlett-Packard Laboratories, Bristol and the University of Bristol Department of Computer Science are engaged in an initiative to explore the design, technology and use of wearable computers.  We describe a way of connecting a wearable computer to companion devices such as displays or cameras using near-field radio technology. The shortrange nature of near-field radio allows relatively high data rates (300 kbps -- 1Mbit), low power consumption and the interpretation of gestures as configuration requests.  Keywords: Near-field radio, dynamic connectivity.  INTRODUCTION  We are particularly interested in communication technologies that exhibit low-power, short range (up to 1 foot) and modest data rates (300 kbps -- 1 Mbs). The action of picking up a companion device (such as a display) establishes the communication link due to the very short range. An important aspect of a suitable communication technology is that the user is not required to touch an electrode and therefore handling of ...",
    "neighbors": [
      1192
    ],
    "mask": "Validation"
  },
  {
    "node_id": 33,
    "label": 4,
    "text": "The CyberShoe: A Wireless Multisensor Interface for a Dancer's Feet : As a bridge between our interest in Wearable Computer systems and new performance interfaces for digital music, we have built a highly instrumented pair of sneakers for interactive dance. These shoes each measure 16 different, continuous parameters expressed by each foot and are able to transmit them wirelessly to a base station placed well over 30 meters away, updating all values up to 60 times per second. This paper describes our system, illustrates its performance, and outlines a few musical mappings that we have created for demonstrations in computer-augmented dance.  ____________________________________  Electronic sensors have been incorporated into footwear for several different applications over the last several years. Employing force-sensing resistor arrays or pixelated capacitive sensing, insoles with very dense pressure sampling have been developed for research at the laboratories of footwear manufacturers and pediatric treatment facilities (Cavanaugh, et. al., 1992). Alth...",
    "neighbors": [
      979
    ],
    "mask": "Train"
  },
  {
    "node_id": 34,
    "label": 3,
    "text": "An Overview of Active Information Gathering in InfoSleuth InfoSleuth is a system of collaborating software agents that can be configured to perform many different information management activities in a distributed environment. InfoSleuth agents advertise semantic constraints about themselves to InfoSleuth brokers using a global domain ontology. When queried, a broker reasons over these constraints to determine the minimal set of agents that can provide a solution to the query. InfoSleuth's architecture is based on a generic agent shell that provides basic agent communication behaviors over a subset of Knowledge Query Manipulation Language. Individual agents are subclasses of this generic shell that provide specific kinds of functionality. InfoSleuth agents perform a number of complex query activities that require resolving ontology-based queries over dynamically changing, distributed, heterogeneous resources, including distributed query, location-independent single-resource updates, event monitoring by means of subscription/notification servi...",
    "neighbors": [
      132,
      663
    ],
    "mask": "Train"
  },
  {
    "node_id": 35,
    "label": 1,
    "text": "Inference and Learning in Hybrid Bayesian Networks We survey the literature on methods for inference and learning in Bayesian Networks composed of discrete and continuous nodes, in which the continuous nodes have a multivariate Gaussian distribution, whose mean and variance depends on the values of the discrete nodes. We also briefly consider hybrid Dynamic Bayesian Networks, an extension of switching Kalman filters. This report is meant to summarize what is known at a sufficient level of detail to enable someone to implement the algorithms, but without dwelling on formalities.  1 1 Introduction  We discuss Bayesian networks (BNs [Jen96]) in which each node is either discrete or continuous, scalar or vector-valued, and in which the joint distribution over all the nodes is Conditional Gaussian (CG) [LW89, Lau92] i.e., for each instantiation i of the discrete nodes Y, the distribution over the continuous nodes X has the form f(xjY = i) = N (x; ~\u00af(i); \\Sigma(i)), where N () represents a multivariate Gaussian (MVG) or Normal density. (Note...",
    "neighbors": [
      791
    ],
    "mask": "Test"
  },
  {
    "node_id": 36,
    "label": 5,
    "text": "Optimising Propositional Modal Satisfiability for Description Logic Subsumption . Effective optimisation techniques can make a dramatic difference in the performance of knowledge representation systems based on expressive description logics. Because of the correspondence between description logics and propositional modal logic many of these techniques carry over into propositional modal logic satisfiability checking. Currently-implemented representation systems that employ these techniques, such as FaCT and DLP, make effective satisfiable checkers for various propositional modal logics. 1 Introduction  Description logics are a logical formalism for the representation of knowledge about individuals and descriptions of individuals. Description logics represent and reason with descriptions similar to \"all people whose friends are both doctors and lawyers\" or \"all people whose children are doctors or lawyers or who have a child who has a spouse\". The computations performed by systems that implement description logics are based around determining whether one descriptio...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 37,
    "label": 3,
    "text": "Improving Data Cleaning Quality using a Data Lineage Facility The problem of data cleaning, which consists of  removing inconsistencies and errors from original  data sets, is well known in the area of decision  support systems and data warehouses. However,  for some applications, existing ETL (Extraction  Transformation Loading) and data cleaning  tools for writing data cleaning programs are insufficient.  One important challenge with them is the  design of a data flow graph that effectively generates  clean data. A generalized difficulty is the lack  of explanation of cleaning results and user interaction  facilities to tune a data cleaning program.  This paper presents a solution to handle this problem  by enabling users to express user interactions  declaratively and tune data cleaning programs.  1",
    "neighbors": [
      546
    ],
    "mask": "Train"
  },
  {
    "node_id": 38,
    "label": 0,
    "text": "Modeling And Simulation Of Mobile Agents Agent-oriented software implies the realization of software components, which are mobile, autonomous, and solve problems by creating new software components during run-time, moving between locations, initiating or joining groups of other software components. Modeling and simulating those multiagent systems requires specific mechanisms for variable structure modeling.  JAMES, a Java-Based Agent Modeling Environment for Simulation, realizes variable structure models including mobility from the perspective of single autonomous agents. JAMES itself is based on parallel DEVS and adopts its abstract simulator model. Simulation takes place as a sending of messages between concurrently active and locally distributed entities which reflect the model's current structure. Thus, modeling and simulation are coined equally by an agent-based perspective.  1 Introduction  The definition of agents subsumes a multitude of different facets [30]. Agents are reactive, deliberative or combine reactive with ...",
    "neighbors": [
      121,
      1038
    ],
    "mask": "Validation"
  },
  {
    "node_id": 39,
    "label": 1,
    "text": "Parameterized Logic Programs where Computing Meets Learning Abstract. In this paper, we describe recent attempts to incorporate learning into logic programs as a step toward adaptive software that can learn from an environment. Although there are a variety of types of learning, we focus on parameter learning of logic programs, one for statistical learning by the EM algorithm and the other for reinforcement learning by learning automatons. Both attempts are not full- edged yet, but in the former case, thanks to the general framework and an e cient EM learning algorithm combined with a tabulated search, we have obtained very promising results that open up the prospect of modeling complex symbolic-statistical phenomena. 1",
    "neighbors": [
      559
    ],
    "mask": "Validation"
  },
  {
    "node_id": 40,
    "label": 0,
    "text": "Verification within the KARO Agent Theory Abstract. This paper discusses automated reasoning in the KARO framework. The KARO framework accommodates a range of expressive modal logics for describing the behaviour of intelligent agents. We concentrate on a core logic within this framework, in particular, we describe two new methods for providing proof methods for this core logic, discuss some of the problems we have encountered in their design, and present an extended example of the use of the KARO framework and the two proof methods. 1",
    "neighbors": [
      687
    ],
    "mask": "Test"
  },
  {
    "node_id": 41,
    "label": 2,
    "text": "Automatically Analyzing and Organizing Music Archives . We are experiencing a tremendous increase in the amount of  music being made available in digital form. With the creation of large multimedia  collections, however, we need to devise ways to make those collections  accessible to the users. While music repositories exist today, they mostly  limit access to their content to query-based retrieval of their items based on  textual meta-information, with some advanced systems supporting acoustic  queries. What we would like to have additionally, is a way to facilitate exploration  of musical libraries. We thus need to automatically organize music  according to its sound characteristics in such a way that we nd similar  pieces of music grouped together, allowing us to nd a classical section, or  a hard-rock section etc. in a music repository.  In this paper we present an approach to obtain such an organization of  music data based on an extension to our SOMLib digital library system  for text documents. Particularly, we employ the Self-Organizing Map to  create a map of a musical archive, where pieces of music with similar sound  characteristics are organized next to each other on the two-dimensional map  display. Locating a piece of music on the map then leaves you with related  music next to it, allowing intuitive exploration of a music archive.  Keywords: Multimedia, Music Library, Self-Organizing Map (SOM), Exploration  of Information Spaces, User Interface, MP3  1",
    "neighbors": [
      298
    ],
    "mask": "Train"
  },
  {
    "node_id": 42,
    "label": 0,
    "text": "Extending a Multi-Agent System for Genomic Annotation . The explosive growth in genomic (and soon, expression and proteomic)  data, exemplified by the Human Genome Project, is a fertile domain for the application  of multi-agent information gathering technologies. Furthermore, hundreds  of smaller-profile, yet still economically important organisms are being studied  that require the efficient and inexpensive automated analysis tools that multiagent  approaches can provide. In this paper we give a progress report on the use  of the DECAF multi-agent toolkit to build reusable information gathering systems  for bioinformatics. We will briefly summarize why bioinformatics is a classic  application for information gathering, how DECAF supports it, and recent  extensions underway to support new analysis paths for genomic information.  1",
    "neighbors": [
      964
    ],
    "mask": "Train"
  },
  {
    "node_id": 43,
    "label": 2,
    "text": "WebSail: From On-line Learning to Web Search In this paper we investigate the applicability of on-line learning algorithms to the real-world problem of web search. Consider that web documents are indexed using n Boolean features. We first present a practically efficient on-line learning algorithm TW2 to search for web documents represented by a disjunction of at most k relevant features. We then design and implement WebSail, a real-time adaptive web search learner, with TW2 as its learning component. WebSail learns from the user's relevance feedback in real-time and helps the user to search for the desired web documents. The architecture and performance of WebSail are also discussed.",
    "neighbors": [
      70,
      216,
      453,
      561,
      596,
      599,
      1000,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 44,
    "label": 4,
    "text": "User Interface Modelling with UML The Unified Modeling Language (UML) is a natural candidate  for user interface (UI) modelling since it is the standard notation  for object oriented modelling of applications. However, it is by no means  clear how to model UIs using UML. This paper presents a user interface  modelling case study using UML. This case study identifies some  aspects of UIs that cannot be modelled using UML notation, and a set  of UML constructors that may be used to model UIs. The modelling  problems indicate some weaknesses of UML for modelling UIs, while  the constructors exploited indicate some strengths. The identification  of such strengths and weaknesses can be used in the formulation of a  strategy for extending UML to provide greater support for user interface  design.",
    "neighbors": [
      744
    ],
    "mask": "Train"
  },
  {
    "node_id": 45,
    "label": 4,
    "text": "Using a Room Metaphor to Ease Transitions in Groupware Many groupware systems contain gaps that hinder or block natural social interaction or that do not let people easily move between different styles of work. We believe that the adoption of a room metaphor can ease people's transitions across these gaps, allowing them to work together more naturally. Using the TeamWave Workplace system as an example, we show how particular gaps are removed. First, we ease a person's transition between single user and groupware applications by making rooms suitable for both individual and group activity. Second, people can move fluidly between asynchronous and synchronous work because room artifacts persist. People can leave messages, documents and annotations for others, or work on them together when occupying the room at the same time. Third, we ease the difficulty of initiating real time work by providing people with awareness of others who may be available for real-time interactions, and by automatically establishing connections as users enter a commo...",
    "neighbors": [
      388,
      836,
      860
    ],
    "mask": "Train"
  },
  {
    "node_id": 46,
    "label": 1,
    "text": "A Connectionist Approach for Learning Search-Control Heuristics for Automated Deduction Systems The central problem in automated deduction is the explosive growth of search spaces when proof length increases. In this paper, a connectionist approach for learning search-control heuristics for automated deduction systems is presented. In particular, we show how folding architecture networks, a new type of neural networks capable of solving supervised learning tasks on structured data, can be used for learning heuristics evaluation functions for algebraic (logical) expressions and how these evaluation functions can then be used to control the search process for new proof problems. Experimental results with the automated deduction system  Setheo in an algebraic domain show a considerable performance improvement. Controlled by heuristics which had been learned from simple problems in this domain the system is able to solve several problems from the same domain which had been out of reach for the original system.   1 Introduction  The goal in automated deduction (AD) is to automatically...",
    "neighbors": [
      91
    ],
    "mask": "Validation"
  },
  {
    "node_id": 47,
    "label": 0,
    "text": "Ruling Agent Motion in Structured Environments . The design and development of cooperative Internet applications  based on mobile agents require appropriate modelling of both the physical  space where agents roam and the conceptual space of mobile agent interaction.  The paper discusses how an open, Internet-based, organisation network can be  modelled as a hierarchical collection of locality domains, where agents can  dynamically acquire information about resource location and availability  according to their permissions. It also analyses the issue of how agent motion  can be ruled and constrained within a structured environment by means of an  appropriate coordination infrastructure.  1 Introduction  Mobile agents are a promising technology for the design and development of cooperative applications on the Internet [3, 5, 12, 13]. Due to their capability of autonomously roaming the Internet, mobile agents can move locally to the resources they need -- let them be users, data, or services -- and there interact with them. This can p...",
    "neighbors": [
      269,
      416
    ],
    "mask": "Train"
  },
  {
    "node_id": 48,
    "label": 3,
    "text": "Dynamic Function Placement for Data-intensive Cluster Computing Optimally partitioning application andfilesystem functionality within a cluster of clients and servers is a difficult problem dueto dynamic variations in application behavior, resource availability, and workload mixes. Thispaper presents A BACUS, a run-time systemthat monitors and dynamically changes function placement for applications that manipulate largedata sets. Several examples of data-intensive workloads are used to show the importance ofproper function placement and its dependence on dynamic run-time characteristics, withperformance differences frequently reaching 2-10X. We evaluate how well the ABACUSprototype adapts to run-time system behavior, including both long-term variation (e.g., filterselectivity) and short-term variation (e.g., multi-phase applications and inter-applicationresource contention). Our experiments with ABACUS indicate that it is possible to adapt inall of these situations and that the adaptation converges most quickly in those cases where theperformance impact is most significant. 1",
    "neighbors": [
      131
    ],
    "mask": "Test"
  },
  {
    "node_id": 49,
    "label": 4,
    "text": "Using Plan Recognition in Human-Computer Collaboration . Human-computer collaboration provides a practical and useful application for  plan recognition techniques. We describe a plan recognition algorithm which is tractable by  virtue of exploiting properties of the collaborative setting, namely: the focus of attention, the  use of partially elaborated hierarchical plans, and the possibility of asking for clarification.  We demonstrate how the addition of our plan recognition algorithm to an implemented  collaborative system reduces the amount of communication required from the user.  1 Introduction  An important trend in recent work on human-computer interaction and user modeling has been to view human-computer interaction as a kind of collaboration (e.g, Ferguson and Allen, 1998, Guinn, 1996, Rich and Sidner, 1998, Rickel and Johnson, 1998). In this approach, the human user and the computer (often personified as an \"agent\") coordinate their actions toward achieving shared goals. A common setting for collaboration, illustrated in Figure 1...",
    "neighbors": [
      312,
      775,
      1191
    ],
    "mask": "Test"
  },
  {
    "node_id": 50,
    "label": 1,
    "text": "Empirical Risk Approximation: An Induction Principle for Unsupervised Learning Unsupervised learning algorithms are designed to extract structure from data without reference to explicit teacher information. The quality of the learned structure is determined by a cost function which guides the learning process. This paper proposes Empirical Risk Approximation as a new induction principle for unsupervised learning. The complexity of the unsupervised learning models are automatically controlled by the two conditions for learning: (i) the empirical risk of learning should uniformly converge towards the expected risk; (ii) the hypothesis class should retain a minimal variety for consistent inference. The maximal entropy principle with deterministic annealing as an efficient search strategy arises from the  Empirical Risk Approximation principle as the optimal inference strategy for large learning problems. Parameter selection of learnable data structures is demonstrated for the case of  k-means clustering. 1 What is unsupervised learning?  Learning algorithms are desi...",
    "neighbors": [
      505,
      615
    ],
    "mask": "Test"
  },
  {
    "node_id": 51,
    "label": 3,
    "text": "A Control Architecture for Flexible Internet Auction Servers The flexibility to support both high activity and low activity auctions is required by any system that allows bidding by both humans and software agents. We present the control architecture of the Michigan Internet AuctionBot, and discuss some of the system engineering issues that arose in its design. 1 Introduction  The Michigan Internet AuctionBot is a highly configurable auction server built to support research on electronic commerce and multiagent negotiation [3]. The first generation architecture was simple and robust, and allowed us to concentrate on other aspects of the system. However, several inefficiencies made it problematic to run auctions with very fast interactions. We have redesigned the core AuctionBot architecture in order to improve overall performance, while still meeting the original goal: a system that is configurable, maintainable, and capable of conducting a large number of simultaneous auctions. In AuctionBot architecture nomenclature, we say an auction is open ...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 52,
    "label": 4,
    "text": "A Semiotic Communication Model for Interface Design This research wants to contribute to the creation of a semiotic framework for interface design. Using the Jakobson's communication model to analyse the HCI approach to interface development, we explain how some central factors of communication are not enough considered by designers.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 53,
    "label": 2,
    "text": "Web Crawling Agents for Retrieving Biomedical Information Autonomous agents for topic driven retrieval of information from the Web are currently a very active area of research. The ability to conduct real time searches for information is important for many users including biomedical scientists, health care professionals and the general public. We present preliminary research on different retrieval agents tested on their ability to retrieve biomedical information, whose relevance is assessed using both genetic and ontological expertise. In particular, the agents are judged on their performance in fetching information about diseases when given information about genes. We discuss several key insights into the particular challenges of agent based retrieval learned from our initial experience in the biomedical domain.",
    "neighbors": [
      1,
      281,
      457,
      662,
      774,
      968
    ],
    "mask": "Train"
  },
  {
    "node_id": 54,
    "label": 4,
    "text": "A Two-stage Scheme for Dynamic Hand Gesture Recognition In this paper a scheme is presented for recognizing hand gestures using the output of a hand tracker which tracks a rectangular window bounding the hand region. A hierarchical scheme for dynamic hand gesture recognition is proposed based on state representation of the dominant feature trajectories using an a priori knowledge of the way in which each gesture is performed.",
    "neighbors": [
      334,
      635
    ],
    "mask": "Train"
  },
  {
    "node_id": 55,
    "label": 4,
    "text": "EventScope: Amplifying Human Knowledge and Experience via Intelligent Robotic Systems and Information Interaction The EventScope program develops publicly accessible \"reality browsers\" that display both archived and updating representations of remote environments derived from on-site robotic sensors. The interface encourages collaborative work within a community of users. Public exploration of real remote sites presents a variety of interface issues addressed by EventScope, including time delay, public exploration via a single robot and communication between geographically separate users from diverse backgrounds. Merging public interface with educational and contextual information extends the notion of \"interface\" to \"remote reality library.\" EventScope is a NASA and private foundationfunded project based at Carnegie Mellon University. 1. Introduction  Publicly funded Earth and planetary exploration is conducted to increase knowledge of our universe. The public traditionally accesses this knowledge passively, through the media. However, the development of the Web and of robotic remote-sensing tech...",
    "neighbors": [
      1157
    ],
    "mask": "Train"
  },
  {
    "node_id": 56,
    "label": 3,
    "text": "Nearest Neighbor Queries in a Mobile Environment Nearest neighbor queries have received much interest in recent  years due to their increased importance in advanced database applications. However, past work",
    "neighbors": [
      27,
      1012
    ],
    "mask": "Train"
  },
  {
    "node_id": 57,
    "label": 3,
    "text": "E-DEVICE: An Extensible Active Knowledge Base System with Multiple Rule Type Support This paper describes E-DEVICE, an extensible active knowledge base system (KBS) that supports the processing of event-driven, production, and deductive rules into the same active OODB system. E-DEVICE provides the infrastructure for the smooth integration of various declarative rule types, such as production and deductive rules, into an active OODB system that supports low-level event-driven rules only by a) mapping each declarative rule into one event-driven rule, offering centralized rule selection control for correct run-time behavior and conflict resolution, and b) using complex events to map the conditions of declarative rules and monitor the database to incrementally match those conditions. E-DEVICE provides the infrastructure for easily extending the system by adding a) new rule types as subtypes of existing ones and b) transparent optimizations to the rule matching network. The resulting system is a flexible, yet efficient, KBS that gives the user the ability to express knowledge in a variety of high-level forms for advanced problem solving in data intensive applications.",
    "neighbors": [
      814
    ],
    "mask": "Train"
  },
  {
    "node_id": 58,
    "label": 1,
    "text": "EWA Learning in Bilateral Call Markets This chapter extends the EWA learning model to bilateral call market games (also known as the \"sealed-bid mechanism\" in two-person bargaining). In these games, a buyer and seller independently draw private values from commonly-known distributions and submit bids. If the buyer's bid is above the seller's, they trade at the midpoint of the two bids; otherwise they don't trade. We apply EWA by assuming that players have value-dependent bidding strategies, and they partially generalize experience from one value/cost condition to another in response to the incentives from nonlinear optimal bid functions. The same learning model can be applied to other market institutions where subjects economize on learning by taking into consideration similarity between past experience and a new environment while still recognizing the difference in market incentives between them. The chapter also presents a new application of EWA to a \"continental divide\" coordination game, and reviews 32 earlier studies comparing EWA, reinforcement, and belief learning. The application shows the advantages of a generalized adaptive model of behavior that includes elements of reinforcement, belief-based and direction learning as special cases at some cost of complexity for the benefit of generality and psychological appeal. It is a good foundation to build upon to extend our understanding of adaptive behavior in more general games and market institutions. In future work, we should investigate the similarity parameters, y and w, to better characterize their magnitude and significance in different market institutions. Keywords: Experimental economics, call markets, sealed-bid mechanism, learning JEL Classification: D44, D83, C92 August 2, 2000. Thanks to Terry Daniel for supplying data. This research has been...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 59,
    "label": 3,
    "text": "Extending the ODMG Object Model with Time Although many temporal extensions of the relational data model have been proposed, there is no comparable amount of work in the context of object-oriented data models. Moreover, extensions to the relational model have been proposed in the framework of SQL standards, whereas no attempts have been made to extend the standard for object-oriented databases, defined by ODMG. This paper presents T ODMG, a temporal extension of the ODMG-93 standard data model. The main contributions of this work are, thus, the formalization of the ODMG standard data model and its extension with time. Another contribution of this work is the investigation, on a formal basis, of the main issues arising from the introduction of time in an object-oriented model.",
    "neighbors": [
      397,
      535
    ],
    "mask": "Train"
  },
  {
    "node_id": 60,
    "label": 1,
    "text": "Reinforcement Learning for Visual Servoing of a Mobile Robot A novel reinforcement learning algorithm is applied to a visual servoing task on a real mobile robot. There is no requirement for camera calibration, an actuator model or a knowledgeable teacher. The controller learns from a critic which gives a scalar reward. The learning algorithm handles continuously valued states and actions and can learn from good and bad experiences including data gathered while performing unrelated behaviours and from historical data. Experimental results are presented. 1 Introduction Visual servoing consists of moving some part of a robot to a desired position using visual feedback [ Hutchinson et al., 1996 ] . It is a basic building block for purposeful robot behaviours such as foraging, target pursuit and landmark based navigation. Some degree of calibration is generally required to achieve visual servoing. This calibration can be a time consuming and error prone process. In this work we show that reinforcement based learning can eliminate the ca...",
    "neighbors": [
      621
    ],
    "mask": "Train"
  },
  {
    "node_id": 61,
    "label": 1,
    "text": "Optimization and Interpretation of Rule-Based Classifiers Abstract. Machine learning methods are frequently used to create rule-based classifiers. For continuous features linguistic variables used in conditions of the rules are defined by membership functions. These linguistic variables should be optimized at the level of single rules or sets of rules. Assuming the Gaussian uncertainty of input values allows to increase the accuracy of predictions and to estimate probabilities of different classes. Detailed interpretation of relevant rules is possible using (probabilistic) confidence intervals. A real life example of such interpretation is given for personality disorders. The approach to optimization and interpretation described here is applicable to any rule-based system. 1 Introduction. In many applications rule-based classifiers are created starting from machine learning, fuzzy logic or neural network methods [1]\u2013[3]. If the number of rules is relatively small and accuracy is sufficiently high such classifiers are an optimal choice, because the reasons for their decisions are easily verified. Crisp logical rules are desirable",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 62,
    "label": 1,
    "text": "Introducing a New Advantage of Crossover: Commonality-Based Selection The Commonality-Based Crossover Framework defines crossover as a two-step process: 1) preserve the maximal common schema of two parents, and 2) complete the solution with a construction heuristic. In these \u201cheuristic \u201d operators, the first step is a form of selection. This commonality-based form of selection has been isolated in GENIE. Using random parent selection and a non-elitist generational replacement scheme, GENIE does not include fitness-based selection. However, a theoretical analysis shows that \u201cideal \u201d construction heuristics in GENIE can potentially converge to optimal solutions. Experimentally, results show that the effectiveness of practical construction heuristics can be amplified by commonalitybased restarts. Overall, it is shown that the commonality hypothesis is valid--schemata common to above-average solutions are indeed above average. Since common schemata can only be identified by multi-parent operators, commonality-based selection is a unique advantage that crossover can enjoy over mutation. 1",
    "neighbors": [
      866
    ],
    "mask": "Train"
  },
  {
    "node_id": 63,
    "label": 3,
    "text": "CAT: the Copying Approach to Tabling The SLG-WAM is an abstract machine that can be characterized as a sharing approach to implementing tabling: The execution environments of suspended computations are interspersed in the WAM stacks. Stacks are frozen using a set of freeze registers, and the WAM trail mechanism is extended so that the suspended computations can be resumed. This technique has a reasonably small execution overhead, but it is not easy to implement on top of an existing Prolog system. It is also quite difficult to understand. We propose a new technique for the implementation of tabling: the copying approach to tabling. CAT does not impose any overhead to the execution of Prolog code and can be introduced into an existing Prolog system orthogonally. Also, CAT is easier to understand. We have implemented CAT in the XSB system by taking out SLG-WAM and adding CAT. We describe the additions needed for adopting CAT in a WAM implementation. We show a case in which CAT performs arbitrarily worse than SLG-WAM, but on the other hand we present empirical evidence that CAT is competitive and often faster than the SLG-WAM. We also briefly discuss issues related to memory management and to the scheduling.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 64,
    "label": 0,
    "text": "PMS: a PVC Management System for ATM Networks . Reported in this paper is the developed PMS, a PVC management  system for ATM networks. PMS provides a scalable, end-to-end path  management solution required for managing todays complex ATM networks.  It aims to assist the network operators to perform PVC operations with  simplified procedures and automatic optimum route selection. It also aims to  provide effective decision-making support for PVC fault identification and  prevention to the network operators.  1 Introduction  ATM communication network is playing more and more important role in todays  telecommunication networks. It has been widely used in backbone networks, transmission networks, access networks, and even enterprise networks. Such emerging large heterogeneous ATM networks have raised many new challenges for researchers and developers in the area of network management. In the management of ATM communication networks that have increased dramatically in size and complexity, the PVC (Permanent Virtual Circuit) managemen...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 65,
    "label": 1,
    "text": "Direct value-approximation for factored MDPs We present a simple approach for computing reasonable policies  for factored Markov decision processes (MDPs), when the optimal  value function can be approximated by a compact linear form.",
    "neighbors": [
      820
    ],
    "mask": "Validation"
  },
  {
    "node_id": 66,
    "label": 5,
    "text": "A Web-Based ITS Controlled by an Expert System Intelligent Tutoring System (ITS) for teaching high school teachers how to use new technologies. It offers course units covering the needs of users with different knowledge levels and characteristics. It tailors the presentation of the educational material to the users' diverse needs by using AI techniques to specify each user's model as well as to make pedagogical decisions. This is achieved via an expert system that uses a hybrid knowledge representation formalism integrating symbolic rules with neurocomputing.",
    "neighbors": [
      1257
    ],
    "mask": "Train"
  },
  {
    "node_id": 67,
    "label": 0,
    "text": "Emergent Cooperative Goal-Satisfaction in Large Scale Automated-Agent Systems Cooperation among autonomous agents has been discussed in the DAI community for several years. Papers about cooperation [6, 45], negotiation [33], distributed planning [5], and coalition formation [28, 48], have provided a variety of approaches and several algorithms and solutions to situations wherein cooperation is possible. However, the case of cooperation in large-scale multi-agent systems (MAS) has not been thoroughly examined. Therefore, in this paper we present a framework for cooperative goal-satisfaction in large-scale environments focusing on a low complexity physics-oriented approach. The multi-agent systems with which we deal are modeled by a physics-oriented model. According to the model, MAS inherit physical properties, and therefore the evolution of the computational systems is similar to the evolution of physical systems. To enable implementation of the model, we provide a detailed algorithm to be used by a single agent within the system. The model and the algorithm are a...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 68,
    "label": 3,
    "text": "Improving the Performance of High-Energy Physics Analysis through Bitmap Indices Bitmap indices are popular multi-dimensional structures for accessing read-mostly data such as data warehouse (DW) applications, decision support systems (DSS) and on-line analytical processing (OLAP). One of their main strengths is that they provide good performance characteristics for complex adhoc and an efficient combination of multiple index in one query. Considerable research work has been done in the area of finite (and low) attribute cardinalities. However, additional complexity is imposed on the design of bitmap indices for high cardinality or even non-discrete attributes, where different optimisation techniques than the ones proposed so far have to be applied. In this paper we discuss the design and implementation of bitmap indices for High-Energy Physics (HEP) analysis, where the potential search space consists of hundreds of independent dimensions. A single HEP query typically covers 10 to 100 dimensions out of the whole searchs space. In this context we evaluated two different bitmap encoding techniques, namely equality encoding and range encoding. For both methods the number of bit slices (or bitmap vectors) per attribute is a a central optimisation parameter. The paper presents some (first) results for choosing the optimal number of bit slices for multi-dimensional indices with attributes of different value distribution and query selectivity. We believe taht this discussion is not only applicable to HEP but also to DW, DSS and OLAP type problems in general.",
    "neighbors": [
      1019
    ],
    "mask": "Train"
  },
  {
    "node_id": 69,
    "label": 2,
    "text": "Logical Structure Detection for Heterogeneous Document Classes We present a fully implemented system based on generic document knowledge for detecting the logical structure of documents for which only general layout information is assumed. In particular, we focus on detecting the reading order. Our system integrates components based on computer vision, articial intelligence, and natural language processing techniques. The prominent feature of our framework is its ability to handle documents from heterogeneous collections. The system has been evaluated on a standard collection of documents to measure the quality of the reading order detection. Experimental results for each component and the system as a whole are presented and discussed in detail. The performance of the system is promising, especially when considering the diversity of the document collection.  Keywords: Document Analysis, Logical Structure Detection, Reading Order Detection, Natural Language Processing, Spatial Reasoning.  1. INTRODUCTION  The goal of document analysis is to automa...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 70,
    "label": 2,
    "text": "Yarrow: A Real-Time Client Side Meta-Search Learner In this paper we report our research on building Yarrow - an intelligent web meta-search engine. The predominant feature of Yarrow is that in contrast to the lack of adaptive learning features in existing metasearch engines, Yarrow is equipped with a practically efficient on-line learning algorithm so that it is capable of helping the user to search for the desired documents with as little feedback as possible. Currently, Yarrow can query eight of the most popular search engines and is able to perform document parsing and indexing, and learning in real-time on client side. Its architecture and performance are also discussed. 1. Introduction As the world wide web evolves and grows so rapidly, web search, an interface between the human users and the vast information gold mine of the web, is becoming a necessary part of people's daily life. Designing and implementing practically effective web search tools is a challenging task. It calls for innovative methods and strategies f...",
    "neighbors": [
      43,
      453,
      561,
      596,
      1000,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 71,
    "label": 1,
    "text": "Hallucinating Faces In most surveillance scenarios there is a large distance between the camera and the objects of interest in the scene. Surveillance cameras are also usually set up with wide fields of view in order to image as much of the scene as possible. The end result is that the objects in the scene normally appear very small in surveillance imagery. It is generally possible to detect and track the objects in the scene, however, for tasks such as automatic face recognition and license plate reading, resolution enhancement techniques are often needed. Although numerous resolution enhancement algorithms have been proposed in the literature, most of them are limited by the fact that they make weak, if any, assumptions about the scene. We propose an algorithm that can be used to learn a prior on the spatial distribution of the image gradient for frontal images of faces. We proceed to show how such a prior can be incorporated into a super-resolution algorithm to yield 4-8 fold improvements in resolution #...",
    "neighbors": [
      1218
    ],
    "mask": "Validation"
  },
  {
    "node_id": 72,
    "label": 1,
    "text": "Two Views of Classifier Systems This work suggests two ways of looking at Michigan classifier systems; as Genetic Algorithm-based systems, and as Reinforcement Learning-based systems, and argues that the former is more suitable for traditional strength-based systems while the latter is more suitable for accuracy-based XCS. The dissociation of the Genetic Algorithm from policy determination in XCS is noted, and the two types of Michigan classifier system are contrasted with Pittsburgh systems.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 73,
    "label": 0,
    "text": "Trust Relationships in a Mobile Agent System . The notion of trust is presented as an important component  in a security infrastructure for mobile agents. A trust model that can  be used in tackling the aspect of protecting mobile agents from hostile  platforms is proposed. We dene several trust relationships in our model,  and present a trust derivation algorithm that can be used to infer new  relationships from existing ones. An example of how such a model can  be utilized in a practical system is provided.  1",
    "neighbors": [
      925
    ],
    "mask": "Train"
  },
  {
    "node_id": 74,
    "label": 0,
    "text": "Improving the Scalability of Multi-agent Systems . There is an increasing demand for designers and developers  to construct ever larger multi-agent systems. Such systems will be composed  of hundreds or even thousands of autonomous agents. Moreover,  in open and dynamic environments, the number of agents in the system  at any one time will uctuate signicantly. To cope with these twin  issues of scalability and variable numbers, we hypothesize that multiagent  systems need to be both self-building (able to determine the most  appropriate organizational structure for the system by themselves at runtime)  and adaptive (able to change this structure as their environment  changes). To evaluate this hypothesis we have implemented such a multiagent  system and have applied it to the domain of automated trading.  Preliminary results supporting the rst part of this hypothesis are presented:  adaption and self-organization do indeed make the system better  able to cope with large numbers of agents.  1 Introduction  When designing or buildin...",
    "neighbors": [
      942
    ],
    "mask": "Train"
  },
  {
    "node_id": 75,
    "label": 4,
    "text": "A Wearable Spatial Conferencing Space Wearable computers provide constant access to computing and communications resources. In this paper we describe how the computing power of wearables can be used to provide spatialized 3D graphics and audio cues to aid communication. The result is a wearable augmented reality communication space with audio enabled avatars of the remote collaborators surrounding the user. The user can use natural head motions to attend to the remote collaborators, can communicate freely while being aware of other side conversations and can move through the communication space. In this way the conferencing space can support dozens of simultaneous users. Informal user studies suggest that wearable communication spaces may offer several advantages, both through the increase in the amount of information it is possible to access and the naturalness of the interface.  1: Introduction  One of the broad trends emerging in human-computer interaction is the increasing portability of computing and communication fac...",
    "neighbors": [
      738,
      886,
      965,
      1043,
      1192
    ],
    "mask": "Test"
  },
  {
    "node_id": 76,
    "label": 0,
    "text": "A Software Fault Tree Approach to Requirements Analysis of an Intrusion Detection System Requirements analysis for an Intrusion Detection System (IDS) involves deriving requirements for the IDS from analysis of the intrusion domain. When the IDS is, as here, a collection of mobile agents that detect, classify, and correlate system and network activities, the derived requirements include what activities the agent software should monitor, what intrusion characteristics the agents should correlate, where the IDS agents should be placed to feasibly detect the intrusions, and what countermeasures the software should initiate. This paper describes the use of software fault trees for requirements identification and analysis in an IDS. Intrusions are divided into seven stages (following Ruiu), and a fault subtree is developed to model each of the seven stages (reconnaissance, penetration, etc.). Two examples are provided. This approach was found to support requirements evolution (as new intrusions were identified), incremental development of the IDS, and prioritization of countermeasures.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 77,
    "label": 3,
    "text": "Constraints in Object-Oriented Databases Normal forms in relational database theory, like 3NF or BCNF, are dened by  means of semantic contraints. Since for these constraints sound and complete axiomatisations  exist and, additionally, for some of these constraints the implication  problem is decidable, computer aided database design is possible for relational  data models. Object-oriented database theory lacks such normal forms, partly  because neither a classication of semantic constraints nor sound and complete  axiomatisations exist.  In this work we present three classes of semantic constraints for object-oriented  data models and show that these constraints have a sound and complete axiomatisation.  Thus we prepare the grounds for normal forms in object-oriented data  models and subsequently for computer aided object-oriented database design.  1 Introduction  The theory of database design for relational data models identies a number of properties to characterise good database schemas. These properties lead then to no...",
    "neighbors": [
      773,
      1073
    ],
    "mask": "Train"
  },
  {
    "node_id": 78,
    "label": 3,
    "text": "Flattening an Object Algebra to Provide Performance Algebraic transformation and optimization techniques have been the method of choice in relational query execution, but applying them in OODBMS has been difficult due to the complexity of object-oriented query languages. This paper demonstrates that the problem can be simplified by mapping a complex storage model to the flat binary model implemented by Monet, a state-of-theart database kernel. We present a generic mapping scheme to flatten data models and study the case of a straightforward object-oriented model. We show how flattening enabled us to implement a full-fledged query algebra on it, using only a very limited set of simple operations. The required primitives and query execution strategies are discussed, and their performance is evaluated on the 1GB TPC-D benchmark, showing that our divide-and-conquer approach yields excellent results. 1 Introduction  During the last decade, relational database technology has grown towards industrial maturity, and the attention of the research...",
    "neighbors": [
      364,
      452,
      1057,
      1128
    ],
    "mask": "Train"
  },
  {
    "node_id": 79,
    "label": 1,
    "text": "A Neuro-Fuzzy Solution for Integrated Visual and Force Control In this paper the use of a B-spline neuro-fuzzy model for different tasks such as vision-based fine-positioning using uncalibrated cameras and force control is presented. It is shown that neuro-fuzzy controllers can be used not only for low-dimensional problems like force control but also for high-dimensional problems like vision-based sensorimotor control and for fusing input from different sensors. Controllers of this type can be modularly combined to solve a given assembly problem.  1 Introduction  It is well-known that general fuzzy rule descriptions of systems with a large number of input variables suffer from the problem of the \"curse of dimensionality.\" In many realworld applications it is difficult to identify the decisive input parameters and thus to reduce the number of input variables to the minimum. A general solution to building fuzzy models is not only interesting from a theoretical point, it may also extend the range of applications of fuzzy control to more complex intel...",
    "neighbors": [
      1121
    ],
    "mask": "Train"
  },
  {
    "node_id": 80,
    "label": 2,
    "text": "A Non-obtrusive User Interface for Increasing Social Awareness on the World Wide Web Arguing for the need of increasing social awareness on the World Wide Web, we describe a user interface based on the metaphor of windows bridging electronic and physical spaces. We present a system that, with the aim of making on-line activity perceptible in the physical world, makes it possible to hear people visiting one's Web site. The system takes advantage of the seamless and continuous network connection offered by handheld Web-appliances such as PDA's.",
    "neighbors": [
      1214
    ],
    "mask": "Train"
  },
  {
    "node_id": 81,
    "label": 4,
    "text": "The Morph Node We discuss potential and limitations of a Morph Node, inspired by the corresponding construct in Java3D. A Morph Node in Java3D interpolates vertex attributes among several homeomorphic geometries. This node is a promising candidate for the delivery of 3D animation in a very compact form. We review the state-of-the-art in Web 3D techniques, allowing for the possibility of interpolating among several geometries. This review leads to a simple extension for VRML-97 as well as a recommendation for necessary changes in Java3D. Furthermore, we discuss various optimization issues for Morph Nodes.  CR Categories and Subject Descriptors. I.3.6 [Computer Graphics ] Methodology and Techniques: Standards - VRML; I.3.7 [Computer Graphics] Three Dimensional Graphics and Realism: Animation; I.3.8 [Computer Graphics] Applications.  Additional Keywords. Animation, Avatars, Morphing, Virtual Humans, VRML.  INTRODUCTION  Animation of three-dimensional shapes involves the change of vertex attributes over ...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 82,
    "label": 0,
    "text": "Combinations of Modal Logics  Combining logics for modelling purposes has become a rapidly expanding enterprise that is inspired mainly by concerns about modularity and the wish to join together different kinds of information. As any interesting real world system is a complex, composite entity, decomposing its descriptive requirements (for design, verification, or maintenance purposes) into simpler, more restricted, reasoning tasks is not only appealing but is often the only plausible way forward. It would be an exaggeration to claim that we currently have a thorough understanding of `combined methods.' However, a core body of notions, questions and results has emerged for an important class of combined logics, and we are beginning to understand how this core theory behaves when it is applied outside this particular class.  In this paper we will consider the combination of modal (including temporal) logics, identifying leading edge research that we, and others, have carried out. Such combined sys",
    "neighbors": [
      159,
      327,
      644
    ],
    "mask": "Train"
  },
  {
    "node_id": 83,
    "label": 1,
    "text": "A Rule Induction Approach to Modeling Regional Pronunciation Variation. This 1)~q)er descril)es the use of rule indue-tion techniques fi)r the mli;omatic exl;ra(:l;ion of l)honemic knowledge mM rules fl'om pairs of l:,romm(:intion lexi(:a. This (:xtra(:ted knowl-edge allows the ndat)tntion of sl)ee(:h pro(:ess-ing systelns tO regional vm'iants of a language. As a case sl;u(ty, we apply the approach to Northern Dutch and Flemish (the wtriant of Dutch spoken in Flan(lers, a t)art; of Bel-gium), based Oll C(?lex and l'bnilex, promm-clarion lexi(:a tbr Norttmrn l)utch mM Fhm,-ish, r(~sl)e(:tively. In our study, we (:omt)ar(~ l;wo rule ilMu(:tion techniques, franslbrmation-B;tsed Error-l)riven Learning ('I'I/E])I,) (Brill, 1995) mM C5.0 (Quinl~m, 1993), and (,valu-ate the extr~tct(xl knowh;dge quanl:it~l;ively (a(:-(:ura.cy) mM qualitatively (linguistic r(;levanc:e of the rules). We (:onchMe that. whereas classificntion-1)ased rule. induct;ion with C5.0 is 11101.'0 a(;(:(lr&l;e ~ th(? |;rallSt~)rnl;~l;ion l&quot;ules le;~rne(t with TBE1)I, can 1)e more easily ini;ert)reted. 1.",
    "neighbors": [
      1168
    ],
    "mask": "Train"
  },
  {
    "node_id": 84,
    "label": 2,
    "text": "A World Wide Web Meta Search Engine Using an Automatic Query Routing Algorithm CONTENTS 1. INTRODUCTION......................................................... ........................................ 6 2. LITERATURE REVIEW............................................................... ........................ 9 2.1 Overview of conventional search techniques............................................. 9 2.2 Conventional query routing systems......................................................... 11 2.2.1 Manual query routing services......................................................... 11 2.2.2 Automated query routing systems based on centroids..................... 12 2.2.3 Automated query routing systems without centroids....................... 12 3. SYSTEM STRUCTURE............................................................ .......................... 14 3.1 System overview............................................................. .......................... 14 3.2 Off-line operations.......................................................",
    "neighbors": [
      410,
      1032
    ],
    "mask": "Train"
  },
  {
    "node_id": 85,
    "label": 1,
    "text": "Advanced DeInterlacing techniques with the use of Zonal Based Algorithms This paper describes a new highly efficient aleinterlacing approach based on motion estimation and compensation techniques. The proposed technique mainly benefits from the motion vector properties of zonal based algorithms, such as the Advanced Predictive Diamond Zonal Search (APDZS) and the Predictive Motion Vector Field Adaptive Search Technique (PMVFAST), multihypothesis motion compensation, but also an additional motion classification phase where, depending on the motion of a pixel, additional spatial and temporal information is also considered to further improve performance. Extensive simulations demonstrate the efficacy of these algorithms, especially when compared to standard deinterlacing techniques such as the line doubling and line averaging algorithms.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 86,
    "label": 4,
    "text": "LivingLab: A white paper The LivingLab is a planned research infrastructure that is pivotal for user-system interaction research in the next decade. This article presents the concept and outlines a research programme that will be served by this facility. These future plans are motivated by a vision of future developments concerning interaction with intelligent environments.",
    "neighbors": [
      684
    ],
    "mask": "Train"
  },
  {
    "node_id": 87,
    "label": 4,
    "text": "Let's Browse: A Collaborative Browsing Agent Web browsing, like most of today's desktop applications, is usually a solitary activity. Other forms of media, such as watching television, are often done by groups of people, such as families or friends. What would it be like to do collaborative Web browsing? Could the computer provide assistance to group browsing by trying to help find mutual interests among the participants? Let's Browse is an experiment in building an agent to assist a group of people in browsing, by suggesting new material likely to be of common interest. It is built as an extension to the single user Web browsing agent Letizia. Let's Browse features automatic detection of the presence of users, automated \"channel surfing\" browsing, and dynamic display of the user profiles and explanation of recommendations. # 1999 Elsevier Science B.V. All rights reserved.  Keywords: Browsing; Collaboration; Agents; User profiles  1. Collaborative browsing  Increasingly, Web browsing will be performed in collaborative  settings, ...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 88,
    "label": 3,
    "text": "Typechecking for XML Transformers We study the typechecking problem for XML transformers: given an XML transformation program and a DTD for the input XML documents, check whether every result of the program conforms to a specified output DTD. We model XML transformers using a novel device called a k- pebble transducer, that can express most queries without data-value joins in XML-QL, XSLT, and other XML query languages. Types are modeled by regular tree languages, a robust extension of DTDs. The main result of the paper is that typechecking for k-pebble transducers is decidable. Consequently, typechecking can be performed for a broad range of XML transformation languages, including XMLQL and a fragment of XSLT.  1. INTRODUCTION  Traditionally, database query languages have focused on data retrieval, with complex data transformations left to applications. The new XML data exchange standard for the Web, and emerging applications requiring data wrapping and integration, have shifted the focus towards data transformations....",
    "neighbors": [
      585
    ],
    "mask": "Test"
  },
  {
    "node_id": 89,
    "label": 5,
    "text": "BISMARC: A Biologically Inspired System for Map-based Autonomous Rover Control As the complexity of the missions to planetary surfaces increases, so too does the need for autonomous rover systems. This need is complicated by the power, mass and computer storage restrictions on such systems (Miller, 1992). To address these problems, we have recently developed a system called BISMARC (Biologically Inspired System for Map-based Autonomous Rover Control) for planetary missions involving multiple small, lightweight surface rovers (Huntsberger, 1997). BISMARC is capable of cooperative planetary surface retrieval operations such as a multiple cache recovery mission to Mars. The system employs autonomous navigation techniques, behavior-based control for surface retrieval operations, and an action selection mechanism based on a modified form of free flow hierarchy (Rosenblatt and Payton, 1989). This paper primarily describes the navigation and map-mapping subsystems of BISMARC. They are inspired by some recent studies of London taxi drivers indicating that the right hippo...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 90,
    "label": 4,
    "text": "Contract-Net-Based Learning in a User-Adaptive Interface Agency . This paper describes a multi-agent learning approach to adaptation to users' preferences realized by an interface agency. Using a contract-net-based negotiation technique, agents as contractors as well as managers negotiate with each other to pursue the overall goal of dynamic user adaptation. By learning from indirect user feedback, the adjustment of internal credit vectors and the assignment of contractors that gained maximal credit with respect to the user's current preferences, the preceding session, and current situational circumstances can be realized. In this way, user adaptation is achieved without accumulating explicit user models but by the use of implicit, distributed user models. 1 Introduction Interface agents are computer programs that enhance the human-computer interaction by mediating a relationship between technical systems and users [Lau90]. On the one hand, they provide assistance to users by acting on his/her behalf and automating his/her actions [Nor94...",
    "neighbors": [
      432
    ],
    "mask": "Test"
  },
  {
    "node_id": 91,
    "label": 1,
    "text": "On the Correspondence between Neural Folding Architectures and Tree Automata The folding architecture together with adequate supervised training algorithms is a special recurrent neural network model designed to solve inductive inference tasks on structured domains. Recently, the generic architecture has been proven as a universal approximator of mappings from rooted labeled ordered trees to real vector spaces. In this article we explore formal correspondences to the automata (language) theory in order to characterize the computational power (representational capabilities) of different instances of the generic folding architecture. As the main result we prove that simple instances of the folding architecture have the computational power of at least the class of deterministic bottom-up tree automata. It is shown how architectural constraints like the number of layers, the type of the activation functions (first-order vs. higher-order) and the transfer functions (threshold vs. sigmoid) influence the representational capabilities. All proofs are carried out in a c...",
    "neighbors": [
      46,
      1223
    ],
    "mask": "Validation"
  },
  {
    "node_id": 92,
    "label": 3,
    "text": "On Computational Representations of Herbrand Models . Finding computationally valuable representations of models of predicate logic formulas is an important issue in the field of automated theorem proving, e.g. for automated model building or semantic resolution. In this article we treat the problem of representing single models independently of building them and discuss the power of different mechanisms for this purpose. We start with investigating context-free languages for representing single Herbrand models. We show their computational feasibility and prove their expressive power to be exactly the finite models. We show an equivalence with \"ground atoms and ground equations\" concluding equal expressive power. Finally we indicate how various other well known techniques could be used for representing essentially infinite models (i.e. models of not finitely controllable formulas), thus motivating our interest in relating model properties with syntactical properties of corresponding Herbrand models and in investigating connections betwe...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 93,
    "label": 3,
    "text": "Integrating Spatial Information And Image Analysis - One Plus One Makes Ten Photogrammetry and remote sensing have proven their efficiency for spatial data collection in many ways. Interactive mapping at digital workstations is performed by skilled operators, which guarantees excellent quality in particular of the geometric data. In this way, worldwide acquisition of a large number of national GIS databases has been supported and still a lot of production effort is devoted to this task. In the field of image analysis, it has become evident that algorithms for scene interpretation and 3D reconstruction of topographic objects, which rely on a single data source, cannot function efficiently. Research in two directions promises to be more successful. Multiple, largely complementary, sensor data like range data from laser scanners, SAR and panchromatic or multi-/hyper-spectral aerial images have been used to achieve robustness and better performance in image analysis. On the other hand, given GIS databases, e.g. layers from topographic maps, can be considered as vi...",
    "neighbors": [
      339
    ],
    "mask": "Validation"
  },
  {
    "node_id": 94,
    "label": 1,
    "text": "Naive Bayes for Regression Abstract. Despite its simplicity, the naive Bayes learning scheme performs well on most classification tasks, and is often significantly more accurate than more sophisticated methods. Although the probability estimates that it produces can be inaccurate, it often assigns maximum probability to the correct class. This suggests that its good performance might be restricted to situations where the output is categorical. It is therefore interesting to see how it performs in domains where the predicted value is numeric, because in this case, predictions are more sensitive to inaccurate probability estimates. This paper shows how to apply the naive Bayes methodology to numeric prediction (i.e., regression) tasks by modeling the probability distribution of the target value with kernel density estimators, and compares it to linear regression, locally weighted linear regression, and a method that produces \u201cmodel trees\u201d\u2014decision trees with linear regression functions at the leaves. Although we exhibit an artificial dataset for which naive Bayes is the method of choice, on real-world datasets it is almost uniformly worse than locally weighted linear regression and model trees. The comparison with linear regression depends on the error measure: for one measure naive Bayes performs similarly, while for another it is worse. We also show that standard naive Bayes applied to regression problems by discretizing the target value performs similarly badly. We then present empirical evidence that isolates naive Bayes \u2019 independence assumption as the culprit for its poor performance in the regression setting. These results indicate that the simplistic statistical assumption that naive Bayes makes is indeed more restrictive for regression than for classification.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 95,
    "label": 1,
    "text": "Applying Parallelism to Improve Genetic Algorithm-based Design Optimization Introduction  The abundance of powerful workstations makes course-grained parallelization an obvious enhancement to many optimization techniques, including genetic algorithms [Gol89, DM97]. While initial modifications have been made to GADO (Genetic Algorithm for Design Optimization [Ras98, RHG97]), such changes have not been carefully analyzed for potential impacts on quality. More generally, parallelization has the potential to improve GA performance through the use of alternative models of computation. Parallelism can certainly reduce the total elapsed clock-time for a solution, but as a change in model of computation (either real or simulated) , it can change the number of simulator calls and even make new solutions achievable. The effects of parallelization on GADO were investigated during my summer internship at the Center for Computational Design. 2 Objectives  Since a straightforward parallelized implementation already existed, my first tasks were to ana",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 96,
    "label": 0,
    "text": "An Intelligent Agent Framework In VRML Worlds actions, e.g. move to next room, are received by it and consequently send to the EAC. Finally, the abstract action arrives at the Virtual Reality Management Unit that specifies in detail the received actions. It provides specific values concerning the orientation and position of the avatar, e.g. it specifies the coordinates, orientation and path so that it can successfully move to the next room, and sends them as commands to the Virtual Reality World Browser. The browser executes the command by altering the virtual environment appropriately. When changes have been performed the AEC unit notifies the logical core that the action has been successfully executed and the logical core goes on by updating its internal and external state. Consequently, the agent looks around into the virtual space, gathers any additional information and decides the next step it should take to satisfy its goals.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 97,
    "label": 1,
    "text": "Model-Free Least-Squares Policy Iteration We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration. Our method is model-free and completely off policy. We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efficient use of sample experiences compared to pure temporal difference algorithms. LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems. Moreover, approximations learned by LSTD are strongly influenced by the visitation distribution over states. Our new algorithm, Least-Squares Policy Iteration (LSPI) addresses these issues. The result is an off-policy method which can use (or reuse) data collected from any source. We test LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efficiently by merely observing a relatively small number of completely random trials.",
    "neighbors": [
      13,
      1238
    ],
    "mask": "Train"
  },
  {
    "node_id": 98,
    "label": 3,
    "text": "The BASIS System: a Benchmarking Approach for Spatial Index Structures This paper describes the design of the BASIS prototype system, which is currently under implementation. BASIS stands for Benchmarking Approach for Spatial Index Structures. It is a prototype system aiming at performance evaluation of spatial access methods and query processing strategies, under different data sets, various query types, and different workloads. BASIS is based on a modular architecture, composed of a simple storage manager, a query processor, and a set of algorithmic techniques to facilitate benchmarking. The main objective of BASIS is twofold: (i) to provide a benchmarking environment for spatial access methods and related query evaluation techniques, and (ii) to allow comparative studies of spatial access methods in different cases but under a common framework. We currently extend it to support the fundamental features of spatiotemporal data management and access methods.",
    "neighbors": [
      1058
    ],
    "mask": "Train"
  },
  {
    "node_id": 99,
    "label": 4,
    "text": "Exploration of Perceptual Computing for Smart-Its The future success of ubiquitous computing depends to a big part on how well applications can adapt to their environment and act accordingly. This thesis has set itself the goal of exploring perceptual computing for Smart-Its, which is one such ubiquitous computing vision.",
    "neighbors": [
      1006
    ],
    "mask": "Validation"
  },
  {
    "node_id": 100,
    "label": 2,
    "text": "Image Retrieval: Current Techniques, Promising Directions And Open Issues This paper provides a comprehensive survey of the technical achievements in the research area of image retrieval, especially content-based image retrieval, an area that has been so active and prosperous in the past few years. The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multidimensional indexing, and system design, three of the fundamental bases of content-based image retrieval. Furthermore, based on the state-of-the-art technology available now and the demand from real-world applications, open research issues are identified and future promising research directions are suggested. C \u25cb 1999 Academic Press 1.",
    "neighbors": [
      118,
      523,
      1203
    ],
    "mask": "Validation"
  },
  {
    "node_id": 101,
    "label": 3,
    "text": "Scalable Trigger Processing +  Current database trigger systems have extremely limited scalability. This paper proposes a way to develop a truly scalable trigger system. Scalability to large numbers of triggers is achieved with a trigger cache to use main memory effectively, and a memory-conserving selection predicate index based on the use of unique expression formats called expression signatures. A key observation is that if a very large number of triggers are created, many will have the same structure, except for the appearance of different constant values. When a trigger is created, tuples are added to special relations created for expression signatures to hold the trigger's constants. These tables can be augmented with a database index or main-memory index structure to serve as a predicate index. The design presented also uses a number of types of concurrency to achieve scalability, including token (tuple)-level, condition-level, rule action-level, and datalevel concurrency.  1. Introduction  Trigger feature...",
    "neighbors": [
      475,
      651
    ],
    "mask": "Train"
  },
  {
    "node_id": 102,
    "label": 1,
    "text": "Robust Entropy Estimation Strategies Based on Edge Weighted Random Graphs (with corrections) In this paper we treat the problem of robust entropy estimation given a multidimensional random sample from an unknown distribution. In particular, we consider estimation of the Renyi entropy of fractional order which is insensitive to outliers, e.g. high variance contaminating distributions, using the k-point minimal spanning tree (kMST) . A greedy algorithm for approximating the NP-hard problem of computing the k-minimal spanning tree is given which is a generalization of the potential function partitioning method of Ravi etal.  1  The basis for our approach is an asymptotic theorem establishing that the log of the overall length or weight of the greedy approximation is a strongly consistent estimator of the Renyi entropy. Quantitative robustness of the estimator to outliers is established using Hampel's method of inuence functions.  2  The structure of the inuence function indicates that the k-MST is a natural extension of the one dimensional -trimmed mean for multi-dimensional...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 103,
    "label": 3,
    "text": "Using Java and CORBA for Implementing Internet Databases We describe an architecture called WebFINDIT that allows dynamic couplings of Web accessible databases based on their content and interest. We propose an implementation using WWW, Java, JDBC, and CORBA's ORBs that communicate via the CORBA's IIOP protocol. The combination of these technologies offers a compelling middleware infrastructure to implement wide-area enterprise applications. In addition to a discussion of WebFINDIT's core concepts and implementation architecture, we also discuss an experience of using WebFINDIT in a healthcare application.  1 Introduction  The growth of the Internet and the Web increased dramatically the need for data sharing. The Web has brought a wave of new users and service providers to the Internet. It contains a huge quantity of heterogeneous information and services (e.g., home pages, online digital libraries, product catalogs, and so on) (Bouguettaya et al. 1998). The result is that the Web is now accepted as the de facto support in all domains of li...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 104,
    "label": 0,
    "text": "Cross Entropy Guided Ant-like Agents Finding Dependable Primary/Backup Path Patterns in Networks Telecommunication network owners and operators have for half a century been well aware of the potential loss of revenue if a major trunk is damaged, thus dependability at high cost has been implemented. A simple, effective and common dependability scheme is 1:1 protection with 100% capacity redundancy in the network. A growing number of applications in need of dependable connections with specific requirements to bandwidth and delay have started using the internet (which only provides best effort transport) as their base communication service. In this paper we adopt the 1:1 protection scheme and incorporate it as part of a routing system applicable for internet infrastructures. 100% capacity redundancy is no longer required. A distributed stochastic path finding (routing) algorithm based on Rubinstein's Cross Entropy method for combinatorial optimisation is presented. Early results from Monte Carlo simulations indeed indicate that the algorithm is capable of finding pairs of independent primary and backup paths satisfying specific bandwidth a constraints.",
    "neighbors": [
      967
    ],
    "mask": "Train"
  },
  {
    "node_id": 105,
    "label": 1,
    "text": "Bayesian Representations and Learning Mechanisms for Content-Based Image Retrieval We have previously introduced a Bayesian framework for content-based image retrieval (CBIR) that relies on a generative model for feature representation based on embedded mixtures. This is a truly generic image representation that can jointly model color and texture and has been shown to perform well across a broad spectrum of image databases. In this paper, we expand the Bayesian framework along two directions.  First, we show that the formulation of CBIR as a problem of Bayesian inference leads to a natural criteria for evaluating local image similarity without requiring any image segmentation. This allows the practical implementation of retrieval systems where users can provide image regions, or objects, as queries. Region-based queries are significantly less ambiguous than queries based on entire images leading to significant improvements in retrieval precision.  Second, we present a Bayesian learning algorithm that relies on belief propagation to integrate feedback provided by the...",
    "neighbors": [
      961
    ],
    "mask": "Train"
  },
  {
    "node_id": 106,
    "label": 0,
    "text": "Specification of Heterogeneous Agent Architectures . Agent-based software applications need to incorporate agents having  heterogeneous architectures in order for each agent to optimally perform its  task. HEMASL is a simple meta-language used to specify intelligent agents and  multi-agent systems when different and heterogeneous agent architectures must  be used. HEMASL specifications are based on an agent model that abstracts several  existing agent architectures. The paper describes some of the features of the  language, presents examples of its use and outlines its operational semantics. We  argue that adding HEMASL to CaseLP, a specification and prototyping environment  for MAS, can enhance its flexibility and usability.  1 Introduction  Intelligent agents and multi-agent systems (MAS) are increasingly being acknowledged as the \"new\" modelling techniques to be used to engineer complex and distributed software applications [17, 9]. Agent-based software development is concerned with the realization of software applications modelled ...",
    "neighbors": [
      485,
      884,
      964,
      1222
    ],
    "mask": "Test"
  },
  {
    "node_id": 107,
    "label": 1,
    "text": "Comparing Evolutionary Programs and Evolutionary Pattern Search Algorithms: A Drug Docking Application Evolutionary programs (EPs) and evolutionary pattern search algorithms (EPSAs) are two general classes of evolutionary methods for optimizing on continuous domains. The relative performance of these methods has been evaluated on standard global optimization test functions, and these results suggest that EPSAs more robustly converge to nearoptimal solutions than EPs. In this paper we evaluate the relative performance of EPSAs and EPs on a real-world application: flexible ligand binding in the Autodock docking software. We compare the performance of these methods on a suite of docking test problems. Our results confirm that EPSAs and EPs have comparable performance, and they suggest that EPSAs may be more robust on larger, more complex problems. 1 Introduction  Evolutionary programs (EPs) and evolutionary pattern search algorithms (EPSAs) are two classes of evolutionary algorithms (EAs) that have been specifically developed for solving problems of the form min  x2R  n  f(x): In particula...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 108,
    "label": 1,
    "text": "Making Use of Population Information in Evolutionary Artificial Neural Networks This paper is concerned with the simultaneous evolution of artificial neural network (ANN) architectures and weights. The current practice in evolving ANN's is to choose the best ANN in the last generation as the final result. This paper proposes a different approach to form the final result by combining all the individuals in the last generation in order to make best use of all the information contained in the whole population. This approach regards a population of ANN's as an ensemble and uses a combination method to integrate them. Although there has been some work on integrating ANN modules [2], [3], little has been done in evolutionary learning to make best use of its population information. Four linear combination methods have been investigated in this paper to illustrate our ideas. Three real-world data sets have been used in our experimental studies, which show that the recursive least-square (RLS) algorithm always produces an integrated system that outperforms the best individual. The results confirm that a population contains more information than a single individual. Evolutionary learning should exploit such information to improve generalization of learned systems.",
    "neighbors": [
      504
    ],
    "mask": "Train"
  },
  {
    "node_id": 109,
    "label": 4,
    "text": "An Agent-Based Approach to the Construction of Floristic Digital Libraries This paper describes an agent-assisted approach to the construction of floristic digital libraries, which consist of very large botanical data repositories and related services. We propose an environment, termed Chrysalis, in which authors of plant morphologic descriptions can enter data into a digital library via a web-based editor. An agent that runs concurrently with the editor suggests potentially useful morphologic descriptions based on similar documents existing in the library. Benefits derived from the introduction of Chrysalis include reduced potential for errors and data inconsistencies, increased parallelism among descriptions, and considerable savings in the time regularly spent in visually checking for parallelism and manually editing data.  KEYWORDS: agents, agent-based interfaces, floristic digital libraries, FNA, Chrysalis.  INTRODUCTION  Constructing the vast data repositories that will support knowledge-intensive activities in digital libraries poses problems of enormo...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 110,
    "label": 0,
    "text": "Response Generation in Collaborative Negotiation In collaborative planning activities, since the agents are autonomous and heterogeneous, it is inevitable that conflicts arise in their beliefs during the planning process. In cases where such conflicts are relevant to the task at hand, the agents should engage in collaborative negotiation  as an attempt to square away the discrepancies in their beliefs. This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution. Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise, and of selecting appropriate evidence to justify the need for such modification. Furthermore, by capturing the negotiation process in a recursive  Propose-Evaluate-Modify cycle of actions, our model can successfully handle embedded negotiation subdialogues.  1 Introduction  In collaborative consultat...",
    "neighbors": [
      1251
    ],
    "mask": "Validation"
  },
  {
    "node_id": 111,
    "label": 3,
    "text": "View Security as the Basis for Data Warehouse Security Access  .  permissions in a data warehouse are  currently managed in a separate world from  the sources' policies. The consequences are  inconsistencies, slow response to change, and  wasted administrative work. We present a  different approach, which treats the sources'  exported tables and the warehouse as part of  the same distributed database. Our main result  is a way to control derived products by  extending SQL grants rather than creating  entirely new mechanisms. We provide a  powerful, sound inference theory that derives  permissions on warehouse tables (both  materialized and virtual), making the system  easier to administer and its applications more  robust. We also propose a new permission  construct suitable for views that filter data  from mutually-suspicious parties.  1 Introduction  A key challenge for data warehouse security is how to manage the entire system coherently -- from sources and their export tables, to warehouse stored tables  (conventional and cubes) and vi...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 112,
    "label": 2,
    "text": "Web Mining Research: A Survey With the huge amount of information available online, the World Wide Web is a fertile area for data mining research. The Web mining research is at the cross road of research from several research communities, such as database, information retrieval, and within AI, especially the sub-areas of machine learning and natural language processing. However, there is a lot of confusions when comparing research efforts from different point of views. In this paper, we survey the research in the area of Web mining, point out some confusions regarded the usage of the term Web mining and suggest three Web mining categories. Then we situate some of the research with respect to these three categories. We also explore the connection between the Web mining categories and the related agent paradigm. For the survey, we focus on representation issues, on the process, on the learning algorithm, and on the application of the recent works as the criteria. We conclude the paper with some research issues.",
    "neighbors": [
      216,
      411,
      774,
      855,
      933,
      1017,
      1120
    ],
    "mask": "Train"
  },
  {
    "node_id": 113,
    "label": 2,
    "text": "Digital Library Resources as a Basis for Collaborative Work The creation of large, networked, digital document resources has greatly facilitated information access and dissemination. We suggest that such resources can further enhance how we work with information, namely, that they can provide a substrate that supports collaborative work. We focus on one form of collaboration, annotation, by which we mean any of an open-ended number of creative document manipulations which are useful to record and to share with others.  Widespread digital document dissemination required technological enablers, such as web clients and servers. The resulting infrastructure is one in which information may be widely shared by individuals across administrative boundaries. To achieve the same ubiquitous availability for annotation requires providing support for spontaneous collaboration, that is, for collaboration across administrative boundaries without significant prior agreements. Annotation is not more commonplace, we suggest, because the technological needs of sp...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 114,
    "label": 0,
    "text": "Compiling for Fast State Capture of Mobile Agents Saving, transporting, and restoring the state of a mobile agent is one of the main problems in implementing a mobile agents system. We present an approach, implemented as part of our Messengers system, that represents a trade-off between the unrestricted use of pointers and the ability to perform fully transparent state capture. When writing the code for an agent, the programmer has a choice between two types of functions. C functions are fully general and may use unrestricted pointers, but they are not allowed to invoke any migration commands. Messengers functions may cause migration but their use of pointers is restricted to only a special type of a dynamic array structure. Under these restrictions, the local variables, the program counter, and the calling stack of an agent can all be made machine-independent and can be captured/restored transparently during migration. 1 Introduction  Saving, transporting, and restoring the state of a mobile agent is one of the main problem in implem...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 115,
    "label": 2,
    "text": "Text Database Selection for Longer Queries A metasearch engine is a system that supports unified access to multiple local search engines. One of the main challenges in building a large-scale metasearch engine is to solve the database (search engine) selection problem, which is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query. For the database of each search engine, a representative which indicates approximately the contents of the database is created in advance to enable database selection. The representatives of all databases can be integrated into a single representative to make the selection process more scalable. While an integrated representative with high scalability has just been proposed and has been found to be effective for short queries, its effectiveness for longer queries is significantly lower. In the Internet environment, most queries initially submitted by users are short queries. However, it has been found that better search effectiveness can often be achieved when additional terms are added to the initial queries through query expansion or relevance feedback. The resulting queries are usually longer than the initial queries. In this paper, we propose a new method to construct database representatives and to decide which databases to select for longer queries. Experimental results are given to compare the performance of the new method with that of a previous method.",
    "neighbors": [
      216,
      224,
      241,
      510,
      696,
      792,
      931,
      1003,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 116,
    "label": 1,
    "text": "Equivalence in Knowledge Representation: Automata, Recurrent Neural Networks, and Dynamical Fuzzy Systems Neurofuzzy systems-the combination of artificial neural networks with fuzzy logic-have become useful in many application domains. However, conventional neurofuzzy models usually need enhanced representation power for applications that require context and state (e.g., speech, time series prediction, control). Some of these applications can be readily modeled as finite state automata. Previously, it was proved that deterministic finite state automata (DFA) can be synthesized by or mapped into recurrent neural networks by directly programming the DFA structure into the weights of the neural network. Based on those results, a synthesis method is proposed for mapping fuzzy finite state automata (FFA) into recurrent neural networks. Furthermore, this mapping is suitable for direct implementation in very large scale integration (VLSI), i.e., the encoding of FFA as a generalization of the encoding of DFA in VLSI systems. The synthesis method requires FFA to undergo a transformation prior to being mapped into recurrent networks. The neurons are provided with an enriched functionality in order to accommodate a fuzzy representation of FFA states. This enriched neuron functionality also permits fuzzy parameters of FFA to be directly represented as parameters of the neural network. We also prove the stability of fuzzy finite state dynamics of the constructed neural networks for finite values of network weight and, through simulations, give empirical validation of the proofs. Hence, we prove various knowledge equivalence representations between neural and fuzzy systems and models of automata.",
    "neighbors": [
      740
    ],
    "mask": "Train"
  },
  {
    "node_id": 117,
    "label": 4,
    "text": "SIDE Surfer: a Spontaneous Information Discovery and Exchange System Development of wireless communications enables the rise of networking applications in embedded systems. Web interactions, which are the most spread, are nowadays available on wireless PDAs. Moreover, we can observe a development of ubiquitous computing. Based on this concept, many works aim to consider user's context as part of the parameters of the applications. The context notion can include the user's location, his social activity . . . Taking part from emerging technologies enabling short range and direct wireless communications (which allow to define a proximity context), the aim of our study is to design a new kind of application, extending the Web paradigm: spontaneous and proximate Web interactions.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 118,
    "label": 2,
    "text": "MetaSEEk: A Content-Based Meta-Search Engine for Images Search engines are the most powerful resources for finding information on the rapidly expanding World Wide Web (WWW). Finding the desired search engines and learning how to use them, however, can be very time consuming. The integration of such search tools enables the users to access information across the world in a transparent and efficient manner. These systems are called meta-search engines. The recent emergence of visual information retrieval (VIR) search engines on the web is leading to the same efficiency problem. This paper describes and evaluates MetaSEEk, a content-based meta-search engine used for finding images on the Web based on their visual information. MetaSEEk is designed to intelligently select and interface with multiple on-line image search engines by ranking their performance for different classes of user queries. User feedback is also integrated in the ranking refinement. We compare MetaSEEk with a base line version of meta-search engine, which does not use the past performance of the different search engines in recommending target search engines for future queries.",
    "neighbors": [
      100,
      768,
      781,
      931,
      1203
    ],
    "mask": "Train"
  },
  {
    "node_id": 119,
    "label": 0,
    "text": "MARS: a Programmable Coordination Architecture for Mobile Agents Mobile agents represent a promising technology for the development of Internet  applications. However, mobile computational entities introduce peculiar problems  w.r.t. the coordination of the application components. The paper outlines the  advantages of Linda-like coordination models, and shows how a programmable  coordination model based on reactive tuple spaces can provide further desirable  features for Internet applications based on mobile agents. Accordingly, the paper  presents the design and the implementation of the MARS coordination architecture for  Java-based mobile agents. MARS defines Linda-like tuple spaces, which can be  programmed to react with specific actions to the accesses made by mobile agents.",
    "neighbors": [
      246,
      308,
      416,
      757
    ],
    "mask": "Train"
  },
  {
    "node_id": 120,
    "label": 1,
    "text": "Projective Rotations applied to a Pan-Tilt Stereo Head A non-metric pan-tilt stereo-head consists of a weakly calibrated stereo rig mounted on a pan-tilt mechanism. It is called non-metric since neither the kinematics of the mechanism, nor camera calibration are required. The Lie group of \"projective rotations\"- homographies of projective space corresponding to pure rotations -- is an original formalism to model the geometry of such a pan-tilt system. A Rodrigues alike formula as well as a minimal parameterization of projective rotations are introduced. Based on this, the practical part devises a numerical optimization technique for accurately estimating projective rotations from point correspondences, only. This procedure recovers sufficient geometry to operate the system. The experiments validate and evaluate the proposed approach on real image data. They show the weak calibration, image prediction, and homing of a non-metric pan-tilt head.  1 Introduction  One of the most useful sensors in computer vision is a pan and tilt stereo head. ...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 121,
    "label": 0,
    "text": "Cooperating Mobile Agents for Dynamic Network Routing this paper we present a contrasting model, a dynamic, wireless, peer to peer network with routing tasks performed in a decentralized and distributed fashion by mobile software agents that cooperate to accumulate and distribute connectivity information. Our agents determine system topology by exploring the network, then store this information in the nodes on the network. Other agents use this stored information to derive multi-hop routes across the network. We study these algorithms in simulation as an example of using populations of mobile agents to manage networks",
    "neighbors": [
      38
    ],
    "mask": "Validation"
  },
  {
    "node_id": 122,
    "label": 3,
    "text": "Functional Join Processing . Inter-object references are one of the key concepts of object-relational and object-oriented database systems. In this work, we investigate alternative techniques to implement inter-object references and make the best use of them in query processing, i.e., in evaluating functional joins. We will give a comprehensive overview and performance evaluation of all known techniques for simple (singlevalued) as well as multi-valued functional joins. Furthermore, we will describe special order-preserving functionaljoin techniques that are particularly attractive for decision support queries that require ordered results. While most of the presentation of this paper is focused on object-relational and object-oriented database systems, some of the results can also be applied to plain relational databases because index nested-loop joins along key/foreign-key relationships, as they are frequently found in relational databases, are just one particular way to execute a functional join.  Key words: O...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 123,
    "label": 5,
    "text": "Managing Robot Autonomy and Interactivity Using Motives and Visual Communication An autonomous mobile robot operating in everyday life conditions will have to face a huge variety of situations and to interact with other agents (living or artificial). Such a robot needs flexible and robust methods for managing its goals and for adapting its control mechanisms to face the contingencies of the world. It also needs to communicate with others in order to get useful information about the world. This paper describes an approach based on a general architecture and on internal variables called `motives' to manage the goals of an autonomous robot. These variables are also used as a basis for communication using a visual communication system. Experiments using a vision- and sonar-based Pioneer I robot, equipped with a visual signaling device, are presented.  1 Introduction  Designing an autonomous mobile robot to operate in unmodified environments, i.e., environments that have not been specifically engineered for the robot, is a very challenging problems. Dynamic and unpredic...",
    "neighbors": [
      1137,
      1231
    ],
    "mask": "Train"
  },
  {
    "node_id": 124,
    "label": 4,
    "text": "The Cub-e, a Novel Virtual 3D Display Device We have designed, and are in the process of building, a visualisation device,  the Cub-e. The Cub-e consists of six TFT screens, arranged in a perspex cube,  with a StrongARM processor and batteries inside. It is a multipurpose device  with applications including teleconferencing, interaction with virtual worlds, and  games.  1",
    "neighbors": [
      12
    ],
    "mask": "Validation"
  },
  {
    "node_id": 125,
    "label": 2,
    "text": "Automatic Text Representation, Classification and Labeling in European Law The huge text archives and retrieval systems of legal information have not achieved yet the representation in the wellknown subject-oriented structure of legal commentaries. Content-based classification and text analysis remains a high priority research topic. In the joint KONTERM, SOM and LabelSOM projects, learning techniques of neural networks are used to achieve similar high compression rates of classification and analysis like in manual legal indexing. The produced maps of legal text corpora cluster related documents in units that are described with automatically selected descriptors. Extensive tests with text corpora in European case law have shown the feasibility of this approach. Classification and labeling proved very helpful for legal research. The Growing Hierarchical Self-Organizing Map represents very interesting generalities and specialties of legal text corpora. The segmentation into document parts improved very much the quality of labeling. The next challenge would be a change from tfxidf vector representation to a modified vector representation taking into account thesauri or ontologies considering learned properties of legal text corpora.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 126,
    "label": 0,
    "text": "Supporting Conflict Resolution in Cooperative Design Systems Complex modern-day artifacts are designed cooperatively by groups of experts, each with their own areas of expertise. The interaction of such experts inevitably involves conflict. This paper presents an implemented computational model, based on studies of human cooperative design, for supporting the resolution of such conflicts. This model is based centrally on the insights that general conflict resolution expertise exists separately from domain-level design expertise, and that this expertise can be instantiated in the context of particular conflicts into specific advice for resolving those conflicts. Conflict resolution expertise consists of a taxonomy of design conflict classes in addition to associated general advice suitable for resolving conflicts in these classes. The abstract nature of conflict resolution expertise makes it applicable to a wide variety of design domains. This paper describes this conflict resolution model and provides examples of its operation from an implemente...",
    "neighbors": [
      724,
      943
    ],
    "mask": "Train"
  },
  {
    "node_id": 127,
    "label": 5,
    "text": "CMUnited-97: RoboCup-97 Small-Robot World Champion Team Robotic soccer is a challenging research domain which involves multiple agents that need to collaborate in an  adversarial environment to achieve specificobjectives. In this paper, we describe CMUnited, the team of small robotic  agents that we developed to enter the RoboCup-97 competition. We designed and built the robotic agents, devised  the appropriate vision algorithm, and developed and implemented algorithms for strategic collaboration between the  robots in an uncertain and dynamic environment. The robots can organize themselves in formations, hold specific  roles, and pursue their goals. In game situations, they have demonstrated their collaborative behaviors on multiple  occasions. We present an overview of the vision processing algorithm which successfully tracks multiple moving  objects and predicts trajectories. The paper then focusses on the agent behaviors ranging from low-level individual  behaviors to coordinated, strategic team behaviors. CMUnited won the RoboCup-97 small-robot competition at  IJCAI-97 in Nagoya, Japan.",
    "neighbors": [
      155,
      430,
      927,
      1266
    ],
    "mask": "Train"
  },
  {
    "node_id": 128,
    "label": 1,
    "text": "Clustering Large Datasets in Arbitrary Metric Spaces Clustering partitions a collection of objects into groups called clusters, such that similar objects fall into the same group. Similarity between objects is defined by a distance function satisfying the triangle inequality; this distance function along with the collection of objects describes a distance space. In a distance space, the only operation possible on data objects is the computation of distance between them. All scalable algorithms in the literature assume a special type of distance space, namely a k-dimensional vector space, which allows vector operations on objects. We present two scalable algorithms designed for clustering very large datasets in distance spaces. Our first algorithm BUBBLE is, to our knowledge, the first scalable clustering algorithm for data in a distance space. Our second algorithm BUBBLE-FM improves upon BUBBLE by reducing the number of calls to the distance function, which may be computationally very expensive. Both algorithms make only a single scan ov...",
    "neighbors": [
      616,
      1183
    ],
    "mask": "Test"
  },
  {
    "node_id": 129,
    "label": 3,
    "text": "Manipulating Interpolated Data is Easier than You Thought Data defined by interpolation is frequently  found in new applications involving geographical  entities, moving objects, or spatiotemporal  data. These data lead to potentially  infinite collections of items, (e.g., the elevation  of any point in a map), whose definitions  are based on the association of a collection of  samples with an interpolation function. The  naive manipulation of the data through direct  access to both the samples and the interpolation  functions leads to cumbersome or inaccurate  queries. It is desirable to hide the  samples and the interpolation functions from  the logical level, while their manipulation is  performed automatically.  We propose to model such data using infinite  relations (e.g., the map with elevation yields  an infinite ternary relation) which can be manipulated  through standard relational query  languages (e.g., SQL), with no mention of the  interpolated definition. The clear separation  between logical and physical levels ensures the  accu...",
    "neighbors": [
      27,
      147
    ],
    "mask": "Train"
  },
  {
    "node_id": 130,
    "label": 4,
    "text": "Developing a Context-aware Electronic Tourist Guide: Some Issues and Experiences In this paper, we describe our experiences of developing and evaluating GUIDE, an intelligent electronic tourist guide. The GUIDE system has been built to overcome many of the limitations of the traditional information and navigation tools available to city visitors. For example, group-based tours are inherently inflexible with fixed starting times and fixed durations and (like most guidebooks) are constrained by the need to satisfy the interests of the majority rather than the specific interests of individuals. Following a period of requirements capture, involving experts in the field of tourism, we developed and installed a system for use by visitors to Lancaster. The system combines mobile computing technologies with a wireless infrastructure to present city visitors with information tailored to both their personal and environmental contexts. In this paper we present an evaluation of GUIDE, focusing on the quality of the visitors experience when using the system.  Keywords  Mobile c...",
    "neighbors": [
      1252
    ],
    "mask": "Train"
  },
  {
    "node_id": 131,
    "label": 3,
    "text": "Highly Concurrent Shared Storage . Shared storage arrays enable thousands of storage devices to be shared and directly  accessed by end hosts over switched system-area networks, promising databases and filesystems highly  scalable, reliable storage. In such systems, hosts perform access tasks (read and write) and management  tasks (migration and reconstruction of data on failed devices.) Each task translates into multiple  phases of low-level device I/Os, so that concurrent host tasks can span multiple shared devices and  access overlapping ranges potentially leading to inconsistencies for redundancy codes and for data  read by end hosts. Highly scalable concurrency control and recovery protocols are required to coordinate  on-line storage management and access tasks. While expressing storage-level tasks as ACID transactions  ensures proper concurrency control and recovery, such an approach imposes high performance  overhead, results in replication of work and does not exploit the available knowledge about storage  le...",
    "neighbors": [
      48
    ],
    "mask": "Validation"
  },
  {
    "node_id": 132,
    "label": 5,
    "text": "Dynamic Service Matchmaking Among Agents in Open Information Environments Introduction  The amount of services and deployed software agents in the most famous offspring of the Internet, the World Wide Web, is exponentially increasing. In addition, the Internet is an open environment, where information sources, communication links and agents themselves may appear and disappear unpredictably. Thus, an effective, automated search and selection of relevant services or agents is essential for human users and agents as well. We distinguish three general agent categories in the Cyberspace, service providers, service requester,  and middle agents. Service providers provide some type of service, such as finding information, or performing some particular domain specific problem solving. Requester agents need provider agents to perform some service for them. Agents that help locate others are called middle agents[2]. Matchmaking  is the process of finding an appropriate provider for a requester thr",
    "neighbors": [
      34,
      675,
      765
    ],
    "mask": "Train"
  },
  {
    "node_id": 133,
    "label": 2,
    "text": "A Study of Approaches to Hypertext Categorization Hypertext poses new research challenges for text classification. Hyperlinks, HTML  tags, category labels distributed over linked documents, and meta data extracted from related Web sites all provide rich information for classifying hypertext documents. How to appropriately represent that information and automatically learn statistical patterns for solving hypertext classification problems is an open question. This paper seeks a principled approach to providing the answers. Specifically, we define five hypertext regularities which may (or may not) hold in a particular application domain, and whose presence (or absence) may significantly influence the optimal design of a classifier. Using three hypertext datasets and three well-known learning algorithms (Naive Bayes, Nearest Neighbor, and First Order Inductive Learner), we examine these regularities in different domains, and compare alternative ways to exploit them. Our results show that the identification of hypertext regularities in the data and the selection of appropriate representations for hypertext in particular domains are crucial, but seldom obvious, in real-world problems. We find that adding the words in the linked neighborhood to the page having those links (both inlinks and outlinks) were helpful for all our classifiers on one data set, but more harmful than helpful for two out of the three classifiers on the remaining datasets. We also observed that extracting meta data from related Web sites was extremely useful for improving classification accuracy in some of those domains. Finally, the relative performance of the classifiers being tested provided insights into their strengths and limitations for solving classification problems involving diverse and often noisy Web pages.",
    "neighbors": [
      142,
      216,
      280,
      379,
      892,
      1178
    ],
    "mask": "Validation"
  },
  {
    "node_id": 134,
    "label": 1,
    "text": "Comparison of Learning Approaches to Appearance-based 3D Object Recognition with and without cluttered background We re-evaluate the application of Support Vector Machines (SVM) to appearance-based 3D object recognition, by comparing it to two other learning approaches: the system developed at Columbia University (\"Columbia\") and a simple image matching system using a nearest neighbor classifier (\"NNC\").  In a first set of experiments, we compare correct recognition rates of the segmented 3D object images of the COIL database. We show that the performance of the simple \"NNC\" system compares to the more elaborated \"Columbia\" and \"SVM\" systems. Only when the experimental setting is more demanding, i.e. when we reduce the number of views during the training phase, some difference in performance can be observed. In a second set of experiments, we consider the more realistic task of 3D object recognition with cluttered background. Also in this case, we obtain that the performance of the three systems are comparable. Only with the recently proposed black/white background training scheme (\"BW\") applied t...",
    "neighbors": [
      973
    ],
    "mask": "Train"
  },
  {
    "node_id": 135,
    "label": 3,
    "text": "Optional Locking Integrated with Operational Transformation in Distributed Real-Time Group Editors Locking is a standard technique in traditional distributed computing and database systems to ensure data integrity by prohibiting concurrent conflicting updates on shared data objects. Operational transformation is an innovative technique invented by groupware research for consistency maintenance in real-time group editors. In this paper, we will examine and explore the complementary roles of locking and operational transformation in consistency maintenance. A novel optional locking scheme is proposed and integrated with operation transformation to maintain both generic and context-specific consistency in a distributed, interactive, and collaborative environment. The integrated optional locking and operational transformation technique is fully distributed, highly responsive, non-blocking, and capable of avoiding locking overhead in the most common case of collaborative editing.  Keywords: Locking, operational transformation, consistency maintenance, group editors, groupware, distribute...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 136,
    "label": 4,
    "text": "Learning and Tracking Cyclic Human Motion We present methods for learning and tracking human motion in  video. We estimate a statistical model of typical activities from a  large set of 3D periodic human motion data by segmenting these  data automatically into \"cycles\". Then the mean and the principal  components of the cycles are computed using a new algorithm  that accounts for missing information and enforces smooth transitions  between cycles. The learned temporal model provides a  prior probability distribution over human motions that can be used  in a Bayesian framework for tracking human subjects in complex  monocular video sequences and recovering their 3D motion.",
    "neighbors": [
      245
    ],
    "mask": "Validation"
  },
  {
    "node_id": 137,
    "label": 1,
    "text": "Learning Strategy Knowledge Incrementally Modern industrial processes require advanced computer tools that should adapt to the user requirements and to the tasks being solved. Strategy learning consists of automating the acquisition of patterns of actions used while solving particular tasks. Current intelligent strategy learning systems acquire operational knowledge to improve the efficiency of a particular problem solver. However, these strategy learning tools should also provide a way of achieving low-cost solutions according to user-specific criteria. In this paper, we present a learning system, hamlet, which is integrated in a planning architecture, prodigy, and acquires control knowledge to guide prodigy to efficiently produce cost-effective plans. hamlet learns from planning episodes, by explaining why the correct decisions were made, and later refines the learned strategy knowledge to make it incrementally correct with experience.",
    "neighbors": [
      626,
      988
    ],
    "mask": "Train"
  },
  {
    "node_id": 138,
    "label": 1,
    "text": "Automatic Discrimination Among Languages Based on Prosody Alone The development of methods for the automatic identification of languages is motivated both by speech-based applications intended for use in a multi-lingual environment, and by theoretical questions of cross-linguistic variation and similarity. We evaluate the potential utility of two prosodic variables, F 0 and amplitude envelope modulation, in a pairwise language discrimination task. Discrimination is done using a novel neural network which can successfully attend to temporal information at a range of timescales. Both variables are found to be useful in discriminating among languages, and confusion patterns, in general, reflect traditional intonational and rhythmic language classes. The methods employed allow empirical determination of prosodic similarity across languages. Die Entwicklung von Methoden zur automatischen Sprachidentifikation wird motiviert sowohl durch sprach-basierte Anwendungen, die zum Einsatz in einer mehrsprachigen Umgebung bestimmt sind, als auch durch theoretisch...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 139,
    "label": 1,
    "text": "Class Representation and Image Retrieval with Non-Metric Distances One of the key problems in appearance-based vision is understanding how to use a set of labeled images to classify new images. Classification systems that can model human performance, or that use robust image matching methods, often make use of similarity judgments that are non-metric; but when the triangle inequality is not obeyed, most existing pattern recognition techniques are not applicable. We note that exemplar-based (or nearest-neighbor) methods can be applied naturally when using a wide class of non-metric similarity functions. The key issue, however, is to find methods for choosing good representatives of a class that accurately characterize it. We show that existing condensing techniques for finding class representatives are ill-suited to deal with non-metric dataspaces. We then focus on developing techniques for solving this problem, emphasizing two points: First, we show that the distance between two images is not a good measure of how well one image can represent another in non-metric spaces. Instead, we use the vector correlation between the distances from each image to other previously seen images. Second, we show that in non-metric spaces, boundary points are less significant for capturing the structure of a class",
    "neighbors": [
      1040
    ],
    "mask": "Train"
  },
  {
    "node_id": 140,
    "label": 0,
    "text": "Organisational Abstractions for the Analysis and Design of Multi-Agent Systems Abstract. The architecture of a multi-agent system can naturally be viewed as a computational organisation. For this reason, we believe organisational abstractions should play a central role in the analysis and design of such systems. To this end, the concepts of agent roles and role models are increasingly being used to specify and design multi-agent systems. However, this is not the full picture. In this paper we introduce three additional organisational concepts \u2014 organisational rules, organisational structures, and organisational patterns \u2014 that we believe are necessary for the complete specification of computational organisations. We view the introduction of these concepts as a step towards a comprehensive methodology for agent-oriented systems. 1",
    "neighbors": [
      451,
      573,
      957
    ],
    "mask": "Train"
  },
  {
    "node_id": 141,
    "label": 0,
    "text": "Dynamic Reconfiguration in Collaborative Problem Solving In this article we will describe our research efforts in coping with a trade-off that can be often found in the control and optimization of todays business processes. Though centralized control may achieve nearto -optimum results in optimizing the system behavior, there are usually social, technical and security restrictions on applying centralized control. Distributed control on the other hand may cope with these restrictions but also entails sub-optimality and communicational overhead. Our concept of composable agents tries to allow a dynamic and fluent transition between globalization and localization in business process control by adapting to the current real-world system structure. We are currently evaluating this concept in the framework of patient flow control at Charit'e Berlin.  Introduction  Research in Distributed Artificial Intelligence (DAI, (Bond & Gasser 1988)) has been traditionally divided into Distributed Problem Solving (DPS) and Multi Agent Systems (MAS). However, r...",
    "neighbors": [
      25,
      964
    ],
    "mask": "Train"
  },
  {
    "node_id": 142,
    "label": 2,
    "text": "Combining Multiple Learning Strategies for Effective Cross Validation Parameter tuning through cross-validation  becomes very difficult when the validation  set contains no or only a few examples of  the classes in the evaluation set. We address  this open challenge by using a combination  of classifiers with different performance  characteristics to effectively reduce the performance  variance on average of the overall  system across all classes, including those  not seen before. This approach allows us to  tune the combination system on available but  less-representative validation data and obtain  smaller performance degradation of this  system on the evaluation data than using a  single-method classifier alone. We tested this  approach by applying k-Nearest Neighbor,  Rocchio and Language Modeling classifiers  and their combination to the event tracking  problem in the Topic Detection and Tracking  (TDT) domain, where new classes (events)  are created constantly over time, and representative  validation sets for new classes are  often difficult to ob...",
    "neighbors": [
      133,
      341,
      1178
    ],
    "mask": "Train"
  },
  {
    "node_id": 143,
    "label": 2,
    "text": "Web Metasearch as Belief Aggregation Web metasearch requires a mechanism for combining rank-ordered lists of ratings returned by multiple search engines in response to a given user query. We view this as being analogous to the need for combining degrees of belief in probabilistic and uncertain reasoning in artificial intelligence. This paper describes a practical method for performing web metasearch based on a novel transformationbased theory of belief aggregation. The consensus ratings produced by this method take into account the item ratings/rankings output by individual search engines as well as the user's preferences. Copyright c fl 2000, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. Introduction Web search engines (WSE) use tools ranging from simple text-based search to more sophisticated methods that attempt to understand the intended meanings of both queries and data items. There has been much work in this area in recent years. The link structure of the web has...",
    "neighbors": [
      224,
      933,
      1007
    ],
    "mask": "Train"
  },
  {
    "node_id": 144,
    "label": 1,
    "text": "Analysis of Approximate Nearest Neighbor Searching with Clustered Point Sets this paper we study the performance of two other splitting methods, and compare them against the kd-tree splitting method. The first, called slidingmidpoint, is a splitting method that was introduced by Mount and Arya in the ANN library for approximate nearest neighbor searching [30]. This method was introduced into the library in order to better handle highly clustered data sets. We know of no analysis (empirical or theoretical) of this method. This method was designed as a simple technique for addressing one of the most serious flaws in the standard kd-tree splitting method. The flaw is that when the data points are highly clustered in low dimensional subspaces, then the standard kd-tree splitting method may produce highly elongated cells, and these can lead to slow query times. This splitting method starts with a simple midpoint split of the longest side of the cell, but if this split results in either subcell containing no data points, it translates (or \"slides\") the splitting plane in the direction of the  points until hitting the first data point. In Section 3.1 we describe this splitting method and analyze some of its properties. The second splitting method, called minimum-ambiguity, is a query-based technique. The tree is given not only the data points, but also a collection of sample query points, called the training points. The algorithm applies a greedy heuristic to build the tree in an attempt to minimize the expected query time on the training points. We model query processing as the problem of eliminating data points from consideration as the possible candidates for the nearest neighbor. Given a collection of query points, we can model any stage of the nearest neighbor algorithm as a bipartite graph, called the candidate graph, whose vertices correspond t...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 145,
    "label": 0,
    "text": "Agents Supporting Information Integration: The Miks Framework During past years we have developed the MOMIS (Mediator envirOnment for Multiple Information Sources) system for the integration of data from structured and semi-structured data sources.  In this paper we propose a new system, MIKS (Mediator agent for Integration of Knowledge Sources), which enriches the MOMIS architecture exploiting the intelligent and mobile agent features.  1. Motivation  The web explosion, both at Internet and intranet level, has transformed the electronic information system from single isolated node to an entry point into a worldwide network of information exchange and business transactions. One of the main challenges for the designers of the e-commerce infrastructures is the information sharing, retrieving data located in different sources thus obtaining an integrated view to overcome any contradiction or redundancy.  During past years we have developed the MOMIS (Mediator envirOnment for Multiple Information Sources) system for the integration of data from struc...",
    "neighbors": [
      766
    ],
    "mask": "Train"
  },
  {
    "node_id": 146,
    "label": 3,
    "text": "&lt;bigwig&gt; -- A language for developing interactive Web services &lt;bigwig&gt; is a high-level programming language and a compiler for developing interactive Web services. The overall goal of the language design is to remove many of the obstacles that face current developers of Web services in order to lower cost while increasing functionality and reliability. The compiler translates programs into a conglomerate of lower-level standard technologies such as CGI-scripts, HTML, JavaScript, and HTTP Authentication. This paper describes the major facets of the language design and the techniques used in their implementation, and compares the design with alternative Web service technologies.",
    "neighbors": [
      203
    ],
    "mask": "Train"
  },
  {
    "node_id": 147,
    "label": 3,
    "text": "A Data Model and Data Structures for Moving Objects Databases We consider spatio-temporal databases supporting spatial objects with continuously  changing position and extent, termed moving objects databases. We formally  define a data model for such databases that includes complex evolving spatial  structures such as line networks or multi-component regions with holes. The data  model is given as a collection of data types and operations which can be plugged  as attribute types into any DBMS data model (e.g. relational, or object-oriented)  to obtain a complete model and query language. A particular novel concept is the  sliced representation which represents a temporal development as a set of units,  where unit types for spatial and other data types represent certain \"simple\" functions  of time. We also show how the model can be mapped into concrete physical  data structures in a DBMS environment.  1 Introduction  A wide and increasing range of database applications has to deal with spatial objects whose position and/or extent changes over time...",
    "neighbors": [
      27,
      129,
      481,
      1158
    ],
    "mask": "Train"
  },
  {
    "node_id": 148,
    "label": 4,
    "text": "Time Series Segmentation for Context Recognition in Mobile Devices Recognizing the context of use is important in making mobile devices as simple to use as possible. Finding out what the user's situation is can help the device and underlying service in providing an adaptive and personalized user interface. The device can infer parts of the context of the user from sensor data: the mobile device can include sensors for acceleration, noise level, luminosity, humidity, etc. In this paper we consider context recognition by unsupervised segmentation of time series produced by sensors.  Dynamic programming can be used to find segments that minimize the intra-segment variances. While this method produces optimal solutions, it is too slow for long sequences of data. We present and analyze randomized variations of the algorithm. One of them, Global Iterative Replacement or GIR, gives approximately optimal results in a fraction of the time required by dynamic programming. We demonstrate the use of time series segmentation in context recognition for mobile phone applications.  1",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 149,
    "label": 2,
    "text": "Stable Algorithms for Link Analysis The Kleinberg HITS and the Google PageRank algorithms are eigenvector methods for identifying \"authoritative\" or \"influential\" articles, given hyperlink or citation information. That such algorithms should give reliable or consistent answers is surely a desideratum, and in [10], we analyzed when they can be expected to give stable rankings under small perturbations to the linkage patterns. In this paper, we extend the analysis and show how it gives insight into ways of designing stable link analysis methods. This in turn motivates two new algorithms, whose performance we study empirically using citation data and web hyperlink data. 1.",
    "neighbors": [
      774,
      1189
    ],
    "mask": "Train"
  },
  {
    "node_id": 150,
    "label": 2,
    "text": "Memory Hierarchies as a Metaphor for Academic Library Collections Research libraries and their collections are a cornerstone of the academic tradition, representing 2000 years of development of the Western Civilization; they make written history widely accessible at low cost. Computer memories are a range of physical devices used for storing digital information that have undergone much formal study over 40 years and are well understood. This paper draws parallels between the organisation of research collections and computer memories, in particular examining their hierarchical structure, and examines the implication for digital libraries.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 151,
    "label": 3,
    "text": "Developments in Spatio-Temporal Query Languages In contrast to the field view of spatial data that basically views spatial data as a mapping from points into some features, the object view clusters points by features and their values into spatial objects of type point, line, or region. When embedding these objects into a data model, such as the relational model, an additional clustering according to conceptually identified objects takes place. For example, we could define a relation City(name: string,center: point,area: region) that combines different features for cities in one relation. An important aspect of this kind of modeling is that clustering happens on two different levels: (i) points are grouped into spatial objects like regions and (ii) different attributes/features are grouped into a perceived object. When talking about data modeling there is no reason why this grouping should be limited to two levels. For example, we can consider storing regions of different population densities for each city in an attribute density: num \u2192 region. Although then the relation is not in first normal form anymore, we can \u201crecover\u201d the first normal form by encapsulating the function num \u2192 region in an abstract data type. The important aspect is that all the required operations on such a type as well as on regions and other complex types can be defined to a large degree independently from the data model. 1 The most important point about the preceding discussion is the way in which complex types can be easily",
    "neighbors": [
      487,
      1012
    ],
    "mask": "Train"
  },
  {
    "node_id": 152,
    "label": 1,
    "text": "A Methodology to Improve Ad Hoc Data-Driven Linguistic Rule Learning Methods by Inducing Cooperation Among Rules Within the Linguistic Modeling eld |one of the most important applications of Fuzzy  Rule-Based Systems|, a family of ecient and simple methods guided by covering criteria  of the data in the example set, called \\ad hoc data-driven methods\", has been proposed in the  literature in the last few years. Their high performance, in addition to their quickness and  easy understanding, have make them very suitable for learning tasks. In this paper we are  going to perform a double task analyzing these kinds of learning methods and introducing  a new methodology to signicantly improve their accuracy keeping their descriptive power  unalterable.  On the one hand, a taxonomy of ad hoc data-driven learning methods based on the way  in which the available data is used to guide the learning will be made. In this sense, we  will distinguish between two approaches: the example-based and the fuzzy-grid-based one.  Whilst in the former each rule is obtained from a specic example, in the latter the e...",
    "neighbors": [
      834
    ],
    "mask": "Train"
  },
  {
    "node_id": 153,
    "label": 3,
    "text": "OMS/Java: Model Extensibility of OODBMS for Advanced Application Domains . We showhow model extensibility of object-oriented data  management systems can be achieved through the combination of a highlevel  core object data model and an architecture designed with model  extensibility in mind. The resulting system, OMS#Java, is both a general  data management system and a framework for the developmentof  advanced database application systems. All aspects of the core model #  constructs, query language and constraints # can easily be generalised to  support, for example, the management of temporal, spatial and versioned  data. Speci#cally,we showhow the framework was used to extend the  core system to a temporal object-oriented database management system.  1 Introduction  Extensibility has often been considered a purely architectural issue in database management systems #DBMS#. In the 1980s, there was an increase in the various forms of DBMS that appeared --- many of whichwere tailored to speci#c application domains such as Geographical Information Systems or ...",
    "neighbors": [
      199
    ],
    "mask": "Train"
  },
  {
    "node_id": 154,
    "label": 3,
    "text": "A Holistic Process Performance Analysis through a Performance Data Warehouse This paper describes how a performance data warehouse can be used to facilitate business process improvement that is based on holistic performance measurement. The feasibility study shows how management and analysis of performance data can be facilitated by a data warehouse approach. It is argued that many of the shortcomings of traditional measurement systems can be overcome with this performance data warehouse approach.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 155,
    "label": 5,
    "text": "A Layered Approach to Learning Client Behaviors in the RoboCup Soccer Server In the past few years, Multiagent Systems (MAS) has emerged as an active subfield of Artificial Intelligence (AI). Because of the inherent complexity of MAS, there is much interest in using Machine Learning (ML) techniques to help build multiagent systems. Robotic soccer is a particularly good domain for studying MAS and Multiagent Learning. Our approach to using ML as a tool for building Soccer Server clients involves layering increasingly complex learned behaviors. In this article, we describe two levels of learned behaviors. First, the clients learn a low-level individual skill that allows them to control the ball effectively. Then, using this learned skill, they learn a higher-level skill that involves multiple players. For both skills, we describe the learning method in detail and report on our extensive empirical testing. We also verify empirically that the learned skills are applicable to game situations. 1 Introduction  In the past few years, Multiagent Systems (MAS) has emerge...",
    "neighbors": [
      25,
      127,
      394,
      430,
      927,
      1266
    ],
    "mask": "Validation"
  },
  {
    "node_id": 156,
    "label": 2,
    "text": "Iterative Information Retrieval Using Fast Clustering and Usage-Specific Genres This paper describes how collection specific empirically defined stylistics based genre prediction can be brought together together with rapid topical clustering to build an interactive information retrieval interface with multi-dimensional presentation of search results. The prototype presented addresses two specific problems of information retrieval: how to enrich the information seeking dialog by encouraging and supporting iterative refinement of queries, and how to enrich the document representation past the shallow semantics allowed by term frequencies.  Searching For More Than Words  Today's tools for searching information in a document database are based on term occurrence in texts. The searcher enters a number of terms and a number of documents where those terms or closely related terms appear comparatively frequently are retrieved and presented by the system in list form. This method works well up to a point. It is intuitively understandable, and for competent users and well e...",
    "neighbors": [
      298,
      903
    ],
    "mask": "Train"
  },
  {
    "node_id": 157,
    "label": 1,
    "text": "Modified Gath-Geva Fuzzy Clustering for Identification of Takagi-Sugeno Fuzzy Models The construction of interpretable Takagi--Sugeno (TS) fuzzy models by means of clustering is addressed. First, it is shown how the antecedent fuzzy sets and the corresponding consequent parameters of the TS model can be derived from clusters obtained by the Gath--Geva algorithm. To preserve the partitioning of the antecedent space, linearly transformed input variables can be used in the model. This may, however, complicate the interpretation of the rules. To form an easily interpretable model that does not use the transformed input variables, a new clustering algorithm is proposed, based on the Expectation Maximization (EM) identification of Gaussian mixture models. This new technique is applied to two well-known benchmark problems: the MPG (miles per gallon) prediction and a simulated second-order nonlinear process. The obtained results are compared with results from the literature.",
    "neighbors": [
      828
    ],
    "mask": "Train"
  },
  {
    "node_id": 158,
    "label": 3,
    "text": "Executing Query Packs in ILP Inductive logic programming systems usually send large numbers  of queries to a database. The lattice structure from which these queries  are typically selected causes many of these queries to be highly similar. As a consequence, independent execution of all queries may involve a lot of redundant computation. We propose a mechanism for executing a hierarchically  structured set of queries (a \"query pack\") through which a lot of redundancy in the computation is removed. We have incorporated our  query pack execution mechanism in the ILP systems Tilde and Warmr by implementing a new Prolog engine ilProlog which provides support  for pack execution at a lower level. Experimental results demonstrate  significant efficiency gains. Our query pack execution mechanism is very  general in nature and could be incorporated in most other ILP systems,  with similar efficiency improvements to be expected.",
    "neighbors": [
      639,
      1261
    ],
    "mask": "Train"
  },
  {
    "node_id": 159,
    "label": 0,
    "text": "Resolution-Based Proof for Multi-Modal Temporal Logics of Knowledge Temporal logics of knowledge are useful in order to specify complex systems in which agents are both dynamic and have information about their surroundings. We present a resolution method for propositional temporal logic combined with multi-modal S5 and illustrate its use on examples. This paper corrects a previous proposal for resolution in multi-modal temporal logics of knowledge.  Keywords: temporal and modal logics, non-classical  resolution, theorem-proving  1 Introduction  Combinations of logics have been useful for specifying and reasoning about complex situations, for example multi-agent systems [21, 24], accident analysis [15], and security protocols [18]. For example, logics to formalise multi-agent systems often incorporate a dynamic component representing change of over time; an informational component to capture the agent's knowledge or beliefs; and a motivational component for notions such as goals, wishes, desires or intentions. Often temporal or dynamic logic is used for...",
    "neighbors": [
      82,
      327,
      953
    ],
    "mask": "Validation"
  },
  {
    "node_id": 160,
    "label": 1,
    "text": "Advances in Analogy-Based Learning: False Friends and Exceptional Items in Pronunciation By Paradigm-Driven Analogy When looked at from a multilingual perspective, grapheme-to-phoneme conversion is a challenging task, fraught with most of the classical NLP \"vexed questions\": bottle-neck problem of data acquisition, pervasiveness of exceptions, difficulty to state range and order of rule application, proper treatment of context-sensitive phenomena and long-distance dependencies, and so on. The hand-crafting of transcription rules by a human expert is onerous and time-consuming, and yet, for some European languages, still stops short of a level of correctness and accuracy acceptable for practical applications. We illustrate here a self-learning multilingual system for analogy-based pronunciation which was tested on Italian, English and French, and whose performances are assessed against the output of both statistically and rule-based transcribers. The general point is made that analogy-based self-learning techniques are no longer just psycholinguistically-plausible models, but competitive tools, combining the advantages of using language-independent, self-learning, tractable algorithms, with the welcome bonus of being more reliable for applications than traditional text-to-speech systems.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 161,
    "label": 2,
    "text": "Ontobroker: The Very High Idea The World Wide Web (WWW) is currently one of the most important electronic information sources. However, its query interfaces and the provided reasoning services are rather limited. Ontobroker consists of a number of languages and tools that enhance query access and inference service of the WWW. The technique is based on the use of ontologies.  Ontologies are applied to annotate web documents and to provide query access and inference service that deal with the semantics of the presented information. In consequence, intelligent brokering services for web documents can be achieved without requiring to change the semiformal nature of web documents. Introduction  The World Wide Web (WWW) contains huge amounts of knowledge about almost all subjects you can think of. HTML documents enriched by multi-media applications provide knowledge in different representations (i.e., text, graphics, animated pictures, video, sound, virtual reality, etc.). Hypertext links between web documents represent r...",
    "neighbors": [
      452,
      1190,
      1233
    ],
    "mask": "Validation"
  },
  {
    "node_id": 162,
    "label": 3,
    "text": "A Theorem Prover-Based Analysis Tool for Object-Oriented Databases We present a theorem-prover based analysis tool for object-oriented database systems with integrity  constraints. Object-oriented database specifications are mapped to higher-order logic (HOL). This allows  us to reason about the semantics of database operations using a mechanical theorem prover such  as Isabelle or PVS. The tool can be used to verify various semantics requirements of the schema (such as  transaction safety, compensation, and commutativity) to support the advanced transaction models used in  workflow and cooperative work. We give an example of method safety analysis for the generic structure  editing operations of a cooperative authoring system.  1 Introduction  Object-oriented specification methodologies and object-oriented programming have become increasingly important in the past ten years. Not surprisingly, this has recently led to an interest in object-oriented program verification in the theorem prover community, mainly using higher-order logic (HOL). Several dif...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 163,
    "label": 5,
    "text": "Embodied Evolution: Embodying an Evolutionary Algorithm in a Population of Robots We introduce Embodied Evolution (EE) as a methodology for the automatic design of robotic controllers. EE is an evolutionary robotics (ER) technique that avoids the pitfalls of the simulate-and-transfer method, allows the speed-up of evaluation time by utilizing parallelism, and is particularly suited to future work on multi-agent behaviors. In EE, an evolutionary algorithm is distributed amongst and embodied within a population of physical robots that reproduce with one another while situated in the task environment. We have built a population of eight robots and successfully implemented our first experiments. The controllers evolved by EE compare favorably to hand-designed solutions for a simple task. We detail our methodology, report our initial results, and discuss the application of EE to more advanced and distributed robotics tasks. 1. Introduction  Our work is inspired by the following vision. A large number of robots freely interact with each other in a shared environment, atte...",
    "neighbors": [
      741
    ],
    "mask": "Validation"
  },
  {
    "node_id": 164,
    "label": 3,
    "text": "CHIME: Customizable Hyperlink Insertion and Maintenance Engine for Software Engineering Environments Source code browsing is an important part of program comprehension. Browsers expose semantic and syntactic relationships (such as between object references and definitions) in GUI-accessible forms. These relationships are derived using tools which perform static analysis on the original software documents. Implementing such browsers is tricky. Program comprehension strategies vary, and it is necessary to provide the right browsing support. Analysis tools to derive the relevant crossreference relationships are often difficult to build. Tools to browse distributed documents require extensive coding for the GUI, as well as for data communications. Therefore, there are powerful motivations for using existing static analysis tools in conjunction with WWW technology to implement browsers for distributed software projects. The chime framework provides a flexible, customizable platform for inserting HTML links into software documents using information generated by existing software analysis tools. Using the chime specification language, and a simple, retargetable database interface, it is possible to quickly incorporate a range of different link insertion tools for software documents, into an existing, legacy software development environment. This enables tool builders to offer customized browsing support with a well-known GUI. This paper describes the chime architecture, and describes our experience with several re-targeting efforts of this system. 1",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 165,
    "label": 5,
    "text": "Word Sense Disambiguation based on Semantic Density This paper presents a Word Sense Disambiguation method based on the idea of semantic density between words. The disambiguation is done in the context of WordNet. The Internet is used as a raw corpora to provide statistical information for word associations. A metric is introduced and used to measure the semantic density and to rank all possible combinations of the senses of two words. This method provides a precision of 58% in indicating the correct sense for both words at the same time. The precision increases as we consider more choices: 70% for top two ranked and 73% for top three ranked.  1 Introduction  Word Sense Disambiguation (WSD) is an open problem in Natural Language Processing. Its solution impacts other tasks such as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et al.1992), (Miller et al.1994), (Agirre and Rig...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 166,
    "label": 1,
    "text": "Learning-Based Vision and Its Application to Autonomous Indoor Navigation Learning-Based Vision and Its Application to Autonomous Indoor Navigation  By  Shaoyun Chen Adaptation is critical to autonomous navigation of mobile robots. Many adaptive mechanisms have been implemented, ranging from simple color thresholding to complicated learning with artificial neural networks (ANN). The major focus of this thesis lies in machine learning for vision-based navigation. Two well known vision-based navigation systems are ALVINN and ROBIN developed by Carnegie-Mellon University and University of Maryland, respectively. ALVINN uses a two-layer feedforward neural network while ROBIN relies on a radial basis function network (RBFN). Although current ANN-based methods have achieved great success in vision-based navigation, they have two major disadvantages: (1) Local minimum problem: The training of either multilayer perceptron or radial basis function network can get stuck at poor local minimums. (2) The flexibility problem: After the system has been trained in certain r...",
    "neighbors": [
      244,
      463
    ],
    "mask": "Train"
  },
  {
    "node_id": 167,
    "label": 2,
    "text": "View-independent Recognition of Hand Postures Since human hand is highly articulated and deformable, hand posture recognition is a challenging example in the research of view-independent object recognition. Due to the difficulties of the modelbased approach, the appearance-based learning approach is promising to handle large variation in visual inputs. However, the generalization of many proposed supervised learning methods to this problem often suffers from the insufficiency of labeled training data. This paper describes an approach to alleviate this difficulty by adding a large unlabeled training set. Combining supervised and unsupervised learning paradigms, a novel and powerful learning approach, the Discriminant-EM (D-EM) algorithm, is proposed in this paper to handle the case of small labeled training set. Experiments show that D-EM outperforms many other learning methods. Based on this approach, we implement a gesture interface to recognize a set o...",
    "neighbors": [
      234,
      609,
      1153
    ],
    "mask": "Validation"
  },
  {
    "node_id": 168,
    "label": 2,
    "text": "Exploiting Structure for Intelligent Web Search Together with the rapidly growing amount of online data we register an immense need for intelligent search engines that access a restricted amount of data as found in intranets or other limited domains. This sort of search engines must go beyond simple keyword indexing/matching, but they also have to be easily adaptable to new domains without huge costs. This paper presents a mechanism that addresses both of these points: first of all, the internal document structure is being used to extract concepts which impose a directorylike structure on the documents similar to those found in classified directories. Furthermore, this is done in an efficient way which is largely language independent and does not make assumptions about the document structure.",
    "neighbors": [
      1000,
      1017,
      1032
    ],
    "mask": "Train"
  },
  {
    "node_id": 169,
    "label": 2,
    "text": "Accurately and Reliably Extracting Data from the Web: A Machine Learning Approach A critical problem in developing information agents for the Web is accessing data that is formatted for human use. We have developed a set of tools for extracting data from web sites and transforming it into a structured data format, such as XML. The resulting data can then be used to build new applications without having to deal with unstructured data. The advantages of our wrapping technology over previous work are the the ability to learn highly accurate extraction rules, to verify the wrapper to ensure that the correct data continues to be extracted, and to automatically adapt to changes in the sites from which the data is being extracted.  1 Introduction  There is a tremendous amount of information available on the Web, but much of this information is not in a form that can be easily used by other applications. There are hopes that XML will solve this problem, but XML is not yet in widespread use and even in the best case it will only address the problem within application domains...",
    "neighbors": [
      243,
      859,
      1233
    ],
    "mask": "Validation"
  },
  {
    "node_id": 170,
    "label": 4,
    "text": "The CLEF 2003 Interactive Track The CLEF 2003 Interactive Track (iCLEF) was the third  year of a shared experiment design to compare strategies for cross-language  search assistance. Two kinds of experiments were performed: a) experiments  in Cross-Language Document Selection, where the user task is to  scan a ranked list of documents written in a foreign language, selecting  those which seem relevant to a given query. The aim here is to compare  di#erent translation strategies for an \"indicative\" purpose; and b)  Full Cross-Language Search experiments, where the user task is to maximize  the number of relevant documents that can be found in a foreignlanguage  collection with the help of an end-to-end cross-language search  system. Participating teams could choose to focus on any aspects of the  search task (e.g., query formulation, query translation and/or relevance  feedback). This paper describes the shared experiment design and briefly  summarizes the experiments run by the five teams that participated.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 171,
    "label": 3,
    "text": "Rotational Polygon Containment and Minimum Enclosure An algorithm and a robust floating point implementation is given for rotational polygon containment:given polygons P 1 ,P 2 ,P 3 ,...,P k and a container polygon C, find rotations and translations for the k polygons that place them into the container without overlapping. A version of the algorithm and implementation also solves rotational minimum enclosure: givenaclass C of container polygons, find a container C in C of minimum area for which containment has a solution. The minimum enclosure is approximate: it bounds the minimum area between (1-epsilon)A and A. Experiments indicate that finding the minimum enclosure is practical for k = 2, 3 but not larger unless optimality is sacrificed or angles ranges are limited (although these solutions can still be useful). Important applications for these algorithm to industrial problems are discussed. The paper also gives practical algorithms and numerical techniques for robustly calculating polygon set intersection, Minkowski sum, and range in...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 172,
    "label": 1,
    "text": "A Quantification Of Distance-Bias Between Evaluation Metrics In Classification This paper provides a characterization of bias  for evaluation metrics in classification (e.g.,  Information Gain, Gini,   2  , etc.). Our characterization  provides a uniform representation  for all traditional evaluation metrics.  Such representation leads naturally to a measure  for the distance between the bias of  two evaluation metrics. We give a practical  value to our measure by observing if  the distance between the bias of two evaluation  metrics correlates with differences in  predictive accuracy when we compare two  versions of the same learning algorithm that  differ in the evaluation metric only. Experiments  on real-world domains show how  the expectations on accuracy differences generated  by the distance-bias measure correlate  with actual differences when the learning  algorithm is simple (e.g., search for the  best single-feature or the best single-rule).  The correlation, however, weakens with more  complex algorithms (e.g., learning decision  trees). Our results sh...",
    "neighbors": [
      1140
    ],
    "mask": "Validation"
  },
  {
    "node_id": 173,
    "label": 1,
    "text": "Reasoning within Fuzzy Description Logics Description Logics (DLs) are suitable, well-known, logics for managing structured knowledge. They allow reasoning about individuals and well defined concepts, i.e. set of individuals with co#hfiP pro# erties. The experience in using DLs inapplicatio#& has sho wn that in many cases we wo#6H like to extend their capabilities. In particular, their use in the co# texto# Multimedia Info#mediafi Retrieval (MIR) leadsto the co# vincement that such DLssho#PF allo w the treatmento f the inherentimprecisio# in multimediao# ject co# tent representatio# and retrieval. In this paper we will present a fuzzyextensio#  ALC,co#  bining Zadeh's fuzzy lo#zy with a classical DL. In particular,co#rticu beco#FK fuzzy and, thus,reaso#HO6 ab o#fi impreciseco#recis is suppo#ppfi6 We will define its syntax, its semantics, describe its pro# erties and present a co#PHOSfi9 tpro#F&fi9KFS calculus for reasoning in it.",
    "neighbors": [
      311,
      644
    ],
    "mask": "Validation"
  },
  {
    "node_id": 174,
    "label": 3,
    "text": "Scalable Processing of Read-Only Transactions in Broadcast Push Recently, push-based delivery has attracted considerable attention as a means of disseminating information to large client populations in both wired and wireless settings. In this paper, we address the problem of ensuring the consistency and currency of client read-only transactions in the presence of updates. To this end, additional control information is broadcast. A suite of methods is proposed that vary in the complexity and volume of the control information transmitted and subsequently differ in response times, degrees of concurrency, and space and processing overheads. The proposed methods are combined with caching to improve query latency. The relative advantages of each method are demonstrated through both simulation results and qualitative arguments. Read-only transactions are processed locally at the client without contacting the server and thus the proposed approaches are scalable, i.e., their performance is independent of the number of clients.  1. Introduction  In traditio...",
    "neighbors": [
      354,
      373,
      470
    ],
    "mask": "Train"
  },
  {
    "node_id": 175,
    "label": 4,
    "text": "Interpretation of Shape-related Iconic Gestures in Virtual Environments The interpretation of iconic gestures in spatial domains is a promising idea to improve the communicative capabilities of human-computer interfaces. So far, approaches towards gesture recognition focused mainly on deictic and emblematic gestures. Iconics, viewed as iconic signs in the sense of Peirce, are different from deictics and emblems, for their relation to the referent is based on similarity. In the work reported here, the breakdown of the complex notion of similarity provides the key idea towards a computational model of gesture semantics for iconic gestures. Based on an empirical study,we describe first steps towards a recognition model for shape-related iconic gestures and its implementation in a prototype gesture recognition system. Observations are focused on spatial concepts and their relation to features of iconic gestural expressions. The recognition model is based on a graphmatching method which compares the decomposed geometrical structures of gesture and object.",
    "neighbors": [
      635,
      921
    ],
    "mask": "Train"
  },
  {
    "node_id": 176,
    "label": 3,
    "text": "A Formal Approach to Detecting Security Flaws in Object-Oriented Databases This paper adopts the method-based authorization model and assumes the following database management policies. Let (m, (c 1 ,c 2 , ...,c n )) be in an authorization for a user u.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 177,
    "label": 2,
    "text": "A Maximum Entropy Language Model Integrating N-Grams And Topic Dependencies For Conversational Speech Recognition A compact language model which incorporates local dependencies in the form of N-grams and long distance dependencies through dynamic topic conditional constraints is presented. These constraints are integrated using the maximum entropy principle. Issues in assigning a topic to a test utterance are investigated. Recognition results on the Switchboard corpus are presented showing that with a very small increase in the number of model parameters, reduction in word error rate and language model perplexity are achieved over trigram models. Some analysis follows, demonstrating that the gains are even larger on content-bearing words. The results are compared with those obtained by interpolating topicindependent and topic-specific N-gram models. The framework presented here extends easily to incorporate other forms of statistical dependencies such as syntactic word-pair relationships or hierarchical topic constraints. 1. INTRODUCTION Language modeling is a crucial component of systems that c...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 178,
    "label": 0,
    "text": "Speech Acts for Dialogue Agents this paper by the U.S. Army Research Office under contract/grant number  DAAH 04 95 10628 and the U.S. National Science Foundation under grant IRI9311988. Some of the work described above was developed in collaboration with James Allen and supported by ONR/DARPA under grant number N00014-92J -1512, by ONR under research grant number N00014-90-J-1811, and by NSF under grant number IRI-9003841.",
    "neighbors": [
      359,
      981
    ],
    "mask": "Validation"
  },
  {
    "node_id": 179,
    "label": 3,
    "text": "On the Extension of UML with Use Case Maps Concepts . Descriptions of reactive systems focus heavily on behavioral aspects,  often in terms of scenarios. To cope with the increasing complexity of services  provided by these systems, behavioral aspects need to be handled early in the  design process with flexible and concise notations as well as expressive concepts.  UML offers different notations and concepts that can help describe such  services. However, several necessary concepts appear to be absent from UML,  but present in the Use Case Map (UCM) scenario notation. In particular, Use  Case Maps allow scenarios to be mapped to different architectures composed of  various component types. The notation supports structured and incremental development  of complex scenarios at a high level of abstraction, as well as their  integration. UCMs specify variations of run-time behavior and scenario structures  through sub-maps \"pluggable\" into placeholders called stubs. This paper  presents how UCM concepts could be used to extend the semantics...",
    "neighbors": [
      19
    ],
    "mask": "Train"
  },
  {
    "node_id": 180,
    "label": 3,
    "text": "Repeating History beyond ARIES In this paper, I describe first the background behind the development of the original ARIES recovery method, and its significant impact on the commercial world and the research community. Next, I provide a brief introduction to the various concurrency control and recovery methods in the ARIES family of algorithms. Subsequently, I discuss some of the recent developments affecting the transaction management area and what these mean for the future. In ARIES, the concept of repeating history turned out to be an important paradigm. As I examine where transaction management is headed in the world of the internet, I observe history repeating itself in the sense of requirements that used to be considered significant in the mainframe world (e.g., performance, availability and reliability) now becoming important requirements of the broader information technology community as well. 1. Introduction Transaction management is one of the most important functionalities provided by a...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 181,
    "label": 1,
    "text": "Generating Accurate Rule Sets Without Global Optimization The two dominant schemes for rule-learning, C4.5 and RIPPER, both operate in two stages. First they induce an initial rule set and then they refine it using a rather complex optimization stage that discards (C4.5) or adjusts (RIPPER) individual rules to make them work better together. In contrast, this paper shows how good rule sets can be learned one rule at a time, without any need for global optimization. We present an algorithm for inferring rules by repeatedly generating partial decision trees, thus combining the two major paradigms for rule generation\u2014creating rules from decision trees and the separate-and-conquer rule-learning technique. The algorithm is straightforward and elegant: despite this, experiments on standard datasets show that it produces rule sets that are as accurate as and of similar size to those generated by C4.5, and more accurate than RIPPER\u2019s. Moreover, it operates efficiently, and because it avoids postprocessing, does not suffer the extremely slow performance on pathological example sets for which the C4.5 method has been criticized.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 182,
    "label": 0,
    "text": "Desiderata for Agent Oriented Programming Languages Multiagent system designers need programming languages in order to develop agents and multiagent systems. Current approaches consist to use classical programming languages like C or C++ and above all Java which is the most preferred language by agent community thanks to its rich library of functions. The aim of Java is not to design multiagent systems so it does not encompass multiagent features. The aim of this paper is to present a set of characteristics which could be present in an agent-oriented programming language. This paper also describes what kind of multiagent systems could be developed with this set of characteristics.",
    "neighbors": [
      472
    ],
    "mask": "Train"
  },
  {
    "node_id": 183,
    "label": 1,
    "text": "Primitive-Based Movement Classification for Humanoid Imitation . Motor control is a complex problem and imitation is a powerful mechanism for acquiring new motor  skills. In this paper, we describe perceptuo-motor primitives, a biologically-inspired notion for a basis set of  perceptual and motor routines. Primitives serve as a vocabulary for classifying and imitating observed human  movements, and are derived from the imitator's motor repertoire. We describe a model of imitation based  on such primitives and demonstrate the feasibility of the model in a constrained implementation. We present  approximate motion reconstruction generated from visually captured data of typically imitated tasks taken from  aerobics, dancing, and athletics.  1 Introduction  Imitation is a powerful mechanism for acquiring new skills. It involves an intricate interaction between perceptual and motor mechanisms, both of which are complex in themselves. Research into vision and motor control has explored the role of \"subroutines\", schemas [1], and other variants based on ...",
    "neighbors": [
      398,
      1143
    ],
    "mask": "Train"
  },
  {
    "node_id": 184,
    "label": 0,
    "text": "Specifying Agents with UML in Robotic Soccer The use of agents and multiagent systems is widespread in computer  science nowadays. Thus the need for methods to specify agents in a clear  and simple manner arises.  In this paper we propose an approach to specifying agents with the  help of UML statecharts. Agents are specified on different levels of abstraction.  In addition a method for specifying multiagent plans with explicit  cooperation is shown.  As an example domain we chose robotic soccer, which lays the basis  of the annual RoboCup competitions. Robotic soccer is an ideal testbed  for research in the fields of robotics and multiagent systems. In the  RoboCup Simulation League the research focus is laid on agents and  multiagent systems, and we will demonstrate our approach by using examples  from this domain.  Keywords: Multiagent Systems, Unified Modeling Language (UML),  Specification, RoboCup, Robotic Soccer  1",
    "neighbors": [
      363,
      508,
      602
    ],
    "mask": "Train"
  },
  {
    "node_id": 185,
    "label": 4,
    "text": "Value-added Mediation in Large-Scale Information Systems Many information-processing tasks can be part of multiple customer applications, as summarizing  stock prices, integrating catolog information from several companies in the same line of business,  predicting the weather, and checking on transportation resources. We assign such sharable services  to an active middleware layer, interposed between clients and servers. We define domain-specific  mediator modules to populate this layer.  Such mediating services must be of value to the customers, so that it will benefit their client  applications to access mediators rather than the server sources directly. Several categories of value  can be considered: improvement in access and coverage, improvement of content, and delegation  of maintenance. We will define criteria for mediating modules: ownership by party who assumes  responsibility for the rseults of the services, domain-specificity to delimit the scope of such a responsibility,  and, of course, conformance with interface standards that ...",
    "neighbors": [
      859,
      1079
    ],
    "mask": "Validation"
  },
  {
    "node_id": 186,
    "label": 2,
    "text": "Information Extraction via Heuristics for a Movie Showtime Query System Semantic interpretation for limited-domain spoken dialogue systems often amounts to extracting information from utterances. For a system that provides movie showtime information, queries are classified along four dimensions: question type, and movie titles, towns and theaters that were mentioned. Simple heuristics suffice for constructing highly accurate classifiers for the latter three attributes; classifiers for the question type attribute are induced from data using features tailored to spoken language phenomena. Since separate classifiers are used for the four attributes, which are not independent, certain errors can be detected and corrected, thus increasing robustness. 1.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 187,
    "label": 5,
    "text": "Geometric Foundations for Interval-Based Probabilities The need to reason with imprecise probabilities arises in a wealth of situations ranging from pooling of knowledge from multiple experts to abstraction-based probabilistic planning. Researchers have typically represented imprecise probabilities using intervals and have developed a wide array of different techniques to suit their particular requirements. In this paper we provide an analysis of some of the central issues in representing and reasoning with interval probabilities. At the focus of our analysis is the probability cross-product operator and its interval generalization, the cc-operator. We perform an extensive study of these operators relative to manipulation of sets of probability distributtions. This study provides insight into the sources of the strengths and weaknesses of various approaches to handling probability intervals. We demonstrate the application of our results to the problems of inference in interval Bayesian networks and projection and evaluation of abstract pro...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 188,
    "label": 2,
    "text": "Authoring and Annotation of Web Pages in CREAM Richly interlinked, machine-understandable data constitute the basis for the Semantic Web.  We provide a framework, CREAM, that allows for creation of metadata. While the annotation  mode of CREAM allows to create metadata for existing web pages, the authoring mode lets  authors create metadata --- almost for free --- while putting together the content of a page.  As a particularity of our framework, CREAM allows to create relational metadata, i.e. metadata  that instantiate interrelated definitions of classes in a domain ontology rather than a comparatively  rigid template-like schema as Dublin Core. We discuss some of the requirements one  has to meet when developing such an ontology-based framework, e.g. the integration of a metadata  crawler, inference services, document management and a meta-ontology, and describe its  implementation, viz. Ont-O-Mat a component-based, ontology-driven Web page authoring and  annotation tool.",
    "neighbors": [
      239,
      644,
      835,
      934,
      1106,
      1233
    ],
    "mask": "Train"
  },
  {
    "node_id": 189,
    "label": 4,
    "text": "Principles of Mixed-Initiative User Interfaces Recent debate has centered on the relative promise of focusing user-interface research on developing new  metaphors and tools that enhance users' abilities to directly manipulate objects versus directing effort toward developing interface agents that provide automation. In this paper, we review principles that show promise for allowing engineers to enhance human---computer interaction through an elegant coupling of automated services with direct manipulation. Key ideas will be highlighted in terms of the LookOut system for scheduling and meeting management.  Keywords  Intelligent agents, direct manipulation, user modeling, probability, decision theory, UI design  INTRODUCTION  There has been debate among researchers about where great opportunities lay for innovating in the realm of human--- computer interaction [10]. One group of researchers has expressed enthusiasm for the development and application of new kinds of automated services, often referred to as interface \"agents.\" The effo...",
    "neighbors": [
      460,
      1160
    ],
    "mask": "Train"
  },
  {
    "node_id": 190,
    "label": 3,
    "text": "Achieving Workflow Adaptability by means of Reflection Belief in the importance of business processes has triggered considerable interest in the workflow systems that automate these processes. However, of the two competing management philosophies that promulgate business processes, Business Process Reengineering proposes radical change, whereas Continuous Process Improvement places much greater emphasis on adaptability. The former school is somewhat discredited, whereas the latter school seems more likely to endure, thus making the flexibility and evolution of workflows an issue of increasing importance. In this paper, we present a programmable object-oriented metalevel framework which aims to reveal the processes of assembling and coordinating the tasks that make up business processes. This is achieved by isolating four key facets -- state, behaviour, location and coordination. In particular, we open up the general process of task coordination and specification, allowing for extensions in a planned way. By suitable manipulation of coordin...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 191,
    "label": 5,
    "text": "Optimal Anytime Search For Constrained Nonlinear Programming In this thesis, we study optimal anytime stochastic search algorithms (SSAs) for solving general constrained nonlinear programming problems (NLPs) in discrete, continuous and mixed-integer space. The algorithms are general in the sense that they do not assume differentiability or convexity of functions. Based on the search algorithms, we develop the theory of SSAs and propose optimal SSAs with iterative deepening in order to minimize their expected search time. Based on the optimal SSAs, we then develop optimal anytime SSAs that generate improved solutions as more search time is allowed. Our SSAs for solving general constrained NLPs are based on the theory of discrete con-strained optimization using Lagrange multipliers that shows the equivalence between the set of constrained local minima (CLMdn) and the set of discrete-neighborhood saddle points (SPdn). To implement this theory, we propose a general procedural framework for locating an SPdn. By incorporating genetic algorithms in the framework, we evaluate new constrained search algorithms: constrained genetic algorithm (CGA) and combined constrained simulated annealing and genetic algorithm (CSAGA).",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 192,
    "label": 3,
    "text": "Design and Implementation of the OLOG Deductive Object-Oriented Database Management System . OLOG is a novel deductive database system for advanced  intelligent information system applications. It directly supports eective  storage, ecient access and inference of large amount of persistent data  with complex structures. It provides a SQL-like data denition language  and data manipulation language, and a declarative rule-based query language.  It combines the best of the deductive, object-oriented, and objectrelational  approaches in a uniform framework. This paper describes the  design and implementation of the OLOG system.  1 Introduction  Deductive, object-oriented, and object-relational databases are three important extensions of the traditional relational database technology. Deductive databases stem from the integration of logic programming and relational databases. It oers representational and operational uniformity, reasoning capabilities, recursion, declarative querying, ecient secondary storage access, etc. However, deductive databases based on relational databas...",
    "neighbors": [
      211,
      501,
      861
    ],
    "mask": "Train"
  },
  {
    "node_id": 193,
    "label": 3,
    "text": "Time Series Classification by Boosting Interval Based Literals A supervised classification method for temporal series, even multivariate, is presented. It is based on boosting very simple classifiers: clauses with one literal in the body. The background predicates are based on temporal intervals. Two types of predicates are used: i) relative predicates, such as \"increases\" and \"stays\", and ii) region predicates, such as \"always\" and \"sometime\", which operate over regions in the dominion of the variable. Experiments on di#erent data sets, several of them obtained from the UCI repositories, show that the proposed method is highly competitive with previous approaches.  Keywords: time series classification, interval based literals, boosting, machine learning.  1",
    "neighbors": [
      719
    ],
    "mask": "Test"
  },
  {
    "node_id": 194,
    "label": 4,
    "text": "LegORB and Ubiquitous CORBA The increasing popularity of ubiquitous computing and the new approaches for low-consumption, shortrange wireless connectivity will enable a future with hundreds of heterogeneous devices interconnected to achieve a common task. However, communication among those network enabled heterogeneous devices requires standard protocols and well defined interfaces.  While existing middleware architectures already offer standard mechanisms (DCOM, CORBA, JINI), they are, in most of the cases, not suitable for most of the heterogeneous devices. The resources required by those middleware solutions normally exceed the computational limits of the heterogeneous devices  We present in this paper a minimalist component-based Object Request Broker (ORB) that can be dynamically reconfigured and requires, for the smallest configuration, 6Kb of memory.  Introduction  The incoming ubiquitous computing trend allows the existence of collections of network-enabled devices attached to rooms, people and buildings....",
    "neighbors": [
      385
    ],
    "mask": "Train"
  },
  {
    "node_id": 195,
    "label": 1,
    "text": "Using Case-Based Reasoning to Acquire User Scheduling Preferences that Change over Time Production/Manufacturing scheduling typically involves the acquisition of user optimization preferences. The ill-structuredness of both the problem space and the desired objectives make practical scheduling problems difficult to formalize and costly to solve, especially when problem configurations and user optimization preferences change over time. This paper advocates an incremental revision framework for improving schedule quality and incorporating user dynamically changing preferences through Case-Based Reasoning. Our implemented system, called CABINS, records situation-dependent tradeoffs and consequences that result from schedule revision to guide schedule improvement. The preliminary experimental results show that CABINS is able to effectively capture both user static and dynamic preferences which are not known to the system and only exist implicitly in a extensional manner in the case base. 1 Introduction Scheduling deals with allocation of a limited set of resources to a nu...",
    "neighbors": [
      476
    ],
    "mask": "Train"
  },
  {
    "node_id": 196,
    "label": 0,
    "text": "The Adaptive Agent Architecture: Achieving FaultTolerance Using Persistent Broker Teams Brokers are used in many multi-agent systems for locating agents, for routing and sharing  information, for managing the system, and for legal purposes, as independent third parties.  However, these multi-agent systems can be incapacitated and rendered non-functional when the  brokers become inaccessible due to failures such as machine crashes, network breakdowns, and  process failures that can occur in any distributed software system.  We propose that the theory of teamwork can be used to create robust brokered architectures that  can recover from broker failures, and we present the Adaptive Agent Architecture (AAA) to show  the feasibility of this approach. The AAA brokers form a team with a joint commitment to serve  any agent that registers with the broker team as long as the agent remains registered with the  team. This commitment enables the brokers to substitute for each other when needed. A multiagent  system based on the AAA can continue to work despite broker failures as long...",
    "neighbors": [
      200,
      312,
      724,
      943
    ],
    "mask": "Validation"
  },
  {
    "node_id": 197,
    "label": 1,
    "text": "Feature Subset Selection Using A Genetic Algorithm Practical pattern classification and knowledge discovery problems require selection of a subset of attributes or features (from a much larger set) to represent the patterns to be classified. This paper presents an approach to the multi-criteria optimization problem of feature subset selection using a genetic algorithm. Our experiments demonstrate the feasibility of this approach for feature subset selection in the automated design of neural networks for pattern classification and knowledge discovery. 1 Introduction Many practical pattern classification tasks (e.g., medical diagnosis) require learning of an appropriate classification function that assigns a given input pattern (typically represented using a vector of attribute or feature values) to one of a finite set of classes. The choice of features, attributes, or measurements used to represent patterns that are presented to a classifier affect (among other things): ffl The accuracy of the classification function that can be learn...",
    "neighbors": [
      620,
      872
    ],
    "mask": "Train"
  },
  {
    "node_id": 198,
    "label": 3,
    "text": "Automatic I/O Hint Generation through Speculative Execution Aggressive prefetching is an effective technique for reducing the execution times of disk-bound applications; that is, applications that manipulate data too large or too infrequently used to be found in file or disk caches. While automatic prefetching approaches based on static analysis or historical access patterns are effective for some workloads, they are not as effective as manually-driven (programmer-inserted) prefetching for applications with irregular or input-dependent access patterns. In this paper, we propose to exploit whatever processor cycles are left idle while an application is stalled on I/O by using these cycles to dynamically analyze the application and predict its future I/O accesses. Our approach is to speculatively pre-execute the application's code in order to discover and issue hints for its future read accesses. Coupled with an aggressive hint-driven prefetching system, this automatic approach could be applied to arbitrary applications, and should be particularl...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 199,
    "label": 3,
    "text": "An Extended Entity-Relationship Approach to Data Management in Object-Oriented Systems Database programming in object-oriented systems can be supported by combining data  modelling and programming technologies such that a data model supports the management of  collections of objects where those objects are as specified by the underlying object-oriented programming  language. This approach is the basis of the object data management services (ODMS)  of the Comandos system. The ODMS data model provides constructs for the representation of  both entities and their relationships and further supports rich classification structures. To complement  the structural model, there is an operational model based on an algebra over collections  of objects.  1 Introduction  Object-oriented technologies are gaining in popularity as the basis for software development platforms. Meanwhile the family of entity-relationship data models retain their wide-spread use and popularity for conceptual modelling. How then can these two successful technologies be combined to support the development of ...",
    "neighbors": [
      153
    ],
    "mask": "Train"
  },
  {
    "node_id": 200,
    "label": 0,
    "text": "Multi-Agent Architectures as Organizational Structures A Multi-Agent System (MAS) is an organization of coordinated autonomous agents that interact in order to achieve particular, possible common goals. Considering real world organizations as an analogy, this paper proposes architectural styles for MAS which adopt concepts from organizational theories. The styles are modeled using the i* framework which o#ers the notions of actor, goal and actor dependency and specified in Formal Tropos. They are evaluated with respect to a set of software quality attributes, such as predictability or adaptability. In addition, we conduct a comparative study of organizational and conventional software architectures using the mobile robot control example from the Software Engineering literature. The research is conducted in the context of Tropos, a comprehensive software system development methodology.",
    "neighbors": [
      196,
      495,
      945,
      963
    ],
    "mask": "Validation"
  },
  {
    "node_id": 201,
    "label": 2,
    "text": "A Unifying Approach to HTML Wrapper Representation and Learning . The number, the size, and the dynamics of Internet information  sources bears abundant evidence of the need for automation in information  extraction. This calls for representation formalisms that match  the World Wide Web reality and for learning approaches and learnability  results that apply to these formalisms.  The concept of elementary formal systems is appropriately generalized to  allow for the representation of wrapper classes which are relevant to the  description of Internet sources in HTML format. Related learning results  prove that those wrappers are automatically learnable from examples.  This is setting the stage for information extraction from the Internet by  exploitation of inductive learning techniques.  1 Motivation  Today's online access to millions or even billions of documents in the World Wide Web is a great challenge to research areas related to knowledge discovery and information extraction (IE). The general task of IE is to locate specific pieces of text i...",
    "neighbors": [
      875,
      1122,
      1232
    ],
    "mask": "Test"
  },
  {
    "node_id": 202,
    "label": 1,
    "text": "Is Machine Colour Constancy Good Enough? . This paper presents a negative result: current machine colour constancy algorithms are not good enough for colour-based object recognition. This result has surprised us since we have previously used the better of these algorithms successfully to correct the colour balance of images for display. Colour balancing has been the typical application of colour constancy, rarely has it been actually put to use in a computer vision system, so our goal was to show how well the various methods would do on an obvious machine colour vision task, namely, object recognition. Although all the colour constancy methods we tested proved insufficient for the task, we consider this an important finding in itself. In addition we present results showing the correlation between colour constancy performance and object recognition performance, and as one might expect, the better the colour constancy the better the recognition rate. 1 Introduction We set out to show that machine colour constancy had matured to...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 203,
    "label": 3,
    "text": "A Runtime System for Interactive Web Services Interactive web services are increasingly replacing traditional static web pages. Producing web services seems to require a tremendous amount of laborious lowlevel coding due to the primitive nature of CGI programming. We present ideas for an improved runtime system for interactive web services built on top of CGI running on virtually every combination of browser and HTTP/CGI server. The runtime system has been implemented and used extensively in <bigwig>, a tool for producing interactive web services.  Keywords: CGI, Interactive Web Service, Web Document Management, Runtime System, Session Model. 1 Introduction  An interactive web service consists of a global shared state (typically a database) and a number of distinct sessions that each contain some local private state and a sequential, imperative action. A web client may invoke an individual thread of one of the given session kinds. The execution of this thread may interact with the client and inspect or modify the global state. One...",
    "neighbors": [
      146
    ],
    "mask": "Train"
  },
  {
    "node_id": 204,
    "label": 0,
    "text": "Intelligent Agents -- A New Technology for Future Distributed Sensor Systems? This master thesis deals with intelligent agents and the possibility to use the intelligent agent technology in future distributed sensor systems. The term future distributed sensor system refers to a system based on several sensors that will be developed within a period of five to ten years. Since researchers have not agreed on a more precise definition of intelligent agents, we first examined what constitutes an intelligent agent and made a definition suited for our application domain. We used our definition as a base for investigating if and how intelligent agents can be used in future distributed sensor systems. We argue that it is not interesting to come up with a general agent definition applicable to every agent, instead one should make a foundation for a definition. When this is done we can decide on more specific features depending on the task the agent will perform and in what domain the agent will work in. Finally we conclude that it is possible to use the agent technology i...",
    "neighbors": [
      854
    ],
    "mask": "Test"
  },
  {
    "node_id": 205,
    "label": 3,
    "text": "Efficient Computation of Temporal Aggregates with Range Predicates A temporal aggregation query is an important but costly operation for applications that maintain timeevolving  data (data warehouses, temporal databases, etc.). Due to the large volume of such data, performance  improvements for temporal aggregation queries are critical. In this paper we examine techniques to compute  temporal aggregates that include key-range predicates (range temporal aggregates). In particular we concentrate  on SUM, COUNT and AVG aggregates. This problem is novel; to handle arbitrary key ranges, previous  methods would need to keep a separate index for every possible key range. We propose an approach based  on a new index structure called the Multiversion SB-Tree, which incorporates features from both the SB-Tree  and the Multiversion B-Tree, to handle arbitrary key-range temporal SUM, COUNT and AVG queries. We  analyze the performance of our approach and present experimental results that show its efficiency.  1",
    "neighbors": [
      706
    ],
    "mask": "Train"
  },
  {
    "node_id": 206,
    "label": 4,
    "text": "The PLAY Research Group: Entertainment and Innovation in Sweden In a short time the research group PLAY has established an unorthodox but effective work style, where a creative approach to research in information technology is combined with a strong focus on achieving high-quality results. Being a young research group (both regarding the time it has existed and the average age of its members) has presented PLAY with both challenges and opportunities. We face the challenge of building a credible basis for research in the academic community, but also think that we have the opportunity to contribute innovative results to the research community and our industrial partners.  Keywords  HCI research groups, future HCI, European HCI, IT design  INTRODUCTION  How can one perform exciting and unorthodox research in information technology, while still assuring that results are useful and of good quality? How can a small group, consisting mostly of relatively inexperienced students, in a small country with very little traditions in groundbreaking IT research, ...",
    "neighbors": [
      854,
      869
    ],
    "mask": "Test"
  },
  {
    "node_id": 207,
    "label": 5,
    "text": "A Survey on Knowledge Compilation this paper we survey recent results in knowledge compilation of propositional knowledge bases. We first define and limit the scope of such a technique, then we survey exact and approximate knowledge compilation methods. We include a discussion of compilation for non-monotonic knowledge bases. Keywords: Knowledge Representation, Efficiency of Reasoning",
    "neighbors": [
      980
    ],
    "mask": "Test"
  },
  {
    "node_id": 208,
    "label": 0,
    "text": "More Realistic Human Behavior Models for Agents in Virtual Worlds: Emotion, Stress, and Value Ontologies This paper focuses on challenges to improving the behavioral realism of computer generated agents and attempts to reflect the state of the art in human behavior modeling with particular attention to value  ontologies, emotion, and stress in game-theoretic settings. The goal is to help those interested in constructing more realistic software agents for use in simulations, in virtual reality environments, and in training and performance aiding settings such as on the web or in embedded applications. This paper pursues this goal by providing a framework for better integrating the theories and models contained in the diverse human behavior modeling literatures, such as those that straddle physiological, cognitive and emotive processes; individual differences; emergent group and crowd behavior; and (punctuated) equilibria in social settings. The framework is based on widely available ontologies of world values and how these  and physiological factors might be construed emotively into subjective expected utilities to guide the  reactions and deliberations of agents. For example what makes one set of opponent groups differ from another? This framework serves as an extension of Markov decision processes appropriate for iterative play in game-theoretic settings, with particular emphasis on agent capabilities for redefining drama and for finding meta-games to counter the human player. This article presents the derivation of the framework and some initial results and lessons learned about integrating behavioral models into interactive dramas and meta-games that stimulate (systemic) thought and training doctrine.  1)",
    "neighbors": [
      267,
      1067
    ],
    "mask": "Train"
  },
  {
    "node_id": 209,
    "label": 3,
    "text": "Temporal Statement Modifiers A wide range of database applications manage time-varying data. Many temporal query languages have been proposed, each one the result of many carefully made yet subtly interacting design decisions. In this article we advocate a different approach to articulating a set of requirements, or desiderata, that directly imply the syntactic structure and core semantics of a temporal extension of an (arbitrary) nontemporal query language. These desiderata facilitate transitioning applications from a nontemporal query language and data model, which has received only scant attention thus far. The paper then introduces the notion of statement modifiers that provide a means of systematically adding temporal support to an existing query language. Statement modifiers apply to all query language statements, for example, queries, cursor definitions, integrity constraints, assertions, views, and data manipulation statements. We also provide a way to systematically add temporal support to an existing implementation. The result is a temporal query language syntax, semantics, and implementation that derives from first principles. We exemplify this approach by extending SQL-92 with statement modifiers. This extended language, termed ATSQL, is formally defined via a denotational-semantics-style mapping of",
    "neighbors": [
      397
    ],
    "mask": "Validation"
  },
  {
    "node_id": 210,
    "label": 3,
    "text": "Selecting and Materializing Horizontally Partitioned Warehouse Views Data warehouse views typically store large aggregate tables based on a subset of dimension attributes of the main data warehouse fact table. Aggregate views can be stored as 2  n  subviews of a data cube with n attributes. Methods have been proposed for selecting only some of the data cube views to materialize in order to speed up query response time, accommodate storage space constraint and reduce warehouse maintenance cost. This paper proposes a method for selecting and materializing views, which selects and horizontally fragments a view, recomputes the size of the stored partitioned view while deciding further views to select. # 2001 Elsevier Science B.V. All rights reserved.  Keywords: Data warehouse; Views; Fragmentation; Performance benet  1. Introduction  Decision support systems (DSS) used by business executives require analyzing snapshots of departmental databases over several periods of time. Departmental databases of the same organization (e.g., a bank) may be stored on dier...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 211,
    "label": 3,
    "text": "Deductive Database Languages: Problems and Solutions this paper, we discuss these problems from four different aspects: complex values, object orientation, higher-orderness, and updates. In each case, we examine four typical languages that address the corresponding issues.",
    "neighbors": [
      192
    ],
    "mask": "Train"
  },
  {
    "node_id": 212,
    "label": 2,
    "text": "Regression Models for Ordinal Data: A Machine Learning Approach In contrast to the standard machine learning tasks of classification and metric regression we investigate the problem of predicting variables of ordinal scale, a setting referred to as ordinal regression. The task of ordinal regression arises frequently in the social sciences and in information retrieval where human preferences play a major role. Also many multi--class problems are really problems of ordinal regression due to an ordering of the classes. Although the problem is rather novel to the Machine Learning Community it has been widely considered in Statistics before. All the statistical methods rely on a probability model of a latent (unobserved) variable and on the condition of stochastic ordering. In this paper we develop a distribution independent formulation of the problem and give uniform bounds for our risk functional. The main difference to classification is the restriction that the mapping of objects to ranks must be transitive and asymmetric. Combining our theoretical framework with results from measurement theory we present an approach that is based on a mapping from objects to scalar utility values and thus guarantees transitivity and asymmetry. Applying the principle of Structural Risk Minimization as employed in Support Vector Machines we derive a new learning algorithm based on large margin rank boundaries for the task of ordinal regression. Our method is easily extended to nonlinear utility functions. We give experimental results for an Information Retrieval task of learning the order of documents with respect to an initial query. Moreover, we show that our algorithm outperforms more naive approaches to ordinal regression such as Support Vector Classification and Support Vector Regression in the case of more than two ranks.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 213,
    "label": 2,
    "text": "Designing a Digital Library for Young Children: An Intergenerational Partnership As more information resources become accessible using computers, our digital interfaces to those resources need to be appropriate for all people. However when it comes to digital libraries, the interfaces have typically been designed for older children or adults. Therefore, we have begun to develop a digital library interface developmentally appropriate for young children (ages 5-10 years old). Our prototype system we now call \"QueryKids\" offers a graphical interface for querying, browsing and reviewing search results. This paper describes our motivation for the research, the design partnership we established between children and adults, our design process, the technology outcomes of our current work, and the lessons we have  learned.  Keywords  Children, digital libraries, information retrieval design techniques, education applications, participatory design, cooperative inquiry, intergenerational design team, zoomable user interfaces (ZUIs).  THE NEED FOR RESEARCH  A growing body of k...",
    "neighbors": [
      450,
      648
    ],
    "mask": "Train"
  },
  {
    "node_id": 214,
    "label": 1,
    "text": "A Sound Algorithm for Region-Based Image Retrieval Using an Index Region-based image retrieval systems aim to improve the effectiveness of content-based search by decomposing each image into a set of \"homogeneous\" regions. Thus, similarity between images is assessed by computing similarity between pairs of regions and then combining the results at the image level. In this paper we propose the first provably sound algorithm for performing region-based similarity search when regions are accessed through an index. Experimental results demonstrate the effectiveness of our approach, as also compared to alternative retrieval strategies. 1. Introduction  Many real world applications, in the field of medicine, weather prediction, and communications, to name a few, require efficient access to image databases based on content. To this end, the goal of content-based image retrieval (CBIR) systems is to define a set of properties (features) able to effectively characterize the content of images and then to use such features during retrieval. Users accessing a CB...",
    "neighbors": [
      1155
    ],
    "mask": "Validation"
  },
  {
    "node_id": 215,
    "label": 4,
    "text": "A Wearable Spatial Conferencing Space Wearable computers provide constant access to computing and communications resources. In this paper we describe how the computing power of wearables can be used to provide spatialized 3D graphics and audio cues to aid communication. The result is a wearable augmented reality communication space with audio enabled avatars of the remote collaborators surrounding the user. The user can use natural head motions to attend to the remote collaborators, can communicate freely while being aware of other side conversations and can move through the communication space. In this way the conferencing space can support dozens of simultaneous users. Informal user studies suggest that wearable communication spaces may offer several advantages, both through the increase in the amount of information it is possible to access and the naturalness of the interface.  1: Introduction  One of the broad trends emerging in human-computer interaction is the increasing portability of computing and communication fac...",
    "neighbors": [
      738,
      886,
      965,
      1043,
      1192
    ],
    "mask": "Train"
  },
  {
    "node_id": 216,
    "label": 2,
    "text": "Authoritative Sources in a Hyperlinked Environment The link structure of a hypermedia environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. Versions of this principle have been studied in the hypertext research community and (in a context predating hypermedia) through journal citation analysis in the field of bibliometrics. But for the problem of searching in hyperlinked environments such as the World Wide Web, it is clear from the prevalent techniques that the information inherent in the links has yet to be fully exploited. In this work we develop a new method for automatically extracting certain types of information about a hypermedia environment from its link structure, and we report on experiments that demonstrate its effectiveness for a variety of search problems on the www.  The central problem we consider is that of determining the relative \"authority\" of pages in such environments. This issue is central to a number of basic hypertext search t...",
    "neighbors": [
      43,
      112,
      115,
      133,
      247,
      322,
      427,
      482,
      526,
      612,
      649,
      845,
      867,
      893,
      1005,
      1165,
      1198
    ],
    "mask": "Validation"
  },
  {
    "node_id": 217,
    "label": 1,
    "text": "Hybrid Neural Systems   This chapter provides an introduction to the eld of hybrid  neural systems. Hybrid neural systems are computational systems which  are based mainly on articial neural networks but also allow a symbolic  interpretation or interaction with symbolic components. In this overview,  we will describe recent results of hybrid neural systems. We will give  a brief overview of the main methods used, outline the work that is  presented here, and provide additional references. We will also highlight  some important general issues and trends.",
    "neighbors": [
      654
    ],
    "mask": "Test"
  },
  {
    "node_id": 218,
    "label": 3,
    "text": "Query Optimization in the Presence of Limited Access Patterns 1 Introduction The goal of a query optimizer of a database system is to translate a declarative query expressed on a logical schema into an imperative query execution plan that accesses the physical storage of the data, and applies a sequence of relational operators. In building query execution plans, traditional relational query optimizers try to find the most efficient method for accessing the necessary data. When possible, a query optimizer will use auxiliary data structures such as an index on a file in order to efficiently retrieve a certain set of tuples in a relation. However, when such structures do not exist or are not useful for the given query, the alternative of scanning the entire relation always exists. The existence of the fall back option to perform a complete scan is an important assumption in traditional query optimization. Several recent query processing applications have the common characteristic that it is not always possible to perform complete scans on the data. Instead, the query optimization problem is complicated by the fact that there are only limited access patterns to the data. One such",
    "neighbors": [
      419,
      420,
      879,
      1219
    ],
    "mask": "Train"
  },
  {
    "node_id": 219,
    "label": 2,
    "text": "Text and Image Metasearch on the Web As the Web continues to increase in size, the relative coverage of Web search engines is decreasing, and search tools that combine the results of multiple search engines are becoming more valuable. This paper provides details of the text and image metasearch functions of the Inquirus search engine developed at the NEC Research Institute. For text metasearch, we describe features including the use of link information in metasearch, and provide statistics on the usage and performance of Inquirus and the Web search engines. For image metasearch, Inquirus queries multiple image search engines on the Web, downloads the actual images, and creates image thumbnails for display to the user. Inquirus handles image search engines that return direct links to images, and engines that return links to HTML pages. For the engines that return HTML pages, Inquirus analyzes the text on the pages in order to predict which images are most likely to correspond to the query. The individual image search engin...",
    "neighbors": [
      224,
      587,
      774,
      1017,
      1031,
      1189
    ],
    "mask": "Train"
  },
  {
    "node_id": 220,
    "label": 4,
    "text": "Control Law Design for Haptic Interfaces to Virtual Reality The goal of control law design for haptic displays is to provide a safe and stable user interface while maximizing the operator's sense of kinesthetic immersion in a virtual environment. This paper outlines a control design approach which stabilizes a haptic interface when coupled to a broad class of human operators and virtual environments. Two-port absolute stability criteria are used to develop explicit control law design bounds for two different haptic display implementations: impedance display and admittance display. The strengths and weaknesses of each approach are illustrated through numerical and experimental results for a three degree-offreedom device. The example highlights the ability of the proposed design procedure to handle some of the more difficult problems in control law synthesis for haptics, including structural flexibility and non-collocation of sensors and actuators. The authors are with the Department of Electrical Engineering University of Washington, Box 352500 Seattle, WA 98195-2500  2  I.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 221,
    "label": 3,
    "text": "Improving Business Process Quality through Exception Understanding, Prediction, and Prevention Business process automation technologies are  being increasingly used by many companies to  improve the efficiency of both internal processes  as well as of e-services offered to customers. In  order to satisfy customers and employees,  business processes need to be executed with a  high and predictable quality. In particular, it is  crucial for organizations to meet the Service  Level Agreements (SLAs) stipulated with the  customers and to foresee as early as possible the  risk of missing SLAs, in order to set the right  expectations and to allow for corrective actions.  In this paper we focus on a critical issue in  business process quality: that of analyzing,  predicting and preventing the occurrence of  exceptions, i.e., of deviations from the desired or  acceptable behavior. We characterize the  problem and propose a solution, based on data  warehousing and mining techniques. We then  describe the architecture and implementation of a  tool suite that enables exception analysis,  prediction, and prevention. Finally, we show  experimental results obtained by using the tool  suite to analyze internal HP processes.  1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 222,
    "label": 0,
    "text": "Formalizing Collaborative Decision-making and Practical Reasoning in Multi-agent Systems In this paper, we present an abstract formal model of decision-making in a social setting that covers all aspects of the process, from recognition of a potential for cooperation through to joint decision. In a multi-agent environment, where self-motivated autonomous agents try to pursue their own goals, a joint decision cannot be taken for granted. In order to decide effectively, agents need the ability to (a) represent and maintain a model of their own mental attitudes, (b) reason about other agents' mental attitudes, and (c) influence other agents' mental states. Social mental shaping is advocated as a general mechanism for attempting to have an impact on agents' mental states in order to increase their cooperativeness towards a joint decision. Our approach is to specify a novel, high-level architecture for collaborative decision-making in which the mentalistic notions of belief, desire, goal, intention, preference and commitment play a central role in guiding the individual agent's and the group's decision-making behaviour. We identify preconditions that must be fulfilled before collaborative decision-making can commence and prescribe how cooperating agents should behave, in terms of their own decision-making apparatus and their interactions with others, when the decision-making process is progressing satisfactorily. The model is formalized through a new, many-sorted, multi-modal logic.",
    "neighbors": [
      557,
      724
    ],
    "mask": "Validation"
  },
  {
    "node_id": 223,
    "label": 0,
    "text": "Plan Recognition in Military Simulation: Incorporating Machine Learning with Intelligent Agents A view of plan recognition shaped by both  operational and computational requirements is  presented. Operational requirements governing  the level of fidelity and nature of the reasoning  process combine with computational requirements  including performance speed and software  engineering effort to constrain the types  of solutions available to the software developer.  By adopting machine learning to provide  spatio-temporal recognition of environmental  events and relationships, an agent can be provided  with a mechanism for mental state recognition  qualitatively different from previous research.  An architecture for integrating machine  learning into a BDI agent is suggested and the  results from the development of a prototype  provide proof-of-concept.  1 Introduction  This paper proposes machine-learning as a tool to assist in the construction of agents capable of plan recognition. This paper focuses on the beliefs-desires-intentions (BDI) class of agents. These agents have been ...",
    "neighbors": [
      277,
      303,
      816
    ],
    "mask": "Train"
  },
  {
    "node_id": 224,
    "label": 2,
    "text": "Searching the world wide web The coverage and recency of the major World Wide Web search engines was analyzed, yielding some surprising results. The coverage of any one engine is significantly limited: No single engine indexes more than about one-third of the \u201cindexable Web, \u201d the coverage of the six engines investigated varies by an order of magnitude, and combining the results of the six engines yields about 3.5 times as many documents on average as compared with the results from only one engine. Analysis of the overlap between pairs of engines gives an estimated lower bound on the size of the indexable Web of 320 million pages. The Internet has grown rapidly since its inception in December 1969 (1) and is anticipated to expand 1000 % over the next few years (2). The amount of scientific information and the number of electronic journals on the Internet continue to increase [about 1000 journals as of 1996 (2, 3)]. The Internet and the World Wide Web (the Web) represent significant advancements for the retrieval and dissemination of scientific and other literature and for the advancement of education (2, 4). With the introduction of full-text search engines such as AltaVista (www.",
    "neighbors": [
      115,
      143,
      219,
      271,
      382,
      433,
      466,
      496,
      526,
      649,
      696,
      792,
      1005,
      1031,
      1066,
      1124,
      1183,
      1253
    ],
    "mask": "Test"
  },
  {
    "node_id": 225,
    "label": 3,
    "text": "DB-Prism: Integrated Data Warehouses and Knowledge Networks for Bank Controlling DB-Prism is an integrated data warehouse system",
    "neighbors": [
      1022
    ],
    "mask": "Train"
  },
  {
    "node_id": 226,
    "label": 3,
    "text": "Efficiently Computing Weighted Proximity Relationships in Spatial Databases Spatial data mining recently emerges from a number of real applications, such as real-estate marketing, urban planning, weather forecasting, medical image analysis, road traffic accident analysis, etc. It demands for efficient solutions for many new, expensive, and complicated problems. In this paper, we investigate the problem of evaluating the top k distinguished \"features\" for a \"cluster\" based on weighted proximity relationships between the cluster and features. We measure proximity in an average fashion to address possible nonuniform data distribution in a cluster. Combining a standard multi-step paradigm with new lower and upper proximity bounds, we presented an efficient algorithm to solve the problem. The algorithm is implemented in several different modes. Our experiment results not only give a comparison among them but also illustrate the efficiency of the algorithm.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 227,
    "label": 0,
    "text": "TravelPlan: A MultiAgent System to Solve Web Electronic Travel Problems This paper presents TravelPlan, a multiagent architecture to co-operative work between different elements (human and/or software) whose main goal is to recommend useful solutions in the electronic tourism domain to system users. The system uses different types of intelligent autonomous agents whose main characteristics are cooperation, negotiation, learning, planning and knowledge sharing. The information used by the intelligent agents is heterogeneous and geographically distributed. The main information source of the system is Internet (the web). Other information sources are distributed knowledge bases in the own system.. The process to obtain, filter, and store the information is performed automatically by agents. This information is translated into a homogeneous format for high-level reasoning in order to obtain different partial solutions. Partial solutions are reconstructed into a general solution (or solutions) to be presented to the user. The system will recommend different solution...",
    "neighbors": [
      261
    ],
    "mask": "Test"
  },
  {
    "node_id": 228,
    "label": 2,
    "text": "Partitioning-Based Clustering for Web Document Categorization Clustering techniques have been used by manyintelligent software agents in order to retrieve, lter, and categorize documents available on the World Wide Web. Clustering is also useful in extracting salient features of related web documents to automatically formulate queries and search for other similar documents on the Web. Traditional clustering algorithms either use a priori knowledge of document structures to de ne a distance or similarity among these documents, or use probabilistic techniques such as Bayesian classi cation. Many of these traditional algorithms, however, falter when the dimensionality of the feature space becomes high relative to the size of the document space. In this paper, we introduce two new clustering algorithms that can e ectively cluster documents, even in the presence of a very high dimensional feature space. These clustering techniques, which are based on generalizations of graph partitioning, do not require pre-speci ed ad hoc distance functions, and are capable of automatically discovering document similarities or associations. We conduct several experiments on real Web data using various feature selection heuristics, and compare our clustering schemes to standard distance-based techniques, such ashierarchical agglomeration clustering, and Bayesian classi cation methods, such as AutoClass.",
    "neighbors": [
      291,
      616,
      780
    ],
    "mask": "Test"
  },
  {
    "node_id": 229,
    "label": 4,
    "text": "LART: flexible, low-power building blocks for wearable computers To ease the implementation of different wearable computers, we developed a low-power processor board (named LART) with a rich set of interfaces. The LART supports dynamic voltage scaling, so performance (and power consumption) can be scaled to match demands: 59-221 MHz, 106-640 mW. High-end wearables can be configured from multiple LARTs operating in parallel; alternatively, FPGA boards can be used for dedicated data-processing, which reduces power consumption significantly.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 230,
    "label": 1,
    "text": "Case-based Learning for Knowledge-based Design Support . We present a general approach to combine methods of interactive knowledge acquisition with methods for machine learning. The approach has been developed in order to deliver knowledge required by support-systems for design-tasks. Learning rests upon a knowledge representation scheme for cases that distinguishes between knowledge needed for subgoaling and knowledge needed for design. We employ traces, i.e., protocols of the user's actions when tackling design-tasks as the initial input for incremental knowledge acquisition. This allows to learn task structures to be used for subgoaling and case-bases plus similarity relations applicable to particular case-bases. 1 INTRODUCTION Integrating incremental learning into a knowledge-based systems seems to be a promising way to lessen the burden of knowledge elicitation to system development [9]. The goal of this paper is to point out how learning can be used in an interactive design-support system that uses Cbr [8] as the main problem solvin...",
    "neighbors": [
      902
    ],
    "mask": "Train"
  },
  {
    "node_id": 231,
    "label": 1,
    "text": "A Genetic Algorithm-Based Solution for the Problem of Small Disjuncts . In essence, small disjuncts are rules covering a small number of  examples. Hence, these rules are usually error-prone, which contributes to a  decrease in predictive accuracy. The problem is particularly serious because,  although each small disjuncts covers few examples, the set of small disjuncts  can cover a large number of examples. This paper proposes a solution to the  problem of discovering accurate small-disjunct rules based on genetic algorithms.  The basic idea of our method is to use a hybrid decision tree / genetic  algorithm approach for classification. More precisely, examples belonging to  large disjuncts are classified by rules produced by a decision-tree algorithm,  while examples belonging to small disjuncts are classified by a new genetic  algorithm, particularly designed for discovering small-disjunct rules.  1 Introduction  In the context of the well-known classification task of data mining, the discovered knowledge is often expressed as a set of IF-THEN predict...",
    "neighbors": [
      832
    ],
    "mask": "Train"
  },
  {
    "node_id": 232,
    "label": 3,
    "text": "Building a Large Location Table to Find Replicas of Physics Objects The problem of building a large location table for physics objects occurs within a number of planned physics data management systems, like those that control reclustering and wide-area replication. To satisfy their e#ciency goals, these systems have to make local or remote replicas of individual physics objects, which contain raw or reconstructed data for a single event, rather than replicas of large run or ntuple files. This replication implies the use of a table to resolve the logical, location independent object descriptor into a physical location where an object replica can be found. For modern physics experiments the table needs to scale to at least some 10  10  objects. We argue that such a table can be e#ciently implemented by limiting the freedom of lookup operations, and by exploiting some specific properties of the physics data model. One specific viable implementation is discussed.  Key words: Object location table, object-oriented databases, object clustering,  object re-cl...",
    "neighbors": [
      1019
    ],
    "mask": "Train"
  },
  {
    "node_id": 233,
    "label": 2,
    "text": "Generating a Topically Focused VirtualReality Internet Surveys highlight that Internet users are frequently frustrated by failing to locate useful information, and by difficulty in browsing anarchically linked web-structures. We present a new Internet browsing application (called VR-net) that addresses these problems. It first identifies semantic domains consisting of tightly interconnected web-page groupings. The second part populates a 3D virtual world with these information sources, representing all relevant pages plus appropriate structural relations. Users can then easily browse through around a semantically focused virtual library. 1 Introduction  The Internet is a probably the most significant global information resource ever created, allowing access to an almost unlimited amount of information. In this paper we describe two inter-related difficulties suffered by Internet users, and their combined influence on web use. We then introduce an integrated \"search and browse\" solution tool that directly tackles both issues. We also examin...",
    "neighbors": [
      587,
      923,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 234,
    "label": 4,
    "text": "An Adaptive Self-Organizing Color Segmentation Algorithm with Application to Robust Real-time Human Hand Localization In Proc. Asian Conf. on Computer Vision, Taiwan, 2000  This paper describes an adaptive self-organizing color segmentation algorithm and a transductive learning algorithm used to localize human hand in video sequences. The color distribution at each time frame is approximated by the proposed 1-D self-organizing map (SOM), in which schemes of growing, pruning and merging are facilitated to find an appropriate number of color cluster automatically. Due to the dynamic backgrounds and changing lighting conditions, the distribution of color over time may not be stationary. An algorithm of SOM transduction is proposed to learn the nonstationary color distribution in HSI color space by combining supervised and unsupervised learning paradigms. Color cue and motion cue are integrated in the localization system, in which motion cue is employed to focus the attention of the system. This approach is also applied to other tasks such as human face tracking and color indexing. Our localization system...",
    "neighbors": [
      167,
      635
    ],
    "mask": "Test"
  },
  {
    "node_id": 235,
    "label": 2,
    "text": "Collection Synthesis The invention of the hyperlink and the HTTP transmission protocol caused an amazing new structure to appear on the Internet -- the World Wide Web. With the Web, there came spiders, robots, and Web crawlers, which go from one link to the next checking Web health, ferreting out information and resources, and imposing organization on the huge collection of information (and dross) residing on the net. This paper reports on the use of one such crawler to synthesize document collections on various topics in science, mathematics, engineering and technology. Such collections could be part of a digital library.",
    "neighbors": [
      291,
      410,
      536,
      662,
      774,
      968,
      1000,
      1017,
      1099
    ],
    "mask": "Train"
  },
  {
    "node_id": 236,
    "label": 0,
    "text": "A Multiagent Architecture For Fuzzy Modeling In this paper a hybrid learning system that combines different fuzzy modeling techniques  is being investigated. In order to implement the different methods, we propose  the use of intelligent agents, which collaborate by means of a multiagent architecture.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 237,
    "label": 5,
    "text": "Probabilistic Deduction with Conditional Constraints over Basic Events We study the problem of probabilistic deduction with conditional constraints over basic events. We show that globally complete probabilistic deduction with conditional constraints over basic events is NP-hard. We then concentrate on the special case of probabilistic deduction in conditional constraint trees. We elaborate very efficient techniques for globally complete probabilistic deduction. In detail, for conditional constraint trees with point probabilities, we present a local approach to globally complete probabilistic deduction, which runs in linear time in the size of the conditional constraint trees. For conditional constraint trees with interval probabilities, we show that globally complete probabilistic deduction can be done in a global approach by solving nonlinear programs. We show how these nonlinear programs can be transformed into equivalent linear programs, which are solvable in polynomial time in the size of the conditional constraint trees. 1. Introduction  Dealing wit...",
    "neighbors": [
      803,
      1015
    ],
    "mask": "Train"
  },
  {
    "node_id": 238,
    "label": 4,
    "text": "Exploring Brick-Based Navigation and Composition in an Augmented Reality . BUILD-IT is a planning tool based on computer vision technology,  supporting complex planning and composition tasks. A group of people, seated  around a table, interact with objects in a virtual scene using real bricks. A plan  view of the scene is projected onto the table, where object manipulation takes  place. A perspective view is projected on the wall. The views are set by virtual  cameras, having spatial attributes like shift, rotation and zoom. However, planar  interaction with bricks provides only position and rotation information. Object  height control is equally constrained by planar interaction. The aim of this paper  is to suggest methods and tools bridging the gap between planar interaction and  three-dimensional control. To control camera attributes, active objects, with intelligent  behaviour are introduced. To control object height, several real and  virtual tools are suggested. Some of the solutions are based on metaphors, like  window, sliding-ruler and floor.  1 I...",
    "neighbors": [
      467
    ],
    "mask": "Train"
  },
  {
    "node_id": 239,
    "label": 3,
    "text": "On2broker: Semantic-Based Access to Information Sources at the WWW On2broker provides brokering services to improve access to heterogeneous, distributed and semistructured information sources as they are presented in the World Wide Web. It relies on the use of ontologies to make explicit the semantics of web pages. In the paper we will discuss the general architecture and main components of On2broker and provide some application scenarios.  1. Introduction  In the paper we describe a tool environment called On2broker  1  that processes information sources and content descriptions in HTML, XML, and RDF and that provides intelligent information retrieval, query answering and maintenance support. Central for our approach is the use of ontologies to describe background knowledge and to make explicit the semantics of web documents. Ontologies have been developed in the area of knowledge-based systems for structuring and reusing large bodies of knowledge (cf. CYC [Lenat, 1995], (KA)2 [Benjamins et al., 1998]). Ontologies are consensual and formal specificat...",
    "neighbors": [
      188,
      835
    ],
    "mask": "Test"
  },
  {
    "node_id": 240,
    "label": 4,
    "text": "A Pattern-Supported Approach to the User Interface Design Process Patterns describe generic solutions to common problems in context. Originating from the world of architecture, patterns have been used mostly in object-oriented programming and data analysis. The goal of HCI patterns is to create an inventory of solutions to help designers (and usability engineers) to resolve UI development problems that are common, difficult and frequently encountered. In this paper, we present our pattern-supported approach to user interface design in the context of information visualization. Using a concrete example from the telecommunications domain, we will focus on a task/subtask pattern to illustrate how knowledge about a task and an appropriate interaction design solution can be captured and communicated.  1",
    "neighbors": [
      15,
      287
    ],
    "mask": "Train"
  },
  {
    "node_id": 241,
    "label": 2,
    "text": "Automatic Discovery of Language Models for Text Databases The proliferation of text databases within large organizations and on the Internet makes it difficult for a person to know which databases to search. Given language models  that describe the contents of each database, a database selection  algorithm such as GlOSS can provide assistance by automatically selecting appropriate databases for an information need. Current practice is that each database provides its language model upon request, but this cooperative  approach has important limitations. This paper demonstrates that cooperation is not required. Instead, the database selection service can construct its own language models by sampling database contents via the normal process of running queries and retrieving documents. Although random sampling is not possible, it can be approximated with carefully selected queries. This sampling  approach avoids the limitations that characterize the cooperative approach, and also enables additional capabilities. Experimental results demonstrate th...",
    "neighbors": [
      115,
      433,
      502,
      510,
      526,
      579,
      599,
      627,
      792,
      1253
    ],
    "mask": "Train"
  },
  {
    "node_id": 242,
    "label": 2,
    "text": "Combining Labeled and Unlabeled Data for Text Classification With a Large Number of Categories A major concern with supervised learning techniques for text classification  is that they often require a large number of labeled examples to learn  accurately. One way to reduce the amount of labeled data required is to  develop algorithms that can learn effectively from a small number of labeled  examples augmented with a large number of unlabeled examples.  In this paper, we develop a framework to incorporate unlabeled data in  the Error-Correcting Output Coding (ECOC) setup by decomposing multiclass  problems into multiple binary problems and then use Co-Training  to learn the individual binary classification problems. We show that our  method is especially useful for classification tasks involving a large number  of categories where Co-training doesn't perform very well by itself  and when combined with ECOC, outperforms several other algorithms  that combine labeled and unlabeled data for text classification.  1",
    "neighbors": [
      439,
      609,
      1133,
      1153
    ],
    "mask": "Train"
  },
  {
    "node_id": 243,
    "label": 2,
    "text": "Hierarchical Wrapper Induction for Semistructured Information Sources With the tremendous amount of information that becomes available on the Web on a daily basis, the abilitytoquickly develop information agents has become a crucial problem. A vital componentofanyWeb-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex documentinto a series of simpler extraction tasks. We introduce an inductive algorithm, stalker, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results showthatstalker requires up to two orders of magnitude fewer examples than other algorithms. Furthermore,    can wrap information sources that could not be wrapped by existing inductivetechniques.",
    "neighbors": [
      169,
      855,
      859
    ],
    "mask": "Train"
  },
  {
    "node_id": 244,
    "label": 1,
    "text": "Vision-Guided Navigation Using SHOSLIF . This paper presents an unconventional approach to vision-guided autonomous navigation. The system recalls information about scenes and navigational experience using content-based retrieval from a visual database. To achieve a high applicability to various road types, we do not impose a priori scene features, such as road edges, that the system must use. But rather, the system automatically derives features from images during supervised learning. To accomplish this, the system uses principle component analysis and linear discriminant analysis to automatically derive the most expressive features (MEF) for scene reconstruction or the most discriminating features (MDF) for scene classi cation. These features best describe or classify the population of the scenes and approximate complex decision regions using piecewise linear boundaries up to a desired accuracy. A new self-organizing scheme called recursive partition tree (RPT) is used for automatic construction of a vision-and-control da...",
    "neighbors": [
      166,
      652
    ],
    "mask": "Train"
  },
  {
    "node_id": 245,
    "label": 4,
    "text": "Learning and Tracking Human Motion Using Functional Analysis We present a method for the modeling and tracking of human motion using a sequence of 2D video images. Our analysis is divided in two parts: statistical learning and Bayesian tracking. First, we estimate a statistical model of typical activities from a large set of 3D human motion data. For this purpose, the human body is represented as a set of articulated cylinders and the evolution of a particular joint angle is described by a time-series. Specifically, we consider periodic motion such as \u201cwalking \u201d in this work, and we develop a new set of tools that allows for the automatic segmentation of the training data into a sequence of identical \u201cmotion cycles\u201d. Then we compute the mean and the principal components of these cycles using a new algorithm to account for missing information and to enforce smooth transitions between different cycles. The learned temporal model provides a prior probability distribution over human motions which is used for tracking. We adopt a Bayesian perspective and approximate the posterior distribution of the body parameters using a particle filter. The resulting algorithm is able to track human subjects in monocular video sequences and to recover their 3D motion in complex unknown environments. 1",
    "neighbors": [
      136
    ],
    "mask": "Validation"
  },
  {
    "node_id": 246,
    "label": 0,
    "text": "Engineering Mobile-agent Applications via Context-dependent Coordination The design and development of Internet applications can take advantage of a paradigm based on autonomous and mobile agents. However, mobility introduces peculiar coordination problems in agent-based Internet applications. First, it suggests the exploitation of an infrastructure based on a multiplicity of local interaction spaces. Second, it may require coordination activities to be adapted both to the characteristics of the execution environment where they occur and to the needs of the application to which the coordinating agents belong. In this context, this paper introduces the concept of context-dependent coordination based on programmable interaction spaces. On the one hand, interaction spaces associated to different execution environments may be independently programmed so as to lead to differentiated, environment-dependent, behaviors. On the other hand, agents can program the interaction spaces of the visited execution environments to obtain an application-dependent behavior of the interaction spaces themselves. Several examples show how an infrastructure enforcing context-dependent coordination can be effectively exploited to simplify and make more modular the design of Internet applications based on mobile agents. In addition, the MARS coordination infrastructure is presented as an example of a system in which the concept of context-dependent coordination has found a clean and efficient implementation.",
    "neighbors": [
      119,
      269,
      421,
      1065
    ],
    "mask": "Train"
  },
  {
    "node_id": 247,
    "label": 2,
    "text": "The Intelligent Surfer: Probabilistic Combination of Link and Content Information in PageRank The PageRank algorithm, used in the Google search engine, greatly  improves the results of Web search by taking into account the link  structure of the Web. PageRank assigns to a page a score proportional  to the number of times a random surfer would visit that page,  if it surfed indefinitely from page to page, following all outlinks  from a page with equal probability. We propose to improve PageRank  by using a more intelligent surfer, one that is guided by a  probabilistic model of the relevance of a page to a query. Efficient  execution of our algorithm at query time is made possible by precomputing  at crawl time (and thus once for all queries) the necessary  terms. Experiments on two large subsets of the Web indicate  that our algorithm significantly outperforms PageRank in the (human  -rated) quality of the pages returned, while remaining efficient  enough to be used in today's large search engines.",
    "neighbors": [
      216,
      427,
      774,
      1000,
      1017,
      1189
    ],
    "mask": "Validation"
  },
  {
    "node_id": 248,
    "label": 3,
    "text": "Recent Advances and Research Problems in Data Warehousing . In the recent years, the database community has witnessed the emergence of a new technology, namely data warehousing. A data warehouse is a global repository that stores pre-processed queries on data which resides in multiple, possibly heterogeneous, operational or legacy sources. The information stored in the data warehouse can be easily and efficiently accessed for making effective decisions. The On-Line Analytical Processing (OLAP) tools access data from the data warehouse for complex data analysis, such as multidimensional data analysis, and decision support activities. Current research has lead to new developments in all aspects of data warehousing, however, there are still a number of problems that need to be solved for making data warehousing effective. In this paper, we discuss recent developments in data warehouse modelling, view maintenance, and parallel query processing. A number of technical issues for exploratory research are presented and possible solutions are discusse...",
    "neighbors": [
      7
    ],
    "mask": "Train"
  },
  {
    "node_id": 249,
    "label": 0,
    "text": "Learning Environmental Features for Pose Estimation We present a method for learning a set of environmental features which are useful for pose estimation. The landmark learning mechanism is designed to be applicable to a wide range of environments, and generalized for di#erent sensing modilities. In the context of computer vision, each landmark is detected as a local extremum of a measure of distinctiveness and represented by an appearance-based encoding which is exploited for matching. The set of obtained landmarks can be parameterized and then evaluated in terms of their utility for the task at hand. The method is used to motivate a general approach to task-oriented sensor fusion. We present experimental evidence that demonstrates the utility of the method. 1 Introduction In this paper, we develop an approach to sensorbased robot localization by learning a set of recognizable features in the robot's environment. In particular, we consider the problem of learning a set of image-domain landmarks from a set of di#erent views of a scene. ...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 250,
    "label": 0,
    "text": "Team Formation by Self-Interested Mobile Agents . A process of team formation by autonomous agents in a distributed  environment is presented. Since the environment is distributed,  there are serious problems with communication and consistent decision  making inside a team. To deal with these problems, the standard technique  of token passing in a computer network is applied. The passing  cycle of the token serves as the communication route. It assures consistent  decision making inside the team maintaining its organizational  integrity. On the other hand it constitutes a component of the plan of  the cooperative work performed by a complete team. Two algorithms for  team formation are given. The first one is based on simple self-interested  agents that still can be viewed as reactive agents (see [14]) although augmented  with knowledge, goal, and cooperation mechanisms. The second  one is based on sophisticated self-interested agents. Moreover, the algorithm  based on fully cooperative agents, which is an adaptation of the  static ...",
    "neighbors": [
      724,
      1156,
      1266
    ],
    "mask": "Train"
  },
  {
    "node_id": 251,
    "label": 1,
    "text": "Data Mining At The Interface Of Computer Science And Statistics This chapter is written for computer scientists, engineers, mathematicians, and scientists who wish to gain a better understanding of the role of statistical thinking in modern data mining. Data mining has attracted considerable attention both in the research and commercial arenas in recent years, involving the application of a variety of techniques from both computer science and statistics. The chapter discusses how computer scientists and statisticians approach data from different but complementary viewpoints and highlights the fundamental differences between statistical and computational views of data mining. In doing so we review the historical importance of statistical contributions to machine learning and data mining, including neural networks, graphical models, and flexible predictive modeling. The primary conclusion is that closer integration of computational methods with statistical thinking is likely to become increasingly important in data mining applications.  Keywords: Data mining, statistics, pattern recognition, transaction data, correlation. 1.",
    "neighbors": [
      443,
      614
    ],
    "mask": "Train"
  },
  {
    "node_id": 252,
    "label": 0,
    "text": "On the Robustness of some Cryptographic Protocols for Mobile Agent Protection Mobile agent security is still a young discipline and most naturally,  the focus up to the time of writing was on inventing new cryptographic protocols  for securing various aspects of mobile agents. However, past experience shows  that protocols can be flawed, and flaws in protocols can remain unnoticed for a  long period of time. The game of breaking and fixing protocols is a necessary  evolutionary process that leads to a better understanding of the underlying problems  and ultimately to more robust and secure systems. Although, to the best of  our knowledge, little work has been published on breaking protocols for mobile  agents, it is inconceivable that the multitude of protocols proposed so far are all  flawless. As it turns out, the opposite is true. We identify flaws in protocols proposed  by Corradi et al., Karjoth et al., and Karnik et al., including protocols based  on secure co-processors.",
    "neighbors": [
      593
    ],
    "mask": "Train"
  },
  {
    "node_id": 253,
    "label": 3,
    "text": "Indexing the Distance: An Efficient Method to KNN Processing In this paper, we present an efficient method, called iDistance, for K-nearest neighbor (KNN) search in a high-dimensional space. iDistance partitions the data and selects a reference point for each partition. The data in each cluster are transformed into a single dimensional space based on their similarity with respect to a reference point. This allows the points to be indexed using a B + -tree structure and KNN search be performed using one-dimensional range search. The choice of partition and reference point provides the iDistance technique with degrees of freedom most other techniques do not have. We describe how appropriate choices here can effectively adapt the index structure to the data distribution. We conducted extensive experiments to evaluate the iDistance technique, and report results demonstrating its effectiveness.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 254,
    "label": 4,
    "text": "A Practical Approach for Recovery of Evicted Variables SRC\u2019s charter is to advance the state of the art in computer systems by doing basic and applied research in support of our company\u2019s business objectives. Our interests and projects span scalable systems (including hardware, networking, distributed systems, and programming-language technology), the Internet (including the Web, e-commerce, and information retrieval), and human/computer interaction (including user-interface technology, computer-based appliances, and mobile computing). SRC was established in 1984 by Digital Equipment Corporation. We test the value of our ideas by building hardware and software prototypes and assessing their utility in realistic settings. Interesting systems are too complex to be evaluated solely in the abstract; practical use enables us to investigate their properties in depth. This experience is useful in the short term in refining our designs and invaluable in the long term in advancing our knowledge. Most of the major advances in information systems have come through this approach, including personal computing, distributed systems, and the Internet. We also perform complementary work of a more mathematical character. Some of",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 255,
    "label": 2,
    "text": "Target Seeking Crawlers and their Topical Performance Topic driven crawlers can complement search engines by targeting relevant portions of the Web. A topic driven crawler must exploit the information available about the topic and its underlying context. In this paper we extend our previous research on the design and evaluation of topic driven crawlers by comparing seven different crawlers on a harder problem, namely, seeking highly relevant target pages. We find that exploration is an important aspect of a crawling strategy. We also study how the performance of crawler strategies depends on a number of topical characteristics based on notions of topic generality, cohesiveness, and authoritativeness. Our results reveal that topic generality is an obstacle for most crawlers, that three crawlers tend to perform better when the target pages are clustered together, and that two of these also display better performance when topic targets are highly authoritative.",
    "neighbors": [
      281,
      662,
      774,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 256,
    "label": 4,
    "text": "Nexus - An Open Global Infrastructure for Spatial-Aware Applications Due to the lack of a generic platform for location- and spatial-aware systems, many basic services have to be reimplemented in each application that uses spatial-awareness. A cooperation among different applications is also difficult to achieve without a common platform. In this paper we present a platform that solves these problems. It provides an infrastructure that is based on computer models of regions of the physical world, which are augmented by virtual objects. We show how virtual objects make the integration of existing information systems and services in spatial-aware systems easier. Furthermore, our platform supports interactions between the computer models and the real world and integrates single models in a global \"Augmented World\".  Contents  1 Introduction 3 2 General Idea 4  2.1 Augmented Areas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Augmented World . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3 Example Scenario 6 4 Require...",
    "neighbors": [
      438
    ],
    "mask": "Test"
  },
  {
    "node_id": 257,
    "label": 0,
    "text": "Compositional Design and Reuse of a Generic Agent Model This paper introduces a formally specified design of a compositional generic agent model (GAM). This  agent model abstracts from specific application domains; it provides a unified formal definition of a model  for weak agenthood. It can be (re)used as a template or pattern for a large variety of agent types and  application domain types. The model was designed on the basis of experiences in a number of application  domains. The compositional development method DESIRE was used to design the agent model GAM at a  conceptual and logical level. It serves as a unified, precisely defined coneptual structure which can be  refined by specialisation and instantiation to a large variety of other, more specific agents. To illustrate  reuse of this agent model, specialisation and instantiation to model co-operative information gathering  agents is described in depth. Moreover, it is shown how GAM can be used to describe in a unified and  hence more comparable manner a large number of agent architectures from the literature.",
    "neighbors": [
      625,
      942
    ],
    "mask": "Train"
  },
  {
    "node_id": 258,
    "label": 2,
    "text": "JRes: A Resource Accounting Interface for Java With the spread of the Internet the computing model on server systems is undergoing several important changes. Recent research ideas concerning dynamic operating system extensibility are finding their way into the commercial domain, resulting in designs of extensible databases and Web servers. In addition, both ordinary users and service providers must deal with untrusted downloadable executable code of unknown origin and intentions.  Across the board, Java has emerged as the language of choice for Internet-oriented software. We argue that, in order to realize its full potential in applications dealing with untrusted code, Java needs a flexible resource accounting interface. The design and prototype implementation of such an interface --- JRes --- is presented in this paper. The interface allows to account for heap memory, CPU time, and network resources consumed by individual threads or groups of threads. JRes allows limits to be set on resources available to threads and it can invoke...",
    "neighbors": [
      465,
      800
    ],
    "mask": "Test"
  },
  {
    "node_id": 259,
    "label": 0,
    "text": "An Agent Based Framework for Mobile Users User mobility together with an easy access to distributed resources is one of the greatest challenge to be faced in the future years. At the same time, agent technology is seen as a very promising approach to deal with distributed computing and user mobility. In this paper an agent-based strategy for support of mobile users is presented. It is based on a mobile agent platform developed at the University of Catania, which has been enhanced in order to allow the user to access network services in a mobile environment. Main functionalities and architecure of the above platform are described.  1 Introduction  The quick expansion of wireless communication technologies and of portable computing devices, has made mobile computing more and more important. The user wishes to access the information he/she needs at any moment, independently of the place where he/she is. The ever increasing computing power available in notebooks, makes them a valid working tool for the user who needs to move from ...",
    "neighbors": [
      317
    ],
    "mask": "Train"
  },
  {
    "node_id": 260,
    "label": 4,
    "text": "System Lag Tests for Augmented and Virtual Environments We describe a simple technique for accurately calibrating the temporal lag in augmented and virtual environments within the Enhanced Virtual Hand Lab (EVHL), a  collection of hardware and software to support research on goal-directed human hand motion. Lag is the sum of various delays in the data pipeline associated with sensing, processing, and displaying information from the physical world to produce an augmented or virtual world. Our main calibration technique uses a modified phonograph turntable to provide easily tracked periodic motion, reminiscent of the pendulum-based calibration technique of Liang, Shaw and Green. Measurements show a three-frame (50 ms) lag for the EVHL. A second technique, which uses a  specialized analog sensor that is part of the EVHL, provides a \"closed loop\" calibration capable of sub-frame accuracy. Knowing the lag to sub-frame accuracy enables a predictive tracking scheme to compensate for the end-toend  lag in the data pipeline. We describe both techniques and the EVHL environment in which they are used.",
    "neighbors": [
      444
    ],
    "mask": "Train"
  },
  {
    "node_id": 261,
    "label": 0,
    "text": "MAPWEB: Cooperation between Planning Agents and Web Agents This paper presents MAPWeb (MultiAgent Planning in the Web), a multiagent system for cooperative work among dierent intelligent software agents whose main goal is to solve user planning problems using the information stored in the World Wide Web (Web). MAPWeb is made of a heterogeneous mixture of intelligent agents whose main characteristics are cooperation, reasoning, and knowledge sharing. The architecture of MAPWeb uses four types of agents:UserAgents that are the bridge between the users and the system; ControlAgents (Manager and Coach Agents) that are responsible to manage the rest of agents; PlannerAgents  that are able to solve planning problems; and nally WebAgents whose aim is to retrieve, represent and share information obtained from the Web. MAPWeb solves planning problems by means of cooperation between PlannerAgents and WebAgents. Instead of trying the PlannerAgent to solve the whole planning problem, the PlannerAgent focuses on a less restricted (and therefore easier to solve) problem (what we call an abstract problem) and cooperates with the WebAgents to validate and complete abstract solutions. In order for cooperation to take place, a common language and data structures have also been dened.  Categories and Subject Descriptors  H.3.5 [Online Information Services]: Data sharing, Webbased services; I.2 [Articial Intelligence]; I.2.6 [Learning]:  Knowledge acquisition; I.2.8 [Problem Solving]: Planning; I.2.11 [Distributed Articial Intelligence]: Intelligent agents, Multi-Agent Systems, Web agents  Keywords  Information System, Agent Architecture, Multi-Agent Systems, Web Agents, Intelligent Agents, Planning.  1.",
    "neighbors": [
      227
    ],
    "mask": "Test"
  },
  {
    "node_id": 262,
    "label": 5,
    "text": "Audio-Visual Speaker Detection using Dynamic Bayesian Networks The development of human-computer interfaces poses a challenging problem: actions and intentions of different users have to be inferred from sequences of noisy and ambiguous sensory data. Temporal fusion of multiple sensors can be efficiently formulated using dynamic Bayesian networks (DBNs). DBN framework allows the power of statistical inference and learning to be combined with contextual knowledge of the problem. We demonstrate the use of DBNs in tackling the problem of audio/visual speaker detection. \"Off-the-shelf\" visual and audio sensors (face, skin, texture, mouth motion, and silence detectors) are optimally fused along with contextual information in a DBN architecture that infers instances when an individual is speaking. Results obtained in the setup of an actual human-machine interaction system (Genie Casino Kiosk) demonstrate superiority of our approach over that of static, context-free fusion architecture.  1. Introduction  Advanced human--computer interfaces increasingly r...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 263,
    "label": 0,
    "text": "An approach to using degrees of belief in BDI agents : The past few years have seen a rise in the popularity of the use of mentalistic attitudes such as beliefs, desires and intentions to describe intelligent agents. Many of the models which formalise such attitudes do not admit degrees of belief, desire and intention. We see this as an understandable simplification, but as a simplification which means that the resulting systems cannot take account of much of the useful information which helps to guide human reasoning about the world. This paper starts to develop a more sophisticated system based upon an existing formal model of these mental attributes.  1 Introduction  In the past few years there has been a lot of attention given to building formal models of autonomous software agents; pieces of software which operate to some extent independently of human intervention and which therefore may be considered to have their own goals and the ability to determine how to achieve those goals. Many of these formal models are based on the use of ...",
    "neighbors": [
      557,
      852,
      953,
      964,
      1051
    ],
    "mask": "Train"
  },
  {
    "node_id": 264,
    "label": 0,
    "text": "Model Checking Agent UML Protocol Diagrams Agents in multiagent systems use protocols in order to exchange messages and to coordinate together. Since agents and objects are not exactly the same, designers do not use directly communication protocols used in distributed systems but a new type called interaction protocols encompassing agent features such as richer messages and the ability to cooperate and to coordinate. Obviously, designers consider formal description techniques used for communication protocols. New graphical modeling languages based on UML appeared several years ago. Agent UML is certainly the best known. Until now, no validation is given for Agent UML. The aim of this paper is to present how to model check Agent UML protocol diagrams.",
    "neighbors": [
      522,
      619,
      874
    ],
    "mask": "Train"
  },
  {
    "node_id": 265,
    "label": 0,
    "text": "Some Considerations about Embodied Agents   As computers are being more and more part of our world we feel the urgent need of proper user interface to interact with. The use of command lines typed on a keyboard are more and more obsolete, specially as computers are receiving so much attention from a large audience. The metaphor of face-to-face communication applied to human-computer interaction is finding a lot of attraction. Humans are used since they are born to communicate with others. Seeing faces, interpreting their expressions, understanding speech are all part of our development and growth. But face-to-face conversation is very complex as it involved a huge number of factors. We speak with our voice, but also with our hand, eye, face and body. Our gesture modifies, emphasizes, contradicts what we say by words. The production of speech and nonverbal behaviors work in parallel and not in antithesis. They seem to be two different forms (voice and body gestures) of the same process (speech). They add info",
    "neighbors": [
      1191,
      1267
    ],
    "mask": "Train"
  },
  {
    "node_id": 266,
    "label": 1,
    "text": "Rewriting Logic as a Metalogical Framework A metalogical framework is a logic with an associated methodology that is used to represent other logics and to reason about their metalogical properties. We propose that logical frameworks can be good metalogical frameworks when their logics support reflective reasoning and their theories always have initial models. We present a concrete realization of this idea in rewriting logic. Theories in rewriting logic always have initial models and this logic supports reflective reasoning. This implies that inductive reasoning is valid when proving properties about the initial models of theories in rewriting logic, and that we can use reflection to reason at the metalevel about these properties. In fact, we can uniformly reflect induction principles for proving metatheorems about rewriting logic theories and their parameterized extensions. We show that this reflective methodology provides an effective framework for di erent, non-trivial, kinds of formal metatheoretic reasoning; one can, for examp...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 267,
    "label": 0,
    "text": "How Emotions and Personality Effect the Utility of Alternative Decisions: A Terrorist Target Selection Case Study The role of emotion modeling in the development of computerized agents has long been unclear. This is  partially due to instability in the philosophical issues of the problem as psychologists struggle to build models for their  own purposes, and partially due to the often-wide gap between these theories and that which can be implemented by an  agent author. This paper describes an effort to use emotion models in part as a deep model of utility for use in decision  theoretic agents. This allows for the creation of simulated forces capable of balancing a great deal of competing goals,  and in doing so they behave, for better or for worse, in a more realistic manner.",
    "neighbors": [
      208
    ],
    "mask": "Train"
  },
  {
    "node_id": 268,
    "label": 3,
    "text": "qRTDB: Qos-Sensitive Real-Time Database Introduction  Recently the demand for real-time database services is exploding. The applications requiring such services include sensor data fusion, decision support, web information service, e-commerce, online trading, and dataintensive smart space applications. Furthermore, the information system is being globalized due to the fast growth of the Internet. Despite the importance and wide applicability, the performance and predictability of a database system    \\Gamma the core component of global information systems \\Gamma are relatively limited compared to the other real-time systems such as real-time operating systems. It can not be easily replicated due to the consistency problem. In addition, the database system has relatively low predictability compared to other real-time systems due to data dependence of the transaction execution, data and resource conflicts, dynamic paging and I/O, and transaction aborts and the resulting rollbacks and restarts [36]. Because of the limited perfo",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 269,
    "label": 0,
    "text": "LIME: Linda Meets Mobility Lime is a system designed to assist in the rapid development of dependable mobile applications over both wired and ad hoc networks. Mobile agents reside on mobile hosts and all communication takes place via transiently shared tuple spaces distributed across the mobile hosts. The decoupled style of computing characterizing the Linda model is extended to the mobile environment. At the application level, both agents and hosts perceive movement as a sudden change of context. The set of tuples accessible by a particular agent residing on a given host is altered transparently in response to changes in the connectivity pattern among the mobile hosts. In this paper we present the key design concepts behind the Lime system. 1 INTRODUCTION Today's users demand ubiquitous network access independent of their physical location. This style of computation, often referred to as mobile computing, is enabled by rapid advances in the wireless communication technology. The networking scenarios enabled ...",
    "neighbors": [
      47,
      246,
      318,
      757,
      880,
      1065
    ],
    "mask": "Train"
  },
  {
    "node_id": 270,
    "label": 2,
    "text": "Evolution of the Walden's Paths Authoring Tools : Changing user skills, available infrastructure, and work practices have caused many  differences in the authoring support provided by the Walden's Paths project since its  conception. In this paper we trace these changes and the transition from the earlier authoring  tools that supported an integrated authoring process, to the more recent tools designed to work  with the Web applications that teachers have become accustomed to.  1. Introduction  Hypertext has come a long way from being found only in research systems to being a part of our everyday lives in the form of the World-Wide Web (WWW or the Web). We use the Web for browsing academic information, for furthering business interests, for entertainment and a variety of other purposes. There is an immense amount of information on the Web that can be used for a variety of reasons. Web-based information can be harnessed to supplement classroom teaching for K-12 students. K-12 teachers can use Web-based information in the curriculum t...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 271,
    "label": 2,
    "text": "Finding the Most Similar Documents across Multiple Text Databases In this paper, we present a methodology for finding the n most similar documents across multiple text databases for any given query and for any positive integer n. This methodology consists of two steps. First, databases are ranked in a certain order. Next, documents are retrieved from the databases according to the order and in a particular way. If the databases containing the n most similar documents for a given query can be ranked ahead of other databases, the methodology will guarantee the retrieval of the n most similar documents for the query. A statistical method is provided to identify databases, each of which is estimated to contain at least one of the n most similar documents. Then, a number of strategies is presented to retrieve documents from the identified databases. Experimental results are given to illustrate the relative performance of different strategies. 1 Introduction The Internet has become a vast information source in recent years and can be considered as the w...",
    "neighbors": [
      224,
      433,
      435,
      477,
      502,
      510,
      526,
      696,
      792,
      897,
      931,
      1124,
      1165,
      1253
    ],
    "mask": "Train"
  },
  {
    "node_id": 272,
    "label": 5,
    "text": "Learning to Perceive the World as Articulated: An Approach for Hierarchical Learning in Sensory-Motor Systems This paper describes how agents can learn an internal model of the world  structurally by focusing on the problem of behavior-based articulation. We develop  an on-line learning scheme -- the so-called mixture of recurrent neural  net (RNN) experts -- in which a set of RNN modules becomes self-organized  as experts on multiple levels in order to account for the different categories  of sensory-motor flow which the robot experiences. Autonomous switching of  activated modules in the lower level actually represents the articulation of the  sensory-motor flow. In the meanwhile, a set of RNNs in the higher level competes  to learn the sequences of module switching in the lower level, by which articulation  at a further more abstract level can be achieved. The proposed scheme  was examined through simulation experiments involving the navigation learning  problem. Our dynamical systems analysis clarified the mechanism of the articulation;  the possible correspondence between the articulation...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 273,
    "label": 0,
    "text": "Integrating Mobile Agents into Off-the-Shelf Web Servers: The M&M Approach The mobile agent paradigm provides a new approach for developing distributed systems. During the last two years, we have been working on a project that tries to overcome some of the limitations found in terms of programmability and usability of the mobile agent paradigm in real applications. In the M&M framework there are no agent platforms. Instead applications become agent-enabled by using simple JavaBeans components. In our approach the agents arrive and departure directly from the applications, interacting with them from the inside.",
    "neighbors": [
      873,
      909
    ],
    "mask": "Test"
  },
  {
    "node_id": 274,
    "label": 2,
    "text": "Flexible Queries to Semi-structured Datasources: the WG-log Approach A line of research is presented aimed at specifying both logical and navigational aspects of semi-structured data sources such as Web sites through the unifying notion of schema. Gracefully supporting schemata that are huge or subject to change, the WG-Log language allows for a uniform representation of queries and views, the latter expressing customized access structures to site information. A survey of related work and some directions for future research involving fuzzy query techniques are also outlined. 1 Introduction and Motivations Modern network-oriented information systems often have to deal with data that are semi-structured, i.e. lack the strict, regular, and complete structure required by traditional database management systems (see [Abi97] and [Suc97] for a survey on semi-structured data and related research). Information is semi-structured also when the structure of data varies w.r.t. time, rather than w.r.t. space: even if data is fairly well structured, such struc...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 275,
    "label": 3,
    "text": "Flow Analysis for Verifying Specifications of Concurrent and Distributed Software This paper presents FLAVERS, a finite state verification approach that analyzes whether concurrent or sequential programs satisfy user-defined correctness properties. In contrast to other finite-state verification techniques, FLAVERS is based on algorithms with low-order polynomial bounds on the running time. FLAVERS achieves this efficiency at the cost of precision. Users, however, can improve the precision of the results by selectively and judiciously incorporating additional semantic information into the analysis problem. The FLAVERS analysis approach has been implemented for programs written in Ada. We report on an empirical study of the performance of applying the FLAVERS/Ada tool set to a collection of multi-tasking Ada programs. This study indicates that sufficient precision for proving program properties can be achieved and that the cost for such analysis grows as a low-order polynomial in the size of the program. 1 Introduction  The application of distributed and concurrent pr...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 276,
    "label": 4,
    "text": "Audio Driven Facial Animation For Audio-Visual Reality In this paper, we demonstrate a morphing based automated audio driven facial animation system. Based on an incoming audio stream, a face image is animated with full lip synchronization and expression. An animation sequence using optical flow between visemes is constructed, given an incoming audio stream and still pictures of a face speaking different visemes. Rules are formulated based on coarticulation and the duration of a viseme to control the continuity in terms of shape and extent of lip opening. In addition to this new viseme-expression combinations are synthesized to be able to generate animations with new facial expressions. Finally various applications of this system are discussed in the context of creating audio-visual reality.  1.",
    "neighbors": [
      898
    ],
    "mask": "Train"
  },
  {
    "node_id": 277,
    "label": 0,
    "text": "Meeting Plan Recognition Requirements for Real-Time Air-Mission Simulations In this paper, the potential synergy between instancebased pattern recognition and means-end (possible world) reasoning is explored, for supporting plan recognition in multi-aeroplane air-mission simulations. A combination of graph matching, induction, probabilistic principles and dynamic programming are applied to traces of aeroplane behaviour during flight manoeuvres. These satisfy the real-time constraints of the simulation. This enables the agents to recognise what other agents are doing and to abstract about their activity, at the instrumentation level. A means-end-reasoning model is then used to deliberate about and invoke standard operating procedures, based on recognised activity. The reasoning model constrains the recognition process by framing queries according to what a pilot would expect during the execution of the current plan(s). Results from experiments involving the dMARS procedural reasoning system and the CLARET pattern matching and induction system are described for ...",
    "neighbors": [
      223,
      405,
      964
    ],
    "mask": "Test"
  },
  {
    "node_id": 278,
    "label": 3,
    "text": "Alternative Correctness Criteria for Multiversion Concurrency Control and a Locking Protocol via Freezing Concurrency control protocols based on multiversions have been used in some commercial transaction processing systems in order to provide the serializable executions of transactions. In the existing protocols, transactions are allowed to read only the most recent version of each data item in order to ensure the correct execution of transactions. However, this feature is not desirable in some advanced database systems which have more requirements such as timing or security constraints besides serializability. In this paper, we propose a new correctness criteria, called F-serializability, for multiversion concurrency control protocols. It is the extended definition of `1-serial' and relaxes the condition so that a protocol provides not only the most recent version but also the previous one to transactions, if necessary. We prove that whenever a multiversion schedule is F-serializable, the schedule is also one-copy serializable. This is the first contribution of our paper. Next, we propos...",
    "neighbors": [
      470
    ],
    "mask": "Train"
  },
  {
    "node_id": 279,
    "label": 1,
    "text": "Learning Hidden Markov Model Structure for Information Extraction Statistical machine learning techniques, while well proven in fields such as speech recognition, are just beginning to be applied to the information extraction domain. We explore the use of hidden Markov models for information extraction tasks, specifically focusing on how to learn model structure from data and how to make the best use of labeled and unlabeled data. We show that a manually-constructed model that contains multiple states per extraction field outperforms a model with one state per field, and discuss strategies for learning the model structure automatically from data. We also demonstrate that the use of distantly-labeled data to set model parameters provides a significant improvement in extraction accuracy. Our models are applied to the task of extracting important fields from the headers of computer science research papers, and achieve an extraction accuracy of 92.9%.  Introduction  Hidden Markov modeling is a powerful statistical machine learning technique that is just ...",
    "neighbors": [
      496,
      570,
      643,
      875,
      956,
      1122,
      1232
    ],
    "mask": "Validation"
  },
  {
    "node_id": 280,
    "label": 2,
    "text": "Data Mining on Symbolic Knowledge Extracted from the Web Information extractors and classifiers operating on unrestricted, unstructured texts are an errorful source of large amounts of potentially useful information, especially when combined with a crawler which automatically augments the knowledge base from the world-wide web. At the same time, there is much structured information on the World Wide Web. Wrapping the web-sites which provide this kind of information provide us with a second source of information; possibly less up-to-date, but reliable as facts. We give a case study of combining information from these two kinds of sources in the context of learning facts about companies. We provide results of association rules, propositional and relational learning, which demonstrate that data-mining can help us improve our extractors, and that using information from two kinds of sources improves the reliability of data-mined rules. 1. INTRODUCTION The World Wide Web has become a significant source of information. Most of this computer-retri...",
    "neighbors": [
      133,
      347
    ],
    "mask": "Train"
  },
  {
    "node_id": 281,
    "label": 2,
    "text": "Evaluating Topic-Driven Web Crawlers Due to limited bandwidth, storage, and computational resources, and to the dynamic nature of the Web, search engines cannot index every Web page, and even the covered portion of the Web cannot be monitored continuously for changes. Therefore it is essential to develop effective crawling strategies to prioritize the pages to be indexed. The issue is even more important for topic-specific search engines, where crawlers must make additional decisions based on the relevance of visited pages. However, it is difficult to evaluate alternative crawling strategies because relevant sets are unknown and the search space is changing. We propose three different methods to evaluate crawling strategies. We apply the proposed metrics to compare three topic-driven crawling algorithms based on similarity ranking, link analysis, and adaptive agents.",
    "neighbors": [
      1,
      53,
      255,
      382,
      649,
      774,
      1005,
      1059,
      1264
    ],
    "mask": "Train"
  },
  {
    "node_id": 282,
    "label": 3,
    "text": "Scalable Consistency Protocols for Distributed Services Abstract\u00d0A common way to address scalability requirements of distributed services is to employ server replication and client caching of objects that encapsulate the service state. The performance of such a system could depend very much on the protocol implemented by the system to maintain consistency among object copies. We explore scalable consistency protocols that never require synchronization and communication between all nodes that have copies of related objects. We achieve this by developing a novel approach called local consistency (LC). LC based protocols can provide increased flexibility and efficiency by allowing nodes control over how and when they become aware of updates to cached objects. We develop two protocols for implementing strong consistency using this approach and demonstrate that they scale better than a traditional invalidation based consistency protocol along the system load and geographic distribution dimensions of scale. Index Terms\u00d0Scalable services, distributed objects, replication, caching, consistency protocols. 1",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 283,
    "label": 3,
    "text": "Query Optimization for Selections using Bitmaps Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous  and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.  1 Introduction  Bitmap indexing has become a promising technique for query processing in DWs. Variations of bitmap indexes include bit-sliced indexes [14, 3], encoded bitmap indexes (EBI) [18], bitmapped join indexes [13], range-based bitmap indexes [20], and others[16]. For query operations, such as selections, aggregates, and joins, query evaluation algorithms using bitmaps have been proposed in ...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 284,
    "label": 0,
    "text": "Socialware: Multiagent Systems for Supporting Network Communities ing with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org.  Community B Community A Community C  Community agent A1 Community agent B1 Community agent B2 Personal unit 1 Personal unit 2 Personal unit 5 Personal unit 6 Personal unit 3 Personal unit 7 Personal unit 4 User 5 to join User 4 to leave Figure 1: A general architecture of socialware as a multiagent system Socialware as Multiagent Systems   There are several characteristics specific to network communities, which make a multiagent architecture attractive to use. The first characteristic is that the participants of a network community are widely distributed and the number of potential participants is large. Hence, no solid, centralized, or monolithic system would be adequate: A distributed system would be required, in which perso...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 285,
    "label": 0,
    "text": "Updating Mental States from Communication . In order to perform effective communication agents must be able to foresee  the effects of their utterances on the addressee's mental state. In this paper we investigate  on the update of the mental state of an hearer agent as a consequence of the utterance  performed by a speaker agent. Given an agent communication language with a STRIPSlike  semantics, we propose a set of criteria that allow to bind the speaker's mental state  to its uttering of a certain sentence. On the basis of these criteria, we give an abductive  procedure that the hearer can adopt to partially recognize the speaker's mental state that  led to a specific utterance. This procedure can be adopted by the hearer to update its own  mental state and its image of the speaker's mental state.  1 Introduction  In multi-agent systems, communication is necessary for the agents to cooperate and coordinate their activities or simply to avoid interfering with one another. If agents are not designed with embedded pre-compiled...",
    "neighbors": [
      808,
      964
    ],
    "mask": "Train"
  },
  {
    "node_id": 286,
    "label": 0,
    "text": "A Conceptual Framework for Agent Definition and Development The use of agents of many different kinds in a variety of fields of computer science and  artificial intelligence is increasing rapidly and is due, in part, to their wide applicability. The  richness of the agent metaphor that leads to many different uses of the term is, however, both a  strength and a weakness: its strength lies in the fact that it can be applied in very many different  ways in many situations for different purposes; the weakness is that the term agent is now used so  frequently that there is no commonly accepted notion of what it is that constitutes an agent. This  paper addresses this issue by applying formal methods to provide a defining framework for agent  systems. The Z specification language is used to provide an accessible and unified formal account  of agent systems, allowing us to escape from the terminological chaos that surrounds agents. In  particular, the framework precisely and unambiguously provides meanings for common concepts  and terms, enables alternative models of particular classes of system to be described within it, and  provides a foundation for subsequent development of increasingly more refined concepts.",
    "neighbors": [
      957
    ],
    "mask": "Train"
  },
  {
    "node_id": 287,
    "label": 4,
    "text": "Principles for a Usability-Oriented Pattern Language High-level usability principles (e.g. \"Familiarity\") are difficult to apply to specific projects, and style guides providing more detailed instructions are often misinterpreted and inaccessible. An approach to usability based on design patterns enables designers to learn how certain recurring problems can be solved according to high-level principles. This paper summarises a review of the desirable properties advocated by five popular style guides, and discusses how this list has been modified to provide an underlying philosophy which is appropriate for a usability-oriented pattern language. A sample pattern which exemplifies this philosophy, involving iteration through selectable objects, is described. KEYWORDS Usability engineering, Design techniques, Style guides.  1. Introduction  There has been considerable discussion about how to reconcile the gaps between software engineering (SE) and human-computer interaction (HCI). One of the primary ways to smoothly integrate the disciplines ...",
    "neighbors": [
      240,
      1273
    ],
    "mask": "Test"
  },
  {
    "node_id": 288,
    "label": 0,
    "text": "Controlling Speculative Computation in Multi-Agent Environments In this paper, we propose a multi-agent system which performs speculative computation under incomplete communication environments. In a master-slave style multi-agent system with speculative computation, a master agent asks queries to slave agents in problem solving, and proceeds computation with default answers when answers from slave agents are delayed. We rst provide a semantics for speculative computation using default logic. Then, in the proposed system, we use the consequence- nding procedure SOL written in the Java language to perform data-driven deductive reasoning. The use of a consequence- nding procedure is convenient for updating agents' beliefs according to situation changes in the world. In our system, slave agents can change their answers frequently, yet a master agent can avoid duplicate computation. As long as actual answers from slave agents do not con- ict with any previously encountered situation, the obtained conclusions are never recomputed. We applied the proposed system to the meeting-room reservation problem to see the usefulness of the framework. 1",
    "neighbors": [
      532
    ],
    "mask": "Train"
  },
  {
    "node_id": 289,
    "label": 3,
    "text": "Binary Decision Diagram Representations Of Firewall And Router Access Lists Network firewalls and routers can use a rule database to decide which packets will be allowed from one network onto another. By filtering packets the firewalls and routers can improve security and performance -- by excluding packets which may pose a security risk to a network or are not relevant to it. However, as the size of the rule list increases, it becomes difficult to maintain and validate the rules, and the cost of rule lookup may add significantly to latency. Ordered binary decision diagrams (BDDs) -- a compact method of representing and manipulating boolean expressions -- are a potential method of representing the rules. This paper explores how BDDs can be used to develop methods that help analysis of rules to validate them and changes to them, to improve performance, and facilitate hardware support.  1 Introduction  The growth of network and internet communication creates several challenges for network design. Two important issues are security and performance. As the volume o...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 290,
    "label": 4,
    "text": "Collision Avoidance and Resolution Multiple Access for Multichannel Wireless Networks The CARMA-NTG protocol is presented and analyzed. CARMA-NTG dynamically divides the channel into cycles of variable length; each cycle consists of a contention period and a group-transmission period. During the contention period, a station with one or more packets to send competes for the right to be added to the group of stations allowed to transmit data without collisions; this is done using a collision resolution splitting algorithm based on a request-to-send/clear-to-send (RTS/CTS) message exchange with non-persistent carrier sensing. CARMA-NTG ensures that one station is added to the group transmission period if one or more stations send requests to be added in the previous contention period. The group-transmission period is a variable-length train of packets, which are transmitted by stations that have been added to the group by successfully completing an RTS/CTS message exchange in previous contention periods. As long as a station maintains its position in the group, it is able to transmit data packets without collision. An upper bound is derived for the average costs of obtaining the first success in the splitting algorithm. This bound is then applied to the computation of the average channel utilization in a fully connected network with a large number of stations. These results indicate that collision resolution is a powerful mechanism in combination with floor acquisition and group allocation multiple access. 1",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 291,
    "label": 2,
    "text": "Centroid-Based Document Classification: Analysis & Experimental Results . In this paper we present a simple linear-time centroid-based document  classification algorithm, that despite its simplicity and robust performance,  has not been extensively studied and analyzed. Our experiments show that this  centroid-based classifier consistently and substantially outperforms other algorithms  such as Naive Bayesian, k-nearest-neighbors, and C4.5, on a wide range  of datasets. Our analysis shows that the similarity measure used by the centroidbased  scheme allows it to classify a new document based on how closely its behavior  matches the behavior of the documents belonging to different classes. This  matching allows it to dynamically adjust for classes with different densities and  accounts for dependencies between the terms in the different classes.  1 Introduction  We have seen a tremendous growth in the volume of online text documents available on the Internet, digital libraries, news sources, and company-wide intranets. It has been forecasted that these docu...",
    "neighbors": [
      228,
      235,
      545,
      726,
      1005,
      1049
    ],
    "mask": "Test"
  },
  {
    "node_id": 292,
    "label": 1,
    "text": "Learning Languages by Collecting Cases and Tuning Parameters . We investigate the problem of case-based learning of formal languages. Case-based reasoning and learning is a currently booming area of artificial intelligence. The formal framework for case-based learning of languages has recently been developed by [JL93] in an inductive inference manner. In this paper, we first show that any indexed class of recursive languages in which finiteness is decidable is case-based representable, but many classes of languages including the class of all regular languages are not case-based learnable with a fixed universal similarity measure, even if both positive and negative examples are presented. Next we consider a framework of case-based learning where the learning algorithm is allowed to learn similarity measures, too. To avoid trivial encoding tricks, we carefully examine to what extent the similarity measure is going to be learned. Then by allowing only to learn a few parameters in the similarity measures, we show that any indexed class of recursive ...",
    "neighbors": [
      1259
    ],
    "mask": "Train"
  },
  {
    "node_id": 293,
    "label": 0,
    "text": "Systems Directions for Pervasive Computing Pervasive computing, with its focus on users and their tasks rather than on computing devices and technology, provides an attractive vision for the future of computing. But, while hardware and networking infrastructure to realize this vision are becoming a reality, precious few applications run in this infrastructure. We believe that this lack of applications stems largely from the fact that it is currently too hard to design, build, and deploy applications in the pervasive computing space. In this paper, we argue that existing approaches to distributed computing are flawed along three axes when applied to pervasive computing; we sketch out alternatives that are better suited for this space. First, application data and functionality need to be kept separate, so that they can evolve gracefully in a global computing infrastructure. Second, applications need to be able to acquire any resource they need at any time, so that they can continuously provide their services in a highly dynamic environment. Third, pervasive computing requires a common system platform, allowing applications to be run across the range of devices and to be automatically distributed and installed. 1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 294,
    "label": 3,
    "text": "SI-Designer: an Integration Framework for E-Commerce Electronic commerce lets people purchase goods  and exchange information on business transactions  on-line. Therefore one of the main challenges for  the designers of the e-commerce infrastructures is  the information sharing, retrieving data located in  different sources thus obtaining an integrated view  to overcome any contradiction or redundancy. Virtual  Catalogs synthesize this approach as they are  conceived as instruments to dynamically retrieve  information from multiple catalogs and present  product data in a unified manner, without directly  storing product data from catalogs.  In this paper we propose SI-Designer, a support  tool for the integration of data from structured and  semi-structured data sources, developed within the  MOMIS (Mediator environment for Multiple Information  Sources) project.  1",
    "neighbors": [
      28,
      766
    ],
    "mask": "Test"
  },
  {
    "node_id": 295,
    "label": 5,
    "text": "Coastal Navigation - Mobile Robot Navigation with Uncertainty in Dynamic Environments Ships often use the coasts of continents for navigation in the absence of better tools such as GPS, since being close to land allows sailors to determine with high accuracy where they are. Similarly for mobile robots, in many environments global and accurate localization is not always feasible. Environments can lack features, and dynamic obstacles such as people can confuse and block sensors. In this paper, we demonstrate a technique for generating trajectories that take into account both the information content of the environment, and the density of the people in the environment. These trajectories reduce the average positional certainty as the robot moves, reducing the likelihood the robot will become lost at any point. Our method was successfully implemented and used by the mobile robot Minerva, a museum tourguide robot, for a 2 week period in the Smithsonian National Museum of American History. 1 Introduction  One essential component of any operational mobile robot system is the ab...",
    "neighbors": [
      369
    ],
    "mask": "Train"
  },
  {
    "node_id": 296,
    "label": 0,
    "text": "The KRAFT Architecture for Knowledge Fusion and Transformation This paper describes the KRAFT architecture which supports the fusion of knowledge  from multiple, distributed, heterogeneous sources. The architecture uses  constraints as a common knowledge interchange format, expressed against a common  ontology. Knowledge held in local sources can be tranformed into the common  constraint language, and fused with knowledge from other sources. The  fused knowledge is then used to solve some problem or deliver some information  to a user. Problem-solving in KRAFT typically exploits pre-existing constraint  solvers. KRAFT uses an open and flexible agent architecture in which knowledge  sources, knowledge fusing entities, and users are all represented by independent  KRAFT agents, communicating using a messaging protocol. Facilitator agents  perform matchmaking and brokerage services between the various kinds of agent.  KRAFT is being applied to an example application in the domain of network data  services design.  1 Introduction and Motivation  Most ...",
    "neighbors": [
      663,
      1051,
      1190
    ],
    "mask": "Train"
  },
  {
    "node_id": 297,
    "label": 0,
    "text": "Planning and Motion Control in Lifelike Gesture: A Refined Approach In this paper an operational model for the automatic generation of lifelike gestures of an anthropomorphic virtual agent is described. The biologically motivated approach to controlling the movements of a highly articulated figure provides a transformation of spatiotemporal gesture specifications into an analog representation of the movement from which the animations are directly rendered. To this end, knowledge-based computer animation techniques are combined with appropriate methods for trajectory formation and articulated figure animation.  1. Introduction  The inclusion of nonverbal modalities into the communicative behaviors of virtual agents has moved into focus of human-computer interface researchers. Humans are more likely to consider computer-generated figures lifelike when appropriate nonverbal behaviors are displayed in addition to speech. This enables the evocation of social communicative attributions to the artificial agent, which are supposed to be advantageous for a natu...",
    "neighbors": [
      921
    ],
    "mask": "Train"
  },
  {
    "node_id": 298,
    "label": 2,
    "text": "Integrating Automatic Genre Analysis into Digital Libraries With the number and types of documents in digital library systems increasing, tools for automatically organizing and presenting the content have to be found. While many approaches focus on topic-based organization and structuring, hardly any system incorporates automatic structural analysis and representation. Yet, genre information (unconsciously) forms one of the most distinguishing features in conventional libraries and in information searches. In this paper we present an approach to automatically analyze the structure of documents and to integrate this information into an automatically created content-based organization. In the resulting visualization, documents on similar topics, yet representing dierent genres, are depicted as books in diering colors. This representation supports users intuitively in locating relevant information presented in a relevant form.  Keywords  Genre Analysis, Self-Organizing Map (SOM), SOMLib, Document Clustering, Visualization, Metaphor Graphics  1.",
    "neighbors": [
      41,
      156,
      903,
      1010
    ],
    "mask": "Validation"
  },
  {
    "node_id": 299,
    "label": 0,
    "text": "Localisation using Automatically Selected Landmarks from Panoramic Images The use of visual landmarks for robot localisation is a promising field. It is apparent that the success of localisation by visual landmarks depends on the landmarks chosen. Good landmarks are those which remain reliable over time and through changes in position and orientation. This paper describes a system which learns places by automatically selecting landmarks from panoramic images and uses them for localisation tasks. An adaption of the biologically inspired Turn Back and Look behaviour is used to evaluate potential landmarks. Normalised correlation is used to overcome the a#ects of changes in illumination in the environment. Results from real robot experiments are reported, showing successful localisation from up to one meter away from the learnt position. 1 Introduction Visual localisation is one of the key problems in making successful autonomous robots. Vision as a sensor is the richest source of information about a mobile agent's environment and as such con...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 300,
    "label": 5,
    "text": "Adaptation Techniques for Intrusion Detection and Intrusion Response Systems This paper examines techniques for providing adaptation in intrusion detection and intrusion response systems. As attacks on computer systems are becoming increasingly numerous and sophisticated, there is a growing need for intrusion detection and response systems to dynamically adapt to better detect and respond to attacks. The Adaptive Hierarchical Agentbased Intrusion Detection System (AHA! IDS) provides detection adaptation by adjusting the amount of system resources devoted to the task of detecting intrusive activities. This is accomplished by dynamically invoking new combinations of lower level detection agents in response to changing circumstances and by adjusting the confidence associated with these lower-level agents. The Adaptive Agentbased Intrusion Response System (AAIRS) provides response adaptation by weighting those responses that have been successful in the past over those techniques that have not been as successful. As a result, the more successful responses are used...",
    "neighbors": [
      547
    ],
    "mask": "Train"
  },
  {
    "node_id": 301,
    "label": 1,
    "text": "Evolutionary Approaches to Off-Line Routing in Backbone Communications Networks Off-line routing in backbone communications networks is an important combinatorial optimisation problem. It has three main uses: first, off-line routing provides reference benchmark results for dynamic (on-line) routing strategies. Second, and more interestingly, off-line routing is becoming more and more investigated and employed in its own right as a way of quickly finding significantly improved routings for live networks which can then be imposed on the network to offer a net improvement in quality of service. Third, it can be used in networks where bandwidth may be booked in advance. In this paper we introduce and investigate a number of heuristic techniques applicable to the routing problem for use in stochastic, iterative search. Results are presented which indicate that these heuristics significantly improve the search for solutions, particularly when on-line performance is considered. We also investigate how computation time can be further reduced by the use of delta-evaluation...",
    "neighbors": [
      598
    ],
    "mask": "Train"
  },
  {
    "node_id": 302,
    "label": 3,
    "text": "Optimizing Object Queries Using an Effective Calculus Object-oriented databases (OODBs) provide powerful data abstractions and modeling facilities but they generally lack a suitable framework for query processing and optimization. One of the key factors for OODB systems to successfully compete with relational systems as well as to meet the performance requirements of many non-traditional applications is the development of an effective query optimizer. We propose an effective framework with a solid theoretical basis for optimizing OODB query languages. Our calculus, called the monoid comprehension calculus, captures most features of ODMG OQL and is a good basis for expressing various optimization algorithms concisely. This paper concentrates on query unnesting, an optimization that, even though improves performance considerably, is not treated properly (if at all) by most OODB systems. Our framework generalizes many unnesting techniques proposed recently in the literature and is capable of removing any form of query nesting using a very si...",
    "neighbors": [
      397,
      493,
      1047
    ],
    "mask": "Train"
  },
  {
    "node_id": 303,
    "label": 0,
    "text": "A Unified View of Plans as Recipes Plans as recipes or abstract structures, as well as plans as mental attitudes that guide an agent in its planning process has been enthusiastically embraced by both philosophers and AI practitioners. They play a central role in a class of rational agents, called Belief-Desire-Intention (BDI) agents. This dual view of plans can not only be used for efficient planning, but can also be used for recognizing the plans of other agents, coordinating one's actions and achieving joint intentions with other members of a larger collective or team, and finally recognizing the collective plans and intentions of other teams. In this paper, we start with a simple notion of execution plans and discuss its operational semantics. We progressively extend this notion of plans to recognition plans, joint execution plans, and joint recognition plans. The primary contribution of this paper is in providing an integrated view of plans that facilitate individual an collective planning and recognition.   1 Int...",
    "neighbors": [
      223,
      816,
      839
    ],
    "mask": "Train"
  },
  {
    "node_id": 304,
    "label": 3,
    "text": "Making LDAP Active with the LTAP Gateway: Case Study in Providing Telecom Integration and Enhanced Services LDAP (Lightweight Directory Access Protocol) directories are being rapidly deployed on the Web. They are currently used to store data like white pages information, user profiles, and network device descriptions. These directories offer a number of advantages over current database technology in that they provide better support for heterogeneity and scalability. However, they lack some basic database functionality (e.g., triggers, transactions) that is crucial for Directory Enabled Networking (DEN) tasks like provisioning network services, allocating resources, reporting, managing end-to-end security, and offering mobile users customized features that follow them. In order to address these limitations while keeping the simplicity and performance features of LDAP directories, unbundled and portable solutions are needed. In this paper we discuss LDAP limitations we faced while building an LDAP meta-directory that integrates data from legacy telecom systems, and how LTAP (Lightweight Trigger Access Process), a portable gateway that adds active functionality to LDAP directories, overcomes these limitations.",
    "neighbors": [
      711,
      752,
      1201
    ],
    "mask": "Test"
  },
  {
    "node_id": 305,
    "label": 3,
    "text": "Textural Features for Image Database Retrieval This paper presents two feature extraction methods and two decision methods to retrieve images having some section in them that is like the user input image. The features used are variances of gray level co-occurrences and line-angle-ratio statistics constituted by a 2-D histogram of angles between two intersecting lines and ratio of mean gray levels inside and outside the regions spanned by those angles. The decision method involves associating with any pair of images either the class \u201crelevant\u201d or \u201cirrelevant\u201d. A Gaussian classifier and nearest neighbor classifier are used. A protocol that translates a frame throughout every image to automatically define for any pair of images whether they are in the relevance class or the irrelevance class is discussed. Experiments on a database of 300 gray scale images with 9,600 groundtruth image pairs showed that the classifier assigned 80 % of the image pairs we were sure were relevant, to the relevance class correctly. The actual retrieval accuracy is greater than this lower bound of 80%.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 306,
    "label": 3,
    "text": "Indexing Semistructured Data This paper describes techniques for building and exploiting indexes on semistructured data: data that may not have a fixed schema and that may be irregular or incomplete. We first present a general framework for indexing values in the presence of automatic type coercion. Then based on  Lore, a DBMS for semistructured data, we introduce four types of indexes and illustrate how they are used during query processing. Our techniques and indexing structures are fully implemented and integrated into the Lore prototype. 1 Introduction  We call data that is irregular or that exhibits type and structural heterogeneity semistructured,  since it may not conform to a rigid, predefined schema. Such data arises frequently on the Web, or when integrating information from heterogeneous sources. In general, semistructured data can be neither stored nor queried in relational or object-oriented database management systems easily and efficiently. We are developing Lore  1  , a database management system d...",
    "neighbors": [
      364,
      488,
      634
    ],
    "mask": "Validation"
  },
  {
    "node_id": 307,
    "label": 4,
    "text": "Visual Contextual Awareness in Wearable Computing Small, body-mounted video cameras enable a different style of wearable computing interface. As processing power increases, a wearable computer can spend more time observing its user to provide serendipitous information, manage interruptions and tasks, and predict future needs without being directly commanded by the user. This paper introduces an assistant for playing the real-space game Patrol. This assistant tracks the wearer's location and current task through computer vision techniques and without off-body infrastructure. In addition, this paper continues augmented reality research, started in 1995, for binding virtual data to physical locations. 1. Introduction  For most computer systems, even virtual reality systems, sensing techniques are a means of getting input directly from the user. However, wearable computers offer a unique opportunity to re-direct sensing technology towards recovering more general user context. Wearable computers have the potential to \"see\" as the user sees...",
    "neighbors": [
      334,
      497,
      531,
      665,
      680,
      728,
      738,
      910,
      1006
    ],
    "mask": "Validation"
  },
  {
    "node_id": 308,
    "label": 0,
    "text": "Knowledge Modeling - State of the Art A major characteristic of developments in the broad field of Artificial Intelligence (AI) during the 1990s has been an increasing integration of AI with other disciplines. A number of other computer science fields and technologies have been used in developing intelligent systems, starting from traditional information systems and databases, to modern distributed systems and the Internet. This paper surveys knowledge modeling techniques that have received most attention in recent years among developers of intelligent systems, AI practitioners and researchers. The techniques are described from two perspectives, theoretical and practical. Hence the first part of the paper presents major theoretical and architectural concepts, design approaches, and research issues. The second part discusses several practical systems, applications, and ongoing projects that use and implement the techniques described in the first part. Finally, the paper briefly covers some of the most recent results in the fields of intelligent manufacturing systems, intelligent tutoring systems, and ontologies.  2 1.",
    "neighbors": [
      119,
      1067
    ],
    "mask": "Train"
  },
  {
    "node_id": 309,
    "label": 3,
    "text": "Run-time Detection in Parallel and Distributed Systems: An Application to Safety-Critical Applications As systems are becoming more complex, there is increasing interest in their runtime analysis, understanding their dynamic behavior and possibly controling it as well. This paper describes complex distributed and parallel applications that use run-time analyses to attain scalability improvements with respect to the amount and complexity of the data transmitted, transformed, and shared among different application components. Such improvements are derived from using database techniques when manipulating data streams. Namely, by imposing a relational model on a data stream, filters and constraints on the stream may be expressed in the form of database queries evaluated against the data events comprising the stream. Streams may then be filtered using runtime optimization techniques derived from query optimization methods. This paper also presents a tool, termed Cnet, which offers (1) means for the dynamic creation of queries and their application to distributed data streams, (2) permits the...",
    "neighbors": [
      18
    ],
    "mask": "Train"
  },
  {
    "node_id": 310,
    "label": 3,
    "text": "Specialising The Other Way Around In this paper, we present a program transformation based on bottom up evaluation of logic programs. We explain that using this technique, programs can be \"specialised\" w.r.t. a set of unit clauses instead of a query. Moreover, examples indicate that excellent specialisation can be obtained when this bottom up transformation is combined with a more traditional top down approach, resulting in conceptually cleaner techniques requiring a less complicated control than one overall approach.  1",
    "neighbors": [
      1250
    ],
    "mask": "Train"
  },
  {
    "node_id": 311,
    "label": 5,
    "text": "A Description Logic for Vague Knowledge This work introduces the concept language ALCFM which is an extension of ALC to many-valued logics. ALCFM allows to express vague concepts, e.g. more or less enlarged or very small. To realize this extension to many-valued logics, the classical notions of satisfiability and subsumption had to be modified appropriately. For example, ALCFM -concepts are no longer either satisfiable or unsatisfiable, but they are satisfiable to a certain degree. The main contribution of this paper is a sound and complete method for computing the degree of subsumption between two  ALCFM -concepts. 1 Introduction  This work takes its motivation from the occurrence of vague concept descriptions in different application areas. Often, application--inherent information is characterized by a very high degree of vagueness. Appropriate information systems must be able to process this kind of data. So far, there are no systems that really solve the corresponding problems due to the lack of powerful basic methods. A...",
    "neighbors": [
      173
    ],
    "mask": "Validation"
  },
  {
    "node_id": 312,
    "label": 0,
    "text": "Towards Robust Teams with Many Agents Agents in deployed multi-agent systems monitor other agents to coordinate and collaborate robustly. However, as the number of agents monitored is scaled up, two key challenges arise: (i) the number of monitoring hypotheses to be considered can grow exponentially in the number of agents; and (ii) agents become physically and logically unconnected (unobservable) to their peers. This paper examines these challenges in teams of cooperating agents, focusing on a monitoring task that is of particular importance to robust teamwork: detecting disagreements among team-members. We present YOYO, a highly scalable disagreement-detection algorithm which guarantees sound detection in time linear in the number of agents despite the exponential number of hypotheses. In addition, we present new upper bounds about the number of agents that must be monitored in a team to guarantee disagreement detection. Both YOYO and the new bounds are explored analytically and empirically in thousands of monitoring problems, scaled to thousands of agents.",
    "neighbors": [
      49,
      196,
      724,
      963
    ],
    "mask": "Train"
  },
  {
    "node_id": 313,
    "label": 4,
    "text": "Classification Space for Augmented Surgery, an Augmented Reality Case Study One of the recent design goals in Human Computer Interaction has been to extend the sensorymotor capabilities of computer systems to combine the real and the virtual in order to assist the user in his environment. Such systems are called Augmented Reality (AR). Although AR systems are becoming more prevalent we still do not have a clear understanding of this interaction paradigm. In this paper we propose OPAS as a generic framework for classifying existing AR systems. Computer Assisted Medical Interventions (CAMI), for which the added value of AR has been demonstrated by experience, are discussed in light of OPAS. We illustrate OPAS using our system, CASPER (Computer ASsisted PERicardial puncture), a CAMI system which assists in surgical procedures (pericardial punctures). KEYWORDS: Augmented Surgery, CAMI, Augmented Reality, Classification Space  1. INTRODUCTION  The term \"Augmented Reality\" (AR) appears in the literature usually in conjunction with the term \"Virtual Reality\" (VR). Th...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 314,
    "label": 2,
    "text": "Amalthaea: Information Discovery and Filtering using a Multiagent Evolving Ecosystem Agents are semi-intelligent programs that assist the user in performing repetitive  and time-consuming tasks. Information discovery and information filtering are  a suitable domain for applying agent technology. Ideas drawn from the field of  autonomous agents and artificial life are combined in the creation of an evolving  ecosystem composed of competing and cooperating agents. A co-evolution model  of information filtering agents that adapt to the various user's interests and information  discovery agents that monitor and adapt to the various on-line information  sources is analyzed. Results from a number of experiments are presented and discussed.  Keywords: Agents, Information Filtering, Evolution, World-Wide-Web  1 Introduction  The exponential increase of computer systems that are interconnected in on-line networks has resulted in a corresponding exponential increase in the amount of information available on-line. This information is distributed among heterogeneous sources and is...",
    "neighbors": [
      492,
      561,
      606,
      952,
      1264
    ],
    "mask": "Train"
  },
  {
    "node_id": 315,
    "label": 3,
    "text": "Leveraging Mediator Cost Models with Heterogeneous Data Sources : Distributed systems require declarative access to diverse data sources of information. One approach to solving this heterogeneous distributed database problem is based on mediator architectures. In these architectures, mediators accept queries from users, process them with respect to wrappers, and return answers. Wrapper provide access to underlying data sources. To efficiently process queries, the mediator must optimize the plan used for processing the query. In classical databases, cost-estimate based query optimization is an effective method for optimization. In a heterogeneous distributed databases, cost-estimate based query optimization is difficult to achieve because the underlying data sources do not export cost information. This paper describes a new method that permits the wrapper programmer to export cost estimates (cost estimate formulas and statistics). For the wrapper programmer to describe all cost estimates may be impossible due to lack of information or burdensome due...",
    "neighbors": [
      530
    ],
    "mask": "Train"
  },
  {
    "node_id": 316,
    "label": 4,
    "text": "Software Infrastructure for Ubiquitous Computing Environments: Supporting Synchronous Collaboration with Heterogeneous Devices In ubiquitous computing environments, multiple users work with a wide range of different devices. In many cases, users interact and collaborate using multiple heterogeneous devices at the same time. The configuration of the devices should be able to change frequently due to a highly dynamic, flexible and mobile nature of new work practices. This produces new requirements for the architecture of an appropriate software infrastructure. In this paper, an architecture designed to meet these requirements is proposed. To test its applicability, this architecture was used as the basis for the implementation of BEACH, the software infrastructure of i-LAND (the ubiquitous computing environment at GMD-IPSI). It provides the functionality for synchronous cooperation and interaction with roomware components, i.e. room elements with integrated information technology. In conclusion, our experiences with the current implementation are presented.",
    "neighbors": [
      576,
      628,
      698
    ],
    "mask": "Train"
  },
  {
    "node_id": 317,
    "label": 0,
    "text": "MAP: Design and Implementation of a Mobile Agents Platform The recent development of telecommunication networks has contributed to the success of applications such as information retrieval and electronic commerce, as well as all the services that take advantage of communication in distributed systems. In this area, the emerging technology of mobile agents aroused considerable interest. Mobile agents are applications that can move through the network for carrying out a given task on behalf of the user. In this work we present a platform (called MAP (Mobile Agents Platform)) for the development and the management of mobile agents. The language used both for developing the platform and for carrying out the agents is Java. The platform gives the user all the basic tools needed for creating some applications based on the use of agents. It enables us to create, run, suspend, resume, deactivate, reactivate local agents, to stop their execution, to make them communicate each other and migrate.  Keywords: mobile agents, distributed computing, Java, net...",
    "neighbors": [
      259
    ],
    "mask": "Test"
  },
  {
    "node_id": 318,
    "label": 0,
    "text": "Klava: a Java Framework for Distributed and Mobile Applications Highly distributed networks have now become a common infrastructure for a new  kind of wide-area distributed applications whose key design principle is network awareness,  namely the ability of dealing with dynamic changes of the network environment.  Network-aware computing has called for new programming languages that exploit the  mobility paradigm as a basic interaction mechanism. In this paper we present the architecture  of Klava, an experimental Java framework for distributed applications and  code mobility. We explain how Klava implements code mobility by relying on Java  and show a few distributed applications that exploit mobile code and are programmed  in Klava.  Keywords: Code Mobility, Distributed Applications, Network Awareness, Language and Middleware Implementation, Tuple Spaces, Java.  1",
    "neighbors": [
      269
    ],
    "mask": "Train"
  },
  {
    "node_id": 319,
    "label": 1,
    "text": "A Bayesian Computer Vision System for Modeling Human Interactions Abstract\u00d0We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task [1]. The system is particularly concerned with detecting when interactions between people occur and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth. Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach [2]. We propose and compare two different state-based learning architectures, namely, HMMs and CHMMs for modeling behaviors and interactions. The CHMM model is shown to work much more efficiently and accurately. Finally, to deal with the problem of limited training data, a synthetic \u00aaAlife-style\u00ba training system is used to develop flexible prior models for recognizing human interactions. We demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training. Index Terms\u00d0Visual surveillance, people detection, tracking, human behavior recognition, Hidden Markov Models. 1",
    "neighbors": [
      26,
      1160
    ],
    "mask": "Test"
  },
  {
    "node_id": 320,
    "label": 5,
    "text": "Probabilistic Roadmap Methods are Embarrassingly Parallel In this paper we report on our experience parallelizing probabilistic roadmap motion planning methods (prms). We show that significant, scalable speedups can be obtained with relatively little effort on the part of the developer. Our experience is not limited to prms, however. In particular, we outline general techniques for parallelizing types of computations commonly performed in motion planning algorithms, and identify potential difficulties that might be faced in other efforts to parallelize sequential motion planning methods. 1 Introduction  Automatic motion planning has application in many areas such as robotics, virtual reality systems, and computer-aided design. Although many different motion planning methods have been proposed, most are not used in practice since they are computationally infeasible except for some restricted cases, e.g., when the robot has very few degrees of freedom (dof) [12, 16]. Indeed, there is strong evidence that any complete planner (one that is guaran...",
    "neighbors": [
      387,
      446
    ],
    "mask": "Train"
  },
  {
    "node_id": 321,
    "label": 2,
    "text": "Accessing Information and Services on the DAML-Enabled Web The DARPA Agent Markup Language (DAML) program aims to allow one to mark up web pages to indicate the meaning of their content; it is intended that the results delivered by a DAML-enabled browser will more closely match the intentions of the user than is possible with today's syntactically oriented search engines.  In this paper we present our vision of a DAML-enabled search architecture. We present a set of queries of increasing complexity that should be answered efficiently in a Semantic Web. We describe several scenarios illustrating how queries are processed, identifying the main software components necessary to facilitate the search. We examine the issue of inference in search, and we address how to characterize procedures and services in DAML, enabling a DAML query language to find web sites with specified capabilities.  Key Words: Semantic Web, DAML, inference, Web services, process modeling.  1.",
    "neighbors": [
      1086
    ],
    "mask": "Train"
  },
  {
    "node_id": 322,
    "label": 2,
    "text": "Discovering Unexpected Information from Your Competitors' Web Sites Ever since the beginning of the Web, finding useful information from the Web has been an important problem. Existing approaches include keyword-based search, wrapper-based information extraction, Web query and user preferences. These approaches essentially find information that matches the user's explicit specifications. This paper argues that this is insufficient. There is another type of information that is also of great interest, i.e., unexpected information, which is unanticipated by the user. Finding unexpected information is useful in many applications. For example, it is useful for a company to find unexpected information about its competitors, e.g., unexpected services and products that its competitors offer. With this information, the company can learn from its competitors and/or design counter measures to improve its competitiveness. Since the number of pages of a typical commercial site is very large and there are also many relevant sites (competitors), it is very difficult for a human user to view each page to discover the unexpected information. Automated assistance is needed. In this paper, we propose a number of methods to help the user find various types of unexpected information from his/her competitors' Web sites. Experiment results show that these techniques are very useful in practice and also efficient.  Keywords  Information interestingness, Web comparison, Web mining.  1.",
    "neighbors": [
      216,
      379,
      536,
      1017,
      1144
    ],
    "mask": "Train"
  },
  {
    "node_id": 323,
    "label": 2,
    "text": "Named Entity Recognition from Diverse Text Types Current research in Information Extraction  tends to be focused on application-specific systems  tailored to a particular domain. The Muse  system is a multi-purpose Named Entity recognition  system which aims to reduce the need for  costly and time-consuming adaptation of systems  to new applications, with its capability for  processing texts from widely di#ering domains  and genres. Although the system is still under  development, preliminary results are encouraging,  showing little degradation when processing  texts of lower quality or of unusual types. The  system currently averages 93% precision and  95% recall across a variety of text types.",
    "neighbors": [
      571
    ],
    "mask": "Validation"
  },
  {
    "node_id": 324,
    "label": 4,
    "text": "Evaluation of Recommender Algorithms for an Internet Information Broker Based Association rules are a widely used technique to generate recommendations in commercial and research recommender systems. Since more and more Web sites, especially of retailers, offer automatic recommender services using Web usage mining, evaluation of recommender algorithms becomes increasingly important. In this paper we first present a framework for the evaluation of different aspects of recommender systems based on the process of discovering knowledge in databases of Fayyad et al. and then we focus on the comparison of the performance of two recommender algorithms based on frequent itemsets. The first recommender algorithm uses association rules, and the other recommender algorithm is based on the repeat-buying theory known from marketing research. For the evaluation we concentrated on how well the patterns extracted from usage data match the concept of useful recommendations of users. We use 6 month of usage data from an educational Internet information broker and compare useful recommendations identified by users from the target group of the broker with the results of the recommender algorithms. The results of the evaluation presented in this paper suggest that frequent itemsets from purchase histories match the concept of useful recommendations expressed by users with satisfactory accuracy (higher than 70%) and precision (between 60% and 90%). Also the evaluation suggests that both algorithms studied in the paper perform similar on real-world data if they are tuned properly.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 325,
    "label": 4,
    "text": "End of First Year Report displays of information (graphs, plots, etc.) are a recent invention at around 1750-1800 [15]. Andrews defines Information Visualisation as the visual presentation of information spaces and structures to facilitate their rapid assimilation and understanding [149]. In the same document, the authors give a collection of Information Visualisation pointers (references also available at http://www.iicm.edu/hci/ivis). A more complete on-line document for Information Visualisation, from Andrews is available at http://www.iicm.edu.hci/ivis/node2.htm. A report on three-dimensional Information Visualisation is given by Young, also available on-line, at http://www.dur.ac.uk/~dcs3py/pages/work/documents/litsurvey /IV-Survey/ [150]. This report gives a visualisation techniques enumeration and a survey of research visualisation systems. Two other Web resources for Information Visualisation are Olive (Online of Information Visualisation Environments - http://otal.umd.edu/Olive/), and the CS348 course...",
    "neighbors": [
      454
    ],
    "mask": "Validation"
  },
  {
    "node_id": 326,
    "label": 4,
    "text": "Gathering User Interface Design Requirements for Social Computing Design for cooperation is a challenge. As designers we note that as we are moving towards the final years of this century, several areas have achieved significant breakthroughs. Among them, it is easy to perceive that areas of Computing and Telecommunications have had an impact of paramount importance to society as a whole. These technologies have allowed an increasing integration of research fields, people of various backgrounds and abilities as well as made the interaction of different cultures possible. As a result, we have been living in the Internet era with a very large number of Web sites which can be visited, queried and played with. That constitutes what we call social computing. Application examples are: digital libraries, health care information systems, Physics collaboratories, and Web-based entertainments like interactive Web games. Within this context, we are concerned with the user interface design requirements gathering for such systems. In that sense, we present a prot...",
    "neighbors": [
      733
    ],
    "mask": "Train"
  },
  {
    "node_id": 327,
    "label": 5,
    "text": "A Tableau-Based Proof Method for Temporal Logics of Knowledge and Belief . In this paper we define two logics, KLn and BLn , and present tableau-based decision procedures for both. KLn is a temporal logic of knowledge. Thus, in addition to the usual connectives of linear discrete temporal logic, it contains a set of unary modal connectives for representing the knowledge  possessed by agents. The logic BLn is somewhat similar; it is a temporal logic that contains connectives for representing the beliefs of agents. In addition to a complete formal definition of the two logics and their decision procedures, the paper includes a brief review of their applications in AI and mainstream computer science, correctness proofs for the decision procedures, a number of worked examples illustrating the decision procedures, and some pointers to further work.  KEYWORDS:Temporal logics of knowledge and belief, theorem proving, tableau. 1 Introduction  This paper presents two logics, called KLn and BLn respectively, and gives tableau-based decision procedures for both. The l...",
    "neighbors": [
      82,
      159,
      953
    ],
    "mask": "Validation"
  },
  {
    "node_id": 328,
    "label": 1,
    "text": "GIB: Steps Toward an Expert-Level Bridge-Playing Program This paper describes Goren In a Box (gib), the first bridge-playing program to approach the level of a human expert. We give a basic overview of the algorithms used, describe their strengths and weaknesses, and present the results of experiments comparing gib to both human opponents and earlier programs. Introduction Of all the classic games of skill, only card games and Go have yet to see the appearance of serious computer challengers. In Go, this appears to be because the game is fundamentally one of pattern recognition as opposed to search; the brute-force techniques that have been so successful in the development of chess-playing programs have failed almost utterly to deal with Go's huge branching factor. Indeed, the arguably strongest Go program in the world was beaten by Janice Kim in the AAAI-97 Hall of Champions after Kim had given the program a monumental 25 stone handicap. Card games appear to be different. Perhaps because they are games of imperfect information, or perhaps...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 329,
    "label": 0,
    "text": "Autonomous Agents For Business Process Management : Traditional approaches to managing business processes are often inadequate for large-scale, organisation -wide, dynamic settings. However since Internet and Intranet technologies have become widespread, an increasing number of business processes exhibit these properties. Therefore a new approach is needed. To this end, we describe the motivation, conceptualisation, design and implementation of a novel agent-based business process management system. The key advance of our system is that responsibility for enacting various components of the business process is delegated to a number of autonomous problem solving agents. To enact their role, these agents typically interact and negotiate with other agents in order to coordinate their actions and to buy in the services they require. This approach leads to a system that is significantly more agile and robust than its traditional counterparts. To help demonstrate these benefits, a companion paper describes the application of our system to a ...",
    "neighbors": [
      683,
      724
    ],
    "mask": "Validation"
  },
  {
    "node_id": 330,
    "label": 1,
    "text": "Learning the Face Space - Representation and Recognition This paper advances an integrated learning and evolutionary computation methodology for approaching the task of learning the face space. The methodology is geared to provide a framework whereby enhanced and robust face coding and classification schemes can be derived and evaluated using both machine and human benchmark studies. In particular we take an interdisciplinary approach, drawing from the accumulated and vast knowledge of both the computer vision and psychology communities, and describe how evolutionary computation and statistical learning can engage in mutually beneficial relationships in order to define an exemplar (Absolute)-Based Coding (ABC) multidimensional face space representation for successfully coping with changing population (face) types, and to leverage past experience for incremental face space definition.  1. Introduction  Among the most challenging tasks for visual form (`shape') analysis and object recognition are understanding how people process and recognize ...",
    "neighbors": [
      949,
      1118
    ],
    "mask": "Train"
  },
  {
    "node_id": 331,
    "label": 1,
    "text": "Regularizing AdaBoost Boosting methods maximize a hard classification margin and are known as powerful techniques that do not exhibit overfitting for low noise cases. Also for noisy data boosting will try to enforce a hard margin and thereby give too much weight to outliers, which then leads to the dilemma of non-smooth fits and overfitting. Therefore we propose three algorithms to allow for soft margin classification by introducing regularization with slack variables into the boosting concept: (1) AdaBoost reg and regularized versions of (2) linear and (3) quadratic programming AdaBoost. Experiments show the usefulness of the proposed algorithms in comparison to another soft margin classifier: the support vector machine.  1 Introduction  Boosting and other ensemble methods have been used with success in several applications, e. g. OCR [12, 7]. For low noise cases several lines of explanation have been proposed as candidates for explaining the well functioning of boosting methods. (a) Breiman proposed that ...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 332,
    "label": 1,
    "text": "The Case Against Accuracy Estimation for Comparing Induction Algorithms We analyze critically the use of classification accuracy to compare classifiers on natural data sets, providing a thorough investigation using ROC analysis, standard machine learning algorithms, and standard benchmark data sets. The results raise serious concerns about the use of accuracy for comparing classifiers and draw into question the conclusions that can be drawn from such studies. In the course of the presentation, we describe and demonstrate what we believe to be the proper use of ROC analysis for comparative studies in machine learning research. We argue that this methodology is preferable both for making practical choices and for drawing scientific conclusions. 1 INTRODUCTION  Substantial research has been devoted to the development and analysis of algorithms for building classifiers, and a necessary part of this research involves comparing induction algorithms. A common methodology for such evaluations is to perform statistical comparisons of the accuracies of learned class...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 333,
    "label": 0,
    "text": "Integrating Peer-to-Peer Networking and Computing in the Agentscape Framework The combination of peer-to-peer networking and agentbased computing seems to be a perfect match. Agents are cooperative and communication oriented, while peerto -peer networks typically support distributed systems in which all nodes have equal roles and responsibilities. AgentScape is a framework designed to support large-scale multi-agent systems. Pole extends this framework with peerto -peer computing. This combination facilitates the development and deployment of new agent-based peer-to-peer applications and services.",
    "neighbors": [
      1054,
      1227
    ],
    "mask": "Train"
  },
  {
    "node_id": 334,
    "label": 4,
    "text": "Real-Time American Sign Language Recognition Using Desk and Wearable Computer Based Video Hidden Markov models (HMM's) have been used prominently and successfully in speech recognition and, more recently, in handwriting recognition. Consequently, they seem ideal for visual recognition of complex, structured hand gestures such as are found in sign language. We describe two experiments that demonstrate a realtime HMM-based system for recognizing sentence level American Sign Language (ASL) without explicitly modeling the fingers. The first experiment tracks hands wearing colored gloves and attains a word accuracy of 99%. The second experiment tracks hands without gloves and attains a word accuracy of 92%. Both experiments have a 40 word lexicon.  1 Introduction  While there are many different types of gestures, the most structured sets belong to the sign languages. In sign language, each gesture already has assigned meaning, and strong rules of context and grammar may be applied to make recognition tractable. To date, most work on sign language recognition has employed expensi...",
    "neighbors": [
      54,
      307,
      351,
      497,
      665,
      779,
      1129
    ],
    "mask": "Test"
  },
  {
    "node_id": 335,
    "label": 3,
    "text": "Structure and Performance of Decision Support Algorithms on Active Disks Growth and usage trends for large decision support databases indicate that there is a need for architectures that scale the processing power as the dataset grows. These trends indicate that the processing demand for large decision support databases is growing faster than the improvement in performance of commodity processors. To meet this need, several researchers have recently proposed Active Disk/IDISK architectures which integrate substantial processing power and memory into disk units. In this paper, we examine the utility of Active Disks for decision support databases. We try to answer the following questions. First, is it possible to restructure algorithms for common decision support tasks to utilize Active Disks? Second, how does the performance of Active Disks compare with that of traditional servers for these tasks? Finally, how would Active Disks be integrated into the software architecture of decision support databases? 1 Introduction  Growth and usage trends for large decis...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 336,
    "label": 3,
    "text": "Algorithms for Temporal Query Operators in XML Databases The contents of an XML database or XML/Web data warehouse is seldom static. New documents  are created, documents are deleted, and more important: documents are updated. In many  cases, we want to be able to search in historical versions, retrieve documents valid at a certain  time, query changes to documents, etc. This can be supported by extending the system with temporal  database features. In this paper we describe the new query operators needed in order to  support an XML query language which supports temporal operations. We also describe the algorithms  which can make efficient implementation of these query operators possible.  Keywords: XML, temporal databases, query processing  1",
    "neighbors": [
      705,
      912,
      1025,
      1069
    ],
    "mask": "Validation"
  },
  {
    "node_id": 337,
    "label": 3,
    "text": "Algebraic Models for Contextual Nets We extend the algebraic approach of Meseguer and Montanari from ordinary place/transition Petri nets to contextual nets, covering both the collective and the individual token philosophy uniformly along the two interpretations of net behaviors.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 338,
    "label": 1,
    "text": "An Ejection Chain Approach for the Generalized Assignment Problem this paper, we propose an ejection chain approach under the framework of tabu search (TS) for the generalized assignment problem (GAP), which is known to be NP-hard (Sahni and Gonzalez 1976). GAP seeks a minimum cost assignment of n jobs to m agents subject to a resource constraint for each agent. Among various heuristic algorithms developed for GAP are: a combination of the greedy method and local search by Martello and Toth (1981, 1990); a tabu search and simulated annealing approach by Osman (1995); a genetic algorithm by Chu and Beasley (1997); VDS methods by Amini and Racer (1995) and Racer and Amini (1994); a tabu search approach by Laguna et al. (1995) (which is proposed for a generalization of GAP); a set partitioning heuristic by Cattrysse et al. (1994); a relaxation heuristic by Lorena and Narciso (1996); a GRASP and MAX-MIN ant system combined with local search and tabu search by Lourenco and Serra (1998); a linear relaxation heuristic by Trick (1992); and so on. Many exact algorithms have also been proposed (e.g., Nauss 2003, Savelsbergh 1997). A simpler version of an ejection chain approach has also been proposed for the GAP in Laguna et al. (1995). Our ejection chain is based on the idea described in Glover (1997)",
    "neighbors": [
      1111
    ],
    "mask": "Train"
  },
  {
    "node_id": 339,
    "label": 3,
    "text": "Atomi - Automated Reconstruction Of Topographic Objects From Aerial Images Using Vectorized Map Information The project ATOMI is a co-operation between the Federal Office of Topography (L+T) and ETH Zurich. The aim of ATOMI is to update vector data of road centerlines and building roof outlines from 1:25,000 maps, fitting it to the real landscape, improve the planimetric accuracy to 1m and derive height information (one representative height for each building) with 1-2 m accuracy. This update should be achieved by using image analysis techniques developed at ETH Zurich and digital aerial imagery. The whole procedure should be implemented as a stand-alone software package, able to import and export data as used at L+T. It should be quasi operational, fast, and the most important reliable. We do not aim at full automation (ca. 80% completeness is a plausible target). The paper will present in detail the aims, input data, strategy and general methods used in ATOMI. We will also present an overview of the results achieved up to now, and problems faced in building and road reconstruction. More de...",
    "neighbors": [
      93
    ],
    "mask": "Validation"
  },
  {
    "node_id": 340,
    "label": 5,
    "text": "Causal Models of Mobile Service Robot Behavior Temporal projection, the process of predicting what will happen when a robot executes its plan, is essential for autonomous service robots to successfully plan their missions. This paper describes a causal model of the behavior exhibited by the mobile robot Rhino  when running concurrent reactive plans for performing office delivery jobs. The model represents aspects of robot behavior that cannot be represented by most action models used in AI planning: it represents the temporal structure of continuous control processes, several modes of their interferences, and various kinds of uncertainty. This enhanced expressiveness enables xfrm  (McD92; BM94), a robot planning system, to predict, and therefore forestall, various kinds of behavior flaws including missed deadlines whilst exploiting incidental opportunities. The proposed causal model is experimentally validated using the robot and its simulator.  Introduction  Temporal projection, the process of predicting what will happen when a ro...",
    "neighbors": [
      459
    ],
    "mask": "Train"
  },
  {
    "node_id": 341,
    "label": 2,
    "text": "Boosting and Rocchio Applied to Text Filtering We discuss two learning algorithms for text filtering: modified Rocchio and a boosting algorithm called AdaBoost. We show how both algorithms can be adapted to maximize any general utility matrix that associates cost (or gain) for each pair of machine prediction and correct label. We first show that AdaBoost significantly outperforms another highly effective text filtering algorithm. We then compare AdaBoost and Rocchio over three large text filtering tasks. Overall both algorithms are comparable and are quite effective. AdaBoost produces better classifiers than Rocchio when the training collection contains a very large number of relevant documents. However, on these tasks, Rocchio runs much faster than AdaBoost. 1 Introduction  With the explosion in the amount of information available electronically, information filtering systems that automatically send articles of potential interest to a user are becoming increasingly important. If users indicate their interests to a filtering system...",
    "neighbors": [
      142,
      674,
      1001,
      1090,
      1094
    ],
    "mask": "Train"
  },
  {
    "node_id": 342,
    "label": 0,
    "text": "Impact: A Platform for Collaborating Agents twork. The Impact server provides the infrastructure upon which different Impact agents can interact. To avoid a performance bottleneck, multiple copies of the server can be replicated and scattered across the network. Impact agents A set of data objects can be represented in a wide variety of ways. When building an application, we'd like to select a data structure that supports the application operations that are the most frequently executed, the most critical, or both. So, any definition of an agent must support such flexible choice of data structures, and agentization must let us extend arbitrary data representations. In Impact, an agent consists of any body of software code whatsoever, with the associated wrapper. Figure 2 shows such an agent's architecture. The software code. The agent's code consists of two parts: . a set of data structures (or data types) manipulated by the agent. For example, if we are building a database agen",
    "neighbors": [
      29,
      1236
    ],
    "mask": "Test"
  },
  {
    "node_id": 343,
    "label": 1,
    "text": "Finding Counterexamples to Inductive Conjectures We present an implementation of a method for \u00a3nding counterexamples to universally quanti\u00a3ed inductive conjectures in \u00a3rst-order logic. Our method uses the proof by consistency strategy to guide a search for a counterexample and a standard \u00a3rst-order theorem prover to perform a concurrent check for inconsistency. We explain brie\u00a4y the theory behind the method, describe our implementation, and evaluate results achieved on a variety of incorrect conjectures from various sources. Some work in progress is also presented: we are applying the method to the veri\u00a3cation of cryptographic security protocols. In this context, a counterexample to a security property can indicate an attack on the protocol, and our method extracts the trace of messages exchanged in order to effect the attack. This application demonstrates the advantages of the method, in that quite complex side conditions decide whether a particular sequence of messages is possible. Using a theorem prover provides a natural way of dealing with this. Some early results are presented and we discuss future work. 1",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 344,
    "label": 1,
    "text": "Induction of decision trees using RELIEFF In the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies between them. Greedy search prevents current inductive machine learning algorithms to detect significant dependencies between the attributes. Recently, Kira and Rendell developed the RELIEF algorithm for estimating the quality of attributes that is able to detect dependencies between attributes. We show strong relation between RELIEF's estimates and impurity functions, that are usually used for heuristic guidance of inductive learning algorithms. We propose to use RELIEFF, an extended version of RELIEF, instead of myopic impurity functions. We have reimplemented Assistant, a system for top down induction of decision trees, using RELIEFF as an estimator of attributes at each selection step. The algorithm is tested on several artificial and several real world problems. Results show the advantage of the presented approach to inductive lea...",
    "neighbors": [
      1016
    ],
    "mask": "Train"
  },
  {
    "node_id": 345,
    "label": 4,
    "text": "Situated Computing: A Paradigm to Enhance the Mobile User's Interaction When people interact with computers, they have to pay attention for things that are not related to the situation of the problems because the interfaces are not contextualized to their working environment. Hence it is sometimes hard to integrate computers as embedded tools, which facilitate users to accomplish their objectives easily in the working life. Situated computing is a new paradigm for mobile computer users based on their physical context and activities carried out in the workspace. It defines the infrastructure how the situated interaction can be provided using applications. In this chapter we first describe a model called situation metaphor to design interaction between the user and mobile computers as the basis for the situated computing. Thereafter, a framework called Situated Information Filing and Filtering (SIFF) is presented as the foundation for situated application development. In general a three-stages schema is given considerting the top stage for situated applications. Four applications based on the SIFF are also presented to demonstrate the enhancement of mobile user's interaction that can be achieved.",
    "neighbors": [
      798
    ],
    "mask": "Train"
  },
  {
    "node_id": 346,
    "label": 2,
    "text": "Jotmail: A Voicemail Interface That Enables You to See What Was Said stevew/julia/urs @ research.att.com Voicemail is a pervasive, but under-researched tool for workplace communication. Despite potential advantages of voicemail over email, current phone-based voicemail UIs are highly problematic for users. We present a novel, Web-based, voicemail interface, Jotmail. The design was based on data from several studies of voicemail tasks and user strategies. The GUI has two main elements: (a) personal annotations that serve as a visual analogue to underlying speech; (b) automatically derived message header information. We evaluated Jotmail in an 8-week field trial, where people used it as their only means for accessing voicemail. Jotmail was successful in supporting most key voicemail tasks, although users ' electronic annotation and archiving behaviors were different from our initial predictions. Our results argue for the utility of a combination of annotation based indexing and automatically derived information, as a general technique for accessing speech archives.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 347,
    "label": 2,
    "text": "Learning to Extract Symbolic Knowledge from the World Wide Web The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more e ective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach istodevelop a trainable information extraction system that takes two inputs. The rst is an ontology that de nes the classes (e.g., Company, Person, Employee, Product) and relations (e.g., Employed.By, Produced.By) ofinterest when creating the knowledge base. The second is a set of training data consisting of labeled regions of hypertext that represent instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system that has created a knowledge base describing university people, courses, and research projects.",
    "neighbors": [
      280,
      439,
      496,
      543,
      609,
      655,
      660,
      682,
      726,
      851,
      865,
      1021,
      1090,
      1153,
      1206
    ],
    "mask": "Train"
  },
  {
    "node_id": 348,
    "label": 2,
    "text": "Using linear classifiers in the integration of user modeling and text content analysis in the personalization of a Web-based Spanish News Service Nowadays many newspapers and news agencies offer personalized  information access services and, moreover, there is a growing interest in the  improvement of these services. In this paper we present a methodology useful  to improve the intelligent personalization of news services and the way it has  been applied to a Spanish relevant newspaper: ABC. Our methodology  integrates textual content analysis tasks and machine learning techniques to  achieve an elaborated user model, which represents separately short-term needs  and long-term multi-topic interests. The characterization of a user's interests  includes his preferences about structure (newspaper sections), content and  information delivery. A wide coverage and non-specific-domain classification  of topics and a personal set of keywords allow the user to define his preferences  about content. Machine learning techniques are used to obtain an initial  representation of each category of the topic classification. Finally, we introduce  some details about the Mercurio system, which is being used to implement this  methodology for ABC. We describe our experience and an evaluation of the  system in comparison with other commercial systems.",
    "neighbors": [
      780,
      1083
    ],
    "mask": "Train"
  },
  {
    "node_id": 349,
    "label": 3,
    "text": "Logical Semantics and Language for Databases with Partial and Complete Tuples and Sets (Extended Abstract) )  Mengchi Liu  Department of Computer Science University of Regina, Regina, Saskatchewan, Canada S4S 0A2 Email: mliu@cs.uregina.ca Abstract  We discuss the semantics of complex object databases with both partial and complete tuples and sets. We redefine the notion of database to reflect the existence of partial and complete tuples and sets and study how to integrate partial information about tuples and sets spread in the database and check consistency in the meantime. We also present a deductive language RLOG  II  for complex objects with null/unknown and inconsistent values based on Relationlog. The main novel feature of the language is that it is the only one that supports the null extended nested relational algebra operations directly and more importantly recursively. This work provides a firm logical foundation for nested relational and complex object databases that have both partial and complete tuples and sets and solves an open problem of supporting recursion with generic null/...",
    "neighbors": [
      861
    ],
    "mask": "Train"
  },
  {
    "node_id": 350,
    "label": 3,
    "text": "ROL2: Towards a Real Deductive Object-Oriented Database Language ROL is a strongly typed deductive object-oriented database language. It integrates  many important features of deductive databases and object-oriented databases. However,  it is only a structurally object-oriented language. In this paper, we present our  extension of ROL called ROL2. Most importantly, ROL2 supports behaviorally objectoriented  features such as rule-based methods and encapsulation so that it is a now real  deductive object-oriented database language. It supports in a rule-based framework  nearly all important object-oriented features such as object identity, complex objects,  typing, information hiding, rule-based methods, encapsulation of such methods, overloading,  late binding, polymorphism, class hierarchies, multiple structural and behavioral  inheritance with overriding, blocking, and conict handling. It is so far the only  deductive system that supports all these features in a pure rule-based framework.  Keywords: object-oriented databases, deductive databases, ...",
    "neighbors": [
      1063
    ],
    "mask": "Train"
  },
  {
    "node_id": 351,
    "label": 4,
    "text": "Virtual Keyboards This paper describes a novel scheme for vision-based human computer interaction in  which traditional input and output devices, monitors, keyboards and mice, are replaced with  augmented reality displays, projection systems and cameras. User input is accomplished by  projecting an image of the interface onto a flat surface in the scene which is monitored with  a video camera. The scheme hinges on the observation that the relationship between the  three surfaces of interest, the work surface, the virtual keyboard and the image obtained by  the camera, can be characterized by projective transformations of RP  2  . This observation  leads to a fast and accurate online calibration algorithm.  The basic advantage of the vision based interaction techniques proposed in this paper  is that they do not involve mechanical input devices such as keyboards, mice and touch  screens. There are no moving parts and no wires to connect to the interface surface. By  avoiding a physical instantiation of t...",
    "neighbors": [
      334
    ],
    "mask": "Validation"
  },
  {
    "node_id": 352,
    "label": 4,
    "text": "Adaptable and Adaptive Information Provision for All Users, Including Disabled and Elderly People Due to the tremendously increasing popularity of the World-Wide Web, hypermedia is going to be the leading online information medium for some years to come and will most likely become the standard gateway for citizens to the \"information highway\". Already today, visitors of web sites are generally heterogeneous and have different needs, and this is likely to increase in the future. The aim of the AVANTI project is to cater hypermedia information to these individual needs by adapting the content and the presentation of web pages to each individual user. The special needs of elderly and disabled users are also partly considered. A model of the characteristics of user groups, individual users and usage environments, and a domain model are exploited in the adaptation process. One aim of this research is to verify that adaptation and user modeling techniques that were hitherto mostly used for catering interactive software systems to able-bodied users also prove useful for adaptation to users with special needs. Another original aspect is the development of a network-wide user modeling server that can concurrently accommodate the user modeling needs of several applications and several instances of an application within a distributed computing environment.",
    "neighbors": [
      793
    ],
    "mask": "Train"
  },
  {
    "node_id": 353,
    "label": 0,
    "text": "Autonomous Robot that Uses Symbol Recognition and Artificial Emotion to Attend the AAAI Conference This paper describes our approach in designing an autonomous  robot for the AAAI Mobile Robot Challenge,  making the robot attend the National Conference on AI.  The goal was to do a simplified version of the whole  task, by integrating methodologies developed in various  research projects conducted in our laboratory. Original  contributions are the use of a symbol recognition technique  to make the robot read signs, artificial emotion for  expressing the state of the robot in the accomplishment  of its goals, a touch screen for human-robot interaction,  and a charging station for allowing the robot to recharge  when necessary. All of these aspects are influenced by  the different steps to be followed by the robot attendee  to complete the task from start-to-end.  Introduction  LABORIUS is a young research laboratory interested in designing autonomous systems that can assist human in real life tasks. To do so, robots need some sort of \"social intelligence \", giving them the ability to ...",
    "neighbors": [
      1137
    ],
    "mask": "Train"
  },
  {
    "node_id": 354,
    "label": 3,
    "text": "Efficient Concurrency Control for Broadcast Environments A crucial consideration in environments where data is broadcast to clients is the low bandwidth available for clients to communicate with servers. Advanced applications in such environments do need to read data that is mutually consistent aswell as current. However, given the asymmetric communication capabilities and the needs of clients in mobile environments, traditional serializability-based approaches are too restrictive, unnecessary, and impractical. We thus propose the use of a weaker correctness criterion called update consistency and outline mechanisms based on this criterion that ensure (1) the mutual consistency of data maintained by the server and read by clients, and (2) the currency of data read by clients. Using these mechanisms, clients can obtain data that is current and mutually consistent \"off the air\", i.e., without contacting the server to, say, obtain locks. Experimental results show a substantial reduction in response times as compared to existing (serializability-based) approaches. A further attractive feature of the approach is that if caching is possible at a client, weaker forms of currency can be obtained while still satisfying the mutual consistency of data.",
    "neighbors": [
      174,
      373,
      470
    ],
    "mask": "Train"
  },
  {
    "node_id": 355,
    "label": 5,
    "text": "Ensemble Learning for Intrusion Detection in Computer Networks The security of computer networks plays a strategic role in modern computer systems. In order to enforce high protection levels against threats, a number of software tools are currently developed. Intrusion Detection Systems aim at detecting intruder who eluded the \"first line\" protection. In this paper, a pattern recognition approach to network intrusion detection based on ensemble learning paradigms is proposed. The potentialities of such an approach for data fusion and some open issues are outlined.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 356,
    "label": 0,
    "text": "Planning Agents in James Abstract \u2014 Testing is an obligatory step in developing multi-agent systems. For testing multi-agent systems in virtual, dynamic environments, simulation systems are required that support a modular, declarative construction of experimental frames, that facilitate the embeddence of a variety of agent architectures, and that allow an efficient parallel, distributed execution. We introduce the system James (A Java-Based Agent Modeling Environment for Simulation). In James agents and their dynamic environment are modeled as reflective, time triggered state automata. Its possibilities to compose experimental frames based on predefined components, to express temporal interdependencies, to capture the phenomenon of pro-activeness and reflectivity of agents are illuminated by experiments with planning agents. The underlying planning system is a general purpose system, about which no empirical results exist besides traditional static benchmark tests. We analyze the interplay between heuristics for selecting goals, viewing range, commitment strategies, explorativeness, and trust in the persistence of the world and uncover properties of the agent, the planning engine and the chosen test scenario: Tileworld. I.",
    "neighbors": [
      957,
      1266
    ],
    "mask": "Train"
  },
  {
    "node_id": 357,
    "label": 0,
    "text": "An Architecture to Guide Crowds Using a Rule-Based Behavior System This paper describes a Client/Server architecture to combine the  control of human agents performing \"intelligent actions\" (guided by a Rule-Based Behavior System -- RBBS) with the management  of autonomous crowds which perform pre-programmed actions.  Our main goal being ability to model crowds formed by a large  number of agents (e.g. 1000), we have used pre-programmed  actions and basic behaviors. In addition, RBBS provides the user with an interface for real-time behavior control of some groups of  the crowd. This paper presents how the Server application deals  with virtual human agent's behaviors using a rule-based system.  Keywords  Multi-agent co-ordination and collaboration, agent architectures,  network agents, real-time performance, synthetic agents, rulebased  system, human crowds' model.  1. INTRODUCTION  Virtual humans grouped together to form crowds populating virtual worlds allow a more intuitive feeling of presence. However, the crowd is not only needed to create an at...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 358,
    "label": 1,
    "text": "Parameter-less Genetic Algorithm: A Worst-case Time and Space Complexity Analysis In this paper, the worst-case analysis of the time and space complexity of the parameter-less genetic algorithm versus the genetic algorithm with an optimal population size is provided and the results of the analysis are discussed. Since the assumptions in order for the analysis to be correct are very weak, the result is applicable to a wide range of problems. Various configurations of the parameter-less genetic algorithm are considered and the results of their time and space complexity are compared. 1 Introduction  A parameter-less genetic algorithm (Harik & Lobo, 1999) is an alternative to a common trialand -error method of tweaking the values of the parameters of the genetic algorithm in order to find a set-up to accurately and reliably solve a given problem. The algorithm manages a number of independent runs of the genetic algorithm with different population sizes with the remaining parameters set to fixed values according to the theory of genetic algorithms' control maps introduce...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 359,
    "label": 0,
    "text": "The Repair of Speech Act Misunderstandings by Abductive Inference this paper, we have concentrated on the repair of mis-understanding. Our colleagues Heeman and Edmonds have looked at the repair of non-understanding. The difference between the two situations is that in the former, the agent derives exactly one interpretation of an utterance and hence is initially unaware of any problem; in the latter, the agent derives either more than one interpretation, with no way to choose between them, or no interpretation at all, and so the problem is immediately apparent. Heeman and Edmonds looked in particular at cases in which a referring expression uttered by one conversant was not understood by the other (Heeman and Hirst 1995; Edmonds 1994; Hirst et. al. 1994). Clark and his colleagues (Clark and Wilkes-Gibbs 1986; Clark 1993) have shown that in such situations, conversants will collaborate on repairing the problem by, in effect, negotiating a reconstruction or elaboration of the referring expression. Heeman and Edmonds model this with a plan recognition and generation system that can recognize faulty plans and try to repair them. Thus (as in our own model) two copies of the system can converse with each other, negotiating referents of referring expressions that are not understood by trying to recognize the referring plans of the other, repairing them where necessary, and presenting the new referring plan to the other for approval.",
    "neighbors": [
      178
    ],
    "mask": "Test"
  },
  {
    "node_id": 360,
    "label": 3,
    "text": "SENTINEL: A Multiple Engine Information Retrieval and Visualization System We describe a prototype Information Retrieval system, SENTINEL, under development at Harris Corporation's Information Systems Division. SENTINEL is a fusion of multiple information retrieval technologies, integrating n-grams, a vector space model, and a neural network training rule. One of the primary advantages of SENTINEL is its 3-dimenstional visualization capability that is based fully upon the mathematical representation of information within SENTINEL. This 3-dimensional visualization capability provides users with an intuitive understanding, with relevance feedback/query refinement techniques that can be better utilized, resulting in higher retrieval accuracy (precision).",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 361,
    "label": 0,
    "text": "Foreign Event Handlers to Maintain Information Consistency and System Adequacy this paper is to describe novel applications of Mobile Code technology which have not appeared yet but should be feasible with our current knowledge of the domain. These new applications contradict the often-made observation that Mobile Code is just another technique that does not really bring much more possibilities than existing technologies for distributed applications. There is a whole class of problems that have not received much attention yet and that are not well managed by current environments. These are the problems of maintaining consistency of dynamic information and maintaining systems in adequacy with the ever changing requirements of customers. Our motivation is that, besides the quantitative improvements that most people expect from using Mobile Code, there is also a qualitative benefit which is even more important but not universally recognized now: Mobile Code allows communication with less conventions than message passing [5, 3]. Processes interconnected by Mobile Code still have to agree on high level encoding and synchronization primitives but these agreements are only a fraction of what is necessary to communicate. Many context dependent aspects can be encapsulated inside Mobile Code and changed when the context changes. Encapsulation has the same benefits here as in other software engineering domains: it reduces the dependency between components, thus reducing the number of modifications that we must make to software in order to adapt it to new requirements. For this reason we think that it is the best way to cope with systems that are distributed, hence not manageable by a single person or organization; that are dynamic, because the information they contain must change when the world itself changes; and that are evolving since the users discover n...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 362,
    "label": 5,
    "text": "A Competitive Layer Model for Feature Binding and Sensory Segmentation We present a recurrent neural network for feature binding and sensory segmentation,  the competitive layer model (CLM). The CLM uses topographically structured  competitive and cooperative interactions in a layered network to partition a set of input  features into salient groups. The dynamics is formulated within a standard additive recurrent  network with linear threshold neurons. Contextual relations among features are  coded by pairwise compatibilities which define an energy function to be minimized by  the neural dynamics. Due to the usage of dynamical winner-take-all circuits the model  gains more flexible response properties than spin models of segmentation by exploiting  amplitude information in the grouping process. We prove analytic results on the convergence  and stable attractors of the CLM, which generalize earlier results on winner-take-all networks, and incorporate deterministic annealing for robustness against local  minima. The piecewise linear dynamics of the CLM allows a linear eigensubspace  analysis which we use to analyze the dynamics of binding in conjunction with annealing.  For the example of contour detection we show how the CLM can integrate  figure-ground segmentation and grouping into a unified model.",
    "neighbors": [
      1095
    ],
    "mask": "Train"
  },
  {
    "node_id": 363,
    "label": 0,
    "text": "Multiagent Systems Specification by UML Statecharts Aiming at Intelligent Manufacturing Multiagent systems are a promising new paradigm in computing,  which are contributing to various fields. Many theories and technologies  have been developed in order to design and specify multiagent systems,  however, no standard procedure is used at present. Industrial applications  often have a complex structure and need plenty of working  resources. They require a standard specification method as well. As the  standard method to design and specify software systems, we believe that  one of the key words is simplicity for their wide acceptance. In this paper,  we propose a method to specify multiagent systems, namely with UML  statecharts. We use them for specifying almost all aspects of multiagent  systems, because we think that it is an advantage to keep everything in  one type of diagram. We apply",
    "neighbors": [
      184,
      508,
      602
    ],
    "mask": "Train"
  },
  {
    "node_id": 364,
    "label": 3,
    "text": "A Performance Evaluation of Alternative Mapping Schemes for Storing XML Data in a Relational Database XML is emerging as one of the dominant data formats for data processing on the Internet. To query  XML data, query languages likeXQL, Lorel, XML-QL, or XML-GL have been proposed. In this paper,  we study how XML data can be stored and queried using a standard relational database system. For this  purpose, we present alternative mapping schemes to store XML data in a relational database and discuss  how XML-QL queries can be translated into SQL queries for every mapping scheme. We present the  results of comprehensive performance experiments that analyze the tradeo#s of the alternative mapping  schemes in terms of database size, query performance and update performance. While our discussion  is focussed on XML and XML-QL, the results of this paper are relevant for most semi-structured data  models and most query languages for semi-structured data.  1 Introduction  It has become clear that not all applications are met by the relational, object-relational, or object-oriented data models. ...",
    "neighbors": [
      17,
      78,
      306,
      634
    ],
    "mask": "Train"
  },
  {
    "node_id": 365,
    "label": 1,
    "text": "Saccadic Search with Gabor features applied to Eye Detection and Real-Time Head Tracking The Gabor decomposition is a ubiquitous tool in computer vision. Nevertheless, it is generally considered computationally demanding for active vision applications. We suggest an attention-driven approach to feature detection inspired by the human saccadic system. A dramatic speedup is achieved by computing the Gabor decomposition only on the points of a sparse retinotopic grid. An off-line eye detection application and a real-time head localisation and tracking system are presented. The real-time system features a novel eyeball-mounted camera designed to simulate the dynamic performance of the human eye and is, to the best of our knowledge, the first example of active vision system based on the Gabor decomposition.",
    "neighbors": [
      1152
    ],
    "mask": "Train"
  },
  {
    "node_id": 366,
    "label": 4,
    "text": "How Many Separately Evolved Emotional Beasties Live Within Us? A problem which bedevils the study of emotions, and the study of consciousness, is that we assume a shared understanding of many everyday concepts, such as `emotion', `feeling', `pleasure', `pain', `desire', `awareness', etc. Unfortunately, these concepts are inherently very complex, ill-defined, and used with different meanings by different people. Moreover this goes unnoticed, so that people think they understand what they are referring to even when their understanding is very unclear. Consequently there is much discussion that is inherently vague, often at cross-purposes, and with apparent disagreements that arise out of people unwittingly talking about different things. We need a framework which explains how there can be all the diverse phenomena that different people refer to when they talk about emotions and other affective states and processes. The conjecture on which this paper is based is that adult humans have a type of information-processing architecture, with components whi...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 367,
    "label": 0,
    "text": "Standardizing Agent Communication An Agent Communication Language (ACL) is a collection of  speech-act-like message types, with agreed-upon semantics, which facilitate  the knowledge and information exchange between software agents.",
    "neighbors": [
      441,
      663,
      1067
    ],
    "mask": "Train"
  },
  {
    "node_id": 368,
    "label": 3,
    "text": "Maintaining Temporal Views Over Non-Temporal Information Sources For Data Warehousing An important use of data warehousing is to provide temporal views over the history of source  data that may itself be non-temporal. While recent work in view maintenance is applicable to  data warehousing, only non-temporal views have been considered. In this paper, we introduce  a framework for maintaining temporal views over non-temporal information sources in a data  warehousing environment. We describe an architecture for the temporal data warehouse that  automatically maintains temporal views over non-temporal source relations, and allows users  to ask temporal queries using these views. Because of the dimension of time, a materialized  temporal view may need to be updated not only when source relations change, but also as time  advances. We present incremental techniques to maintain temporal views for both cases, and  outline the implementation of our approach in the WHIPS warehousing prototype at Stanford.  1 Introduction  A data warehouse is a repository for efficient querying ...",
    "neighbors": [
      795
    ],
    "mask": "Train"
  },
  {
    "node_id": 369,
    "label": 1,
    "text": "Active Markov Localization for Mobile Robots Localization is the problem of determining the position of a mobile robot from sensor data. Most existing localization approaches are passive, i.e., they do not exploit the opportunity to control the robot's effectors during localization. This paper proposes an active localization approach. The approach is based on Markov localization and provides rational criteria for (1) setting the robot's motion direction (exploration), and (2) determining the pointing direction of the sensors so as to most efficiently localize the robot. Furthermore, it is able to deal with noisy sensors and approximative world models. The appropriateness of our approach is demonstrated empirically using a mobile robot in a structured office environment. Key words: Robot Position Estimation, Autonomous Service Robots 1 Introduction To navigate reliably in indoor environments, a mobile robot must know where it is. Over the last few years, there has been a tremendous scientific interest in algorithms for estimating ...",
    "neighbors": [
      295,
      899,
      1194
    ],
    "mask": "Test"
  },
  {
    "node_id": 370,
    "label": 5,
    "text": "Three New Algorithms for Projective Bundle Adjustment with Minimum Parameters Bundle adjustment is a technique used to compute the maximum likelihood estimate of structure and motion from image feature correspondences. It practice, large non-linear systems have to be solved, most of the time using an iterative optimization process starting from a sub-optimal solution obtained by using linear methods. The behaviour, in terms of convergence, and the computational cost of this process depend on the parameterization used to represent the problem, i.e. of structure and motion.",
    "neighbors": [
      623
    ],
    "mask": "Test"
  },
  {
    "node_id": 371,
    "label": 0,
    "text": "Non-Supervised Sensory-Motor Agents Learning This text discusses a proposal for creation and destruction of neurons based on the sensory-motor activity. This model, called sensory-motor schema, is used to define a sensory-motor agent as a collection of activity schemata. The activity schema permits a useful distribution of neurons in a conceptual space, creating concepts based on action and sensation. Such approach is inspired in the theory of the Swiss psychologist and epistemologist Jean Piaget, and intends to make explicit the account of the processes of continuous interaction between sensory-motor agents and their environments when agents are producing cognitive structures. 1. Introduction  The notion of an autonomous agent plays a central role in contemporaneous research on Artificial Intelligence [3]. Cognitive agents are based on symbolic processing mechanisms. Reactive agents are based on alternative computational mechanisms like neural networks, analogic processing, etc. The alternative approach using autonomous agents b...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 372,
    "label": 3,
    "text": "Simultaneous Proxy Evaluation The Simultaneous Proxy Evaluation (SPE) architecture is designed to evaluate multiple web proxies in parallel using object requests which are duplicated and passed to each proxy. The SPE architecture reduces problems of unrealistic test environments, dated and/or inappropriate workloads, and is additionally applicable to contentbased prefetching proxies. It is intended to measure byte and object hit rates, client-perceived latencies, and cache consistency. We characterize a space of proxy evaluation methodologies and place this architecture within it. 1 Introduction This paper presents a new architecture for the evaluation of proxy caches. Initially, it grew out of research in techniques for prefetching in web caches. In particular, we found that existing mechanisms for the evaluation of proxy caches were not well suited to prefetching systems. Objective evaluation is paramount to all research, whether applied or academic. Since this is certainly relevant when exploring various approac...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 373,
    "label": 3,
    "text": "Exploiting Versions for Handling Updates in Broadcast Disks Recently, broadcasting has attracted considerable attention as a means of disseminating information to large client populations in both wired and wireless settings. In this paper, we exploit versions to increase the concurrency of client transactions in the presence of updates. We consider three alternative mediums for storing versions: (a) the air: older versions are broadcast along with current data, (b) the client's local cache: older versions are maintained in cache, and (c) a local database or warehouse at the client: part of the server's database is maintained at the client in the form of a multiversion materialized view. The proposed techniques are scalable in that they provide consistency without any direct communication from clients to the server. Performance results show that the overhead of maintaining versions can be kept low, while providing a considerable increase in concurrency.   1 Introduction  While traditionally data are delivered from servers to clients on demand, a...",
    "neighbors": [
      174,
      354,
      470
    ],
    "mask": "Train"
  },
  {
    "node_id": 374,
    "label": 3,
    "text": "The Persistent Cache: Improving OID Indexing in Temporal Object-Oriented Database Systems In a temporal OODB, an OID index (OIDX) is needed to map from OID to the physical location of the object. In a transaction time temporal OODB, the OIDX should also index the object versions. In this case, the index entries, which we call object descriptors (OD), also include the commit timestamp of the transaction that created the object version. The OIDX in a non-temporal OODB only needs to be updated when an object is created, but in a temporal OODB, the OIDX have to be updated every time an object is updated. We have in a previous study shown that this can be a potential bottleneck, and in this report, we present the Persistent Cache (PCache), a novel approach which reduces the index update and lookup costs in temporal OODBs. In this report, we develop a cost model for the PCache, and use this to show that the use of a PCache can reduce the average access cost to only a fraction of the cost when not using the PCache. Even though the primary context of this report is OID indexing in ...",
    "neighbors": [
      630
    ],
    "mask": "Train"
  },
  {
    "node_id": 375,
    "label": 0,
    "text": "A Knowledge-Based Approach for Designing Intelligent Team Training Systems This paper presents a knowledge approach to designing team training systems using intelligent agents. We envision a computer-based training system in which teams are trained by putting them through scenarios, which allow them to practice their team skills. There are two important roles that intelligent agents can play; these are virtual team members, and tutors. To carry out these functions, these agents must be equipped with an understanding of the task domain, the team structure, the selected decision-making process and their beliefs about other team members' mental states. Even though existing agent teamwork models incorporate many of the elements listed above, they have not focused on analyzing information needs of team members to support proactive agent interactions. To encode the team knowledge, we have developed a representation language, based on the BDI model, called MALLET. A Petri Net model of an individual agent's plans and information needs can be derived from the role des...",
    "neighbors": [
      686,
      775,
      964
    ],
    "mask": "Train"
  },
  {
    "node_id": 376,
    "label": 4,
    "text": "Improving Interaction with Virtual Environments Introduction  Virtual environments (VEs) provide a computer-based interface to a real-life or abstract space, using 3D graphics and 3D interaction techniques. VEs represent a novel interface style which offers new possibilities and challenges to human-computer interface design. However, studies of the design of VEs (Kaur et al., 1996) show that designers lack a coherent approach to design, especially interaction design. Designers appear to be pre-occupied with difficult technical issues and think little about supporting user interaction. However, major interaction problems have been found with current VEs, such as disorientation, perceptual misjudgements and difficulty finding and understanding available interactions (McGovern, 1993; COVEN, 1997). These common problems have been known to result in user frustration and a low usability and acceptability for the VE (Kaur et al., 1996; Miller 1994). Guidance is needed on interaction design for VEs to avoid such usability problems.",
    "neighbors": [
      678,
      1077,
      1105
    ],
    "mask": "Train"
  },
  {
    "node_id": 377,
    "label": 5,
    "text": "From Markov Random Fields to Associative Memories and Back: Spin-Glass Markov Random Fields this paper we propose a fully connected energy function for Markov Random Field (MRF) modeling which is inspired by Spin-Glass Theory (SGT). Two major tasks in MRF modeling are how to define the neighborhood system for irregular sites and how to choose the energy function for a proper encoding of constraints. The proposed energy function offers two major advantages that makes it possible to avoid MRF modeling problems in the case of irregular sites. First, full connectivity makes the neighborhood definition irrelevant, and second, the energy function is defined independently of the considered application. A basic assumption in SGT is the infinite dimension of the configuration space in which the energy is defined; the choice of a particular energy function, which depends on the scalar product between configurations, allows us to use a kernel function in the energy formulation; this solves the problem of high dimensionality and makes it possible to use SGT results in an MRF framework. We call this new model Spin Glass- - Markov Random Field (SG-MRF). Experiments on textures and objects database show the correctness and effectiveness of the proposed model",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 378,
    "label": 5,
    "text": "HICAP: An Interactive Case-Based Planning Architecture and its Application to Noncombatant Evacuation Operations This paper describes HICAP (Hierarchical Interactive Case-based Architecture for Planning), a general purpose planning architecture that we have developed and applied to assist military commanders and their staff with planning NEOs (Noncombatant Evacuation Operations). HICAP integrates a hierarchical task editor, HTE, with a conversational case-based planning tool, NaCoDAE/HTN. In this application, HTE maintains an agenda of tactical planning tasks that, according to the guidelines indicated by military doctrine, must be addressed in a NEO plan. It also supports several bookkeeping tasks, which are crucial for large-scale planning tasks that differ greatly among different NEO operations. Military planning personnel select a task to decompose from HTE and then use NaCoDAE/HTN to interactively refine it into an operational plan by selecting and applying cases, which represent task decompositions from previous NEO operations. Thus, HICAP helps commanders by using previous experience to fo...",
    "neighbors": [
      924
    ],
    "mask": "Train"
  },
  {
    "node_id": 379,
    "label": 2,
    "text": "Learning to Construct Knowledge Bases from the World Wide Web The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach is to develop a trainable information extraction system that takes two inputs. The first is an ontology that defines the classes (e.g., company, person, employee, product) andrelations (e.g., employed by, produced by) of interest when creating the knowledge base. The second is a set of training data consisting of labeled regions of hypertext that represent instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This article describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system that has created a knowledge base describing university people, courses, and research projects.",
    "neighbors": [
      133,
      322,
      609,
      643,
      855,
      875,
      956,
      1122,
      1178
    ],
    "mask": "Train"
  },
  {
    "node_id": 380,
    "label": 0,
    "text": "Algorithms for Optimizing Leveled Commitment Contracts In automated negotiation systems consisting of self-interested agents, contracts have traditionally been binding. Leveled commitment contracts\u2014i.e. contracts where each party can decommit by paying a predetermined penalty were recently shown to improve Pareto efficiency even if agents rationally decommit in Nash equilibrium using inflated thresholds on how good their outside offers must be before they decommit. This paper operationalizes the four leveled commitment contracting protocols by presenting algorithms for using them. Algorithms are presented for computing the Nash equilibrium decommitting thresholds and decommitting probabilities given the contract price and the penalties. Existence and uniqueness of the equilibrium are analyzed. Algorithms are also presented for optimizing the contract itself (price and penalties). Existence and uniqueness of the optimum are analyzed. Using the algorithms we offer a contract optimization service on the web as part of ('Mediator, our next generation electronic commerce server. Finally, the algorithms are generalized to contracts involving more than two agents. 1",
    "neighbors": [
      589
    ],
    "mask": "Validation"
  },
  {
    "node_id": 381,
    "label": 3,
    "text": "ADOME: An Advanced Object Modelling Environment ADOME, ADvanced Object Modeling Environment, an approach to integrating data and knowledge management based on object-oriented technology, is presented. Next generation information systems will require more flexible data modelling capabilities than those provided by current object-oriented DBMSs. In particular, integration of data and knowledge management capabilities will become increasingly important. In this context, ADOME provides versatile role facilities that serve as \"dynamic binders\" between data objects and production rules, thereby facilitating flexible data and knowledge management integration. A prototype that implements this mechanism and the associated operators has been constructed on top of a commercial object-oriented DBMS and a rule base system.  Index Terms: Object modeling, knowledge semantics, dynamic roles, object-oriented databases, nextgeneration information systems 1 Introduction  Increasingly, organizations require more intelligent information management. In o...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 382,
    "label": 2,
    "text": "Information Retrieval on the World Wide Web and Active Logic: A Survey and Problem Definition As more information becomes available on the World Wide Web (there are currently over 4 billion pages covering most areas of human endeavor), it becomes more difficult to provide effective search tools for information access. Today, people access web information through two main kinds of search interfaces: Browsers (clicking and following hyperlinks) and Query Engines (queries in the form of a set of keywords showing the topic of interest). The first process is tentative and time consuming and the second may not satisfy the user because of many inaccurate and irrelevant results. Better support is needed for expressing one's information need and returning high quality search results by web search tools. There appears to be a need for systems that do reasoning under uncertainty and are flexible enough to recover from the contradictions, inconsistencies, and irregularities that such reasoning involves.",
    "neighbors": [
      224,
      281,
      453,
      457,
      774,
      933,
      1031,
      1059,
      1183
    ],
    "mask": "Validation"
  },
  {
    "node_id": 383,
    "label": 0,
    "text": "Antisocial Agents and Vickrey Auctions In recent years auctions have become more and more important in  the field of multiagent systems as useful mechanisms for resource allocation and  task assignment. In many cases the Vickrey (second-price sealed-bid) auction  is used as a protocol that prescribes how the individual agents have to interact  in order to come to an agreement. We show that the Vickrey auction, despite  its theoretical benefits, is inappropriate if \"antisocial\" agents participate in the  auction process. More specifically, an antisocial attitude for economic agents that  makes reducing the profit of competitors their main goal besides maximizing their  own profit is introduced. Under this novel condition, agents need to deviate from  the dominant truth-telling strategy. This paper presents a strategy for bidders in  repeated Vickrey auctions who are intending to inflict losses to fellow agents in  order to be more successful, not in absolute measures, but relatively to the group  of bidders. The strategy is evaluated in a simple task allocation scenario.",
    "neighbors": [
      624,
      667
    ],
    "mask": "Train"
  },
  {
    "node_id": 384,
    "label": 4,
    "text": "Practical Guidelines for the Readability of IT-architecture Diagrams This paper presents the work done to establish guidelines for the creation of readable IT-architecture diagrams and gives some examples of guidelines and some examples of improved diagrams. These guidelines are meant to assist practicing IT-architects in preparing the diagrams to communicate their architectures to the various stakeholders. Diagramming has always been important in information technology (IT), but the recent interest in ITarchitecture, the widespread use of software and developments in electronic communication, make it necessary to again look at the rt of making diagrams'for this particular class and its users. The guidelines indicate how various visual attributes, like hierarchy, layout, color, form, graphics, etc. can contribute to the readability of IT-architecture diagrams. The emphasis is on the outward appearance of diagrams. Some additional support is given for the thinking/reasoning processes while designing or using a set of diagrams and an attempt is made to arrive at a rationale of these guidelines. An evaluation process has been performed with three groups of practicing IT-architects. The outcome of this evaluation is presented. This work is part of a more comprehensive research project on \"Visualisation of IT- architecture\".",
    "neighbors": [
      423
    ],
    "mask": "Validation"
  },
  {
    "node_id": 385,
    "label": 4,
    "text": "Gaia: Enabling Active Spaces Ubiquitous computing promotes physical spaces with hundreds of specialized embedded devices that increase our productivity, alleviate some specific everyday tasks and provide new ways of interacting with the computational environment. Personal computers lose the focus of attention due to the fact that the computational environment is spread across the physical space. Therefore, the users' view of the computational environment is finally extended beyond the physical limits of the computer. Physical spaces become computer systems, or in other terms, Active Spaces. However, these Active Spaces require novel system software capable of seamlessly coordinating their hidden complexity. Our goal is to extend the model provided by current computer systems to allow interaction with physical spaces and their contained entities (physical and virtual) by means of a single abstraction called Active Space.  1. Introduction  Ubiquitous computing promotes the proliferation of embedded devices specializ...",
    "neighbors": [
      194
    ],
    "mask": "Train"
  },
  {
    "node_id": 386,
    "label": 1,
    "text": "Fuzzy Concepts and Formal Methods: A Fuzzy Logic Toolkit for Z It has been recognised that formal methods are useful as a modelling tool in requirements engineering. Specification languages such as Z permit the precise and unambiguous modelling of system properties and behaviour. However some system problems, particularly those drawn from the IS problem domain, may be difficult to model in crisp or precise terms. It may also be desirable that formal modelling should commence as early as possible, even when our understanding of parts of the problem domain is only approximate. This paper suggests fuzzy set theory as a possible representation scheme for this imprecision or approximation. We provide a summary of a toolkit that defines the operators, measures and modifiers necessary for the manipulation of fuzzy sets and relations. We also provide some examples of the laws which establishes an isomorphism between the extended notation presented here and conventional Z when applied to boolean sets and relations.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 387,
    "label": 5,
    "text": "Choosing Good Distance Metrics and Local Planners for Probabilistic Roadmap Methods This paper presents a comparative evaluation of different distance metrics and local planners within the context of probabilistic roadmap methods for motion planning. Both C-space and Workspace distance metrics and local planners are considered. The study concentrates on cluttered three-dimensional Workspaces typical, e.g., of mechanical designs. Our results include recommendations for selecting appropriate combinations of distance metrics and local planners for use in motion planning methods, particularly probabilistic roadmap methods. Our study of distance metrics showed that the importance of the translational distance increased relative to the rotational distance as the environment become more crowded. We find that each local planner makes some connections than none of the others do \u2014 indicating that better connected roadmaps will be constructed using multiple local planners. We propose a new local planning method we call rotate-at-s that outperforms the common straight-line in C-space method in crowded environments.",
    "neighbors": [
      320,
      446
    ],
    "mask": "Train"
  },
  {
    "node_id": 388,
    "label": 4,
    "text": "Mining Usability Information from Log Files: AMulti-Pronged Approach rooms is configurable by its occupants in how they organize various tools housing their data, documents, and graphics. The TW system provides for synchronous and asynchronous user interactions, but importantly these interactions are in the context of relevant data. The work in this experiment was characterized by several full group meetings (for planning and coordination) interspersed with periods of individual activity (asychronous work) and smaller coordination meetings of two or three team members around the \"hand-off\" of output from a task used as input for another task.  Collected Data  The native version of TW produces a server-based log-file that contains information about the identity of users entering the distributed application, the identity of the rooms through which users navigate, file uploads, and message passing between users. This set of interactions was deemed too rudimentary for capturing the type of data needed for usability analysis. Since the source co",
    "neighbors": [
      45
    ],
    "mask": "Train"
  },
  {
    "node_id": 389,
    "label": 3,
    "text": "Scalable Algorithms for Large-Scale Temporal Aggregation The ability to model time-varying natures is essential to many database applications such as data warehousing and mining. However, the temporal aspects provide many unique characteristics and challenges for query processing and optimization. Among the challenges is computing temporal aggregates, which is complicated by having to compute temporal grouping. In this paper, we introduce a variety of temporal aggregation algorithms that overcome major drawbacks of previous work. First, for small-scale aggregations, both the worst-case and average-case processing time have been improved significantly. Second, for large-scale aggregations, the proposed algorithms can deal with a database that is substantially larger than the size of available memory. Third, the parallel algorithm designed on a shared-nothing architecture achieves scalable performance by delivering nearly linear scale-up and speed-up. The contributions made in this paper are particularly important because the rate of increase ...",
    "neighbors": [
      7,
      706
    ],
    "mask": "Train"
  },
  {
    "node_id": 390,
    "label": 4,
    "text": "Feasibility Discussion of a Collaborative Virtual Environment - FINDING ALTERNATIVE WAYS FOR UNIVERSITY MEMBERS INTERACTION This paper discusses the potential impact and roadmap for the creation of a Collaborative Virtual Environment where all university members can interact in novel ways. Some actual NetLab figures are presented to justify this evolution as feasible. A related project that uses the potential created by the \"laptops for all\" action is a virtual incubator to simulate entrepreneurship bias is presented.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 391,
    "label": 2,
    "text": "Automatic Multi-Lingual Information Extraction Information Extraction (IE) is a burgeoning technique because of the explosion of internet. So far, most of the IE systems are focusing on English text; and most of them are in the supervised learning framework, which requires large amount of human labor; and most of them can only work in narrow domain, which is domain dependent. These systems are difficult to be ported to other languages, other domains because of these inherent shortcomings. Currently, besides western languages like English, there are many other Asian languages which are much di erent from English. In English, words are delimited by white-spaces so computer can easily tokenize the input text string. In many languages like Chinese, Japanese, Thai and Korea, they do not have word boundaries between words. This poses a difficult problem for the information extraction for those languages. In this thesis, we intend to implement a self-contained, language independent automatic IE system. The system is automatic because we are using a unsupervised learning framework in which no labeled data is required for training or a semi-supervised learning framework in which small amount of labeled data and large amount of unlabeled data are used. Specifically, we deal with Chinese and English languages name entity recognition and entity relation extraction, but the system can be easily extended to any other languages and other tasks. We implement an unsupervised Chinese word segmenter, a Chinese POS tagger, and we extend maximum entropy models to incorporate unlabeled data for general information extraction.",
    "neighbors": [
      401,
      437,
      523,
      601,
      609,
      891
    ],
    "mask": "Test"
  },
  {
    "node_id": 392,
    "label": 3,
    "text": "Simplifying Data Access: The Energy Data Collection (EDC) Project The massive amount of statistical and text data available from government agencies has created a set  of daunting challenges to both research and analysis communities. These problems include  heterogeneity, size, distribution, and control of terminology. At the Digital Government Research  Center we are investigating solutions to these key problems. In this paper we focus on  (1) ontological mappings for terminology standardization, (2) data integration across data bases with  high speed query processing, and (3) interfaces for query input and presentation of results. This  collaboration between researchers from Columbia University and the Information Sciences Institute  of the University of Southern California employs technology developed at both locations, in  particular the SENSUS ontology, the SIMS multi-database access planner, the LKB automated  dictionary and terminology analysis system, and others. The pilot application targets gasoline data  from the Bureau of Labor Statistics, the Energy Information Administration of the Department of  Energy, the Census Bureau, and other government agencies.  1",
    "neighbors": [
      506
    ],
    "mask": "Test"
  },
  {
    "node_id": 393,
    "label": 1,
    "text": "Beyond Euclidean Eigenspaces: Bayesian Matching for Visual Recognition We propose a novel technique for direct visual matching of images for the purposes of face  recognition and database search. Speci#cally,we argue in favor of a probabilistic measure of  similarity, in contrast to simpler methods which are based on standard Euclidean L2 norms #e.g.,  template matching# or subspace-restricted norms #e.g., eigenspace matching#. The proposed  similarity measure is based on a Bayesian analysis of image di#erences: we model twomutually  exclusive classes of variation between two facial images: intra-personal #variations in appearance  of the same individual, due to di#erent expressions or lighting# and extra-personal #variations in  appearance due to a di#erence in identity#. The high-dimensional probability density functions  for each respective class are then obtained from training data using an eigenspace density  estimation technique and subsequently used to compute a similarity measure based on the a  posteriori probability of membership in the intra-personal class, which is used to rank matches  in the database. The performance advantage of this probabilistic matching technique over  standard Euclidean nearest-neighbor eigenspace matching is demonstrated using results from  ARPA's 1996 #FERET\" face recognition competition, in which this algorithm was found to be  the top performer.",
    "neighbors": [
      1218
    ],
    "mask": "Validation"
  },
  {
    "node_id": 394,
    "label": 5,
    "text": "The CMUnited-97 Robotic Soccer Team: Perception and Multiagent Control Robotic soccer is a challenging research domain which involves multiple  agents that need to collaborate in an adversarial environment to achieve  specificobjectives. In this paper, we describe CMUnited, the team of small  robotic agents that we developed to enter the RoboCup-97 competition. We  designed and built the robotic agents, devised the appropriate vision algorithm,  and developed and implemented algorithms for strategic collaboration  between the robots in an uncertain and dynamic environment. The robots  can organize themselves in formations, hold specificroles, and pursue their  goals. In game situations, they have demonstrated their collaborative behaviors  on multiple occasions. The robots can also switch roles to maximize  the overall performance of the team. We present an overview of the vision  processing algorithm which successfully tracks multiple moving objects and  predicts trajectories. The paper then focusses on the agent behaviors ranging  from low-level individual behaviors to coordinated, strategic team behaviors.",
    "neighbors": [
      155,
      430,
      927,
      1266
    ],
    "mask": "Validation"
  },
  {
    "node_id": 395,
    "label": 2,
    "text": "Improving Short-Text Classification Using Unlabeled Background Knowledge to Assess Document Similarity We describe a method for improving the classification  of short text strings using a combination  of labeled training data plus a secondary corpus  of unlabeled but related longer documents. We  show that such unlabeled background knowledge  can greatly decrease error rates, particularly if  the number of examples or the size of the strings  in the training set is small. This is particularly  useful when labeling text is a labor-intensive job  and when there is a large amount of information  available about a particular problem on the World  Wide Web. Our approach views the task as one  of information integration using WHIRL, a tool  that combines database functionalities with techniques  from the information-retrieval literature.  1. Introduction  The task of classifying textual data that has been culled from sites on the World Wide Web is both difficult and intensively studied (Cohen & Hirsh, 1998; Joachims, 1998; Nigam et al., 1999). Applications of various machine learning techniqu...",
    "neighbors": [
      609,
      643,
      1144
    ],
    "mask": "Train"
  },
  {
    "node_id": 396,
    "label": 0,
    "text": "Composable Agents for Patient Flow Control - Preliminary Concepts In this article we describe our research efforts in coping with a trade-off that can be often found in the control and optimization of todays business processes. Though centralized control may achieve better results in controlling the system behavior, there are usually social, technical and security constraints on applying centralized control. Distributed control on the other hand may cope with these constraints but also entails suboptimal results and communicational overhead. Our concept of composable agents tries to allow a dynamic and fluent transition between globalization and localization in business process control by adapting to the current real-world system structure. We are currently evaluating this concept in the framework of a patient flow control project at Charit'e Berlin. Todays applications of information technology face at least two major aspects of business settings. The first aspect is the partially or fully automated execution of complex business processes. This enfo...",
    "neighbors": [
      25,
      964
    ],
    "mask": "Train"
  },
  {
    "node_id": 397,
    "label": 3,
    "text": "A Query Calculus for Spatio-Temporal Object Databases The development of any comprehensive proposal for spatio-temporal databases involves significant extensions to many aspects of a non-spatio-temporal architecture. One aspect that has received less attention than most is the development of a query calculus that can be used to provide a semantics for spatio-temporal queries and underpin an effective query optimization and evaluation framework. In this paper, we show how a query calculus for spatiotemporal object databases that builds upon the monoid calculus proposed by Fegaras and Maier for ODMG-compliant database systems can be developed. The paper shows how an extension of the ODMG type system with spatial and temporal types can be accommodated into the monoid approach. It uses several queries over historical (possibly spatial) data to illustrate how, by mapping them into monoid comprehensions, the way is open for the application of a logical optimizer based on the normalization algorithm proposed by Fegaras and Maier.",
    "neighbors": [
      59,
      209,
      302,
      481,
      493,
      692
    ],
    "mask": "Train"
  },
  {
    "node_id": 398,
    "label": 0,
    "text": "Making Complex Articulated Agents Dance - An analysis of control methods drawn from robotics, animation, and biology .  We discuss the tradeoffs involved in control of complex articulated agents, and present three implemented controllers for a complex task: a physically-based humanoid torso dancing the Macarena. The three controllers are drawn from animation, biological models, and robotics, and illustrate the issues of joint-space vs. Cartesian space task specification and implementation. We evaluate the controllers along several qualitative and quantitative dimensions, considering naturalness of movement and controller flexibility. Finally, we propose a general combination approach to control, aimed at utilizing the strengths of each alternative within a general framework for addressing complex motor control of articulated agents.  Key words: articulated agent control, motor control, robotics, animation 1. Introduction  Control of humanoid agents, dynamically simulated or physical, is an extremely difficult problem due to the high dimensionality of the control space, i.e., the many degrees of freed...",
    "neighbors": [
      183
    ],
    "mask": "Validation"
  },
  {
    "node_id": 399,
    "label": 3,
    "text": "Segment-Based Approach for Subsequence Searches in Sequence Databases This paper investigates the subsequence searching problem under time warping in sequence databases. Time warping enables to find sequences with similar changing patterns even when they are of different lengths. Our work is motivated by the observation that subsequence searches slow down quadratically as the total length of data sequences increases. To resolve this problem, we propose the SegmentBased Approach for Subsequence Searches (SBASS), which modifies the similarity measure from time warping to piecewise time warping and limits the number of possible subsequences to be compared with a query sequence.  For efficient retrieval of similar subsequences without false dismissal  1  , we extract feature vectors from all data segments exploiting their monotonically changing properties, and build a multi-dimensional index such as R-tree or R  - tree. Using this index, queries are processed with four steps: 1) index filtering, 2) feature filtering, 3) successor filtering, and 4) post-proce...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 400,
    "label": 3,
    "text": "Abduction in Logic Programming This paper is a survey and critical overview of recent work on the extension of Logic Programming to perform Abductive Reasoning (Abductive Logic Programming). We outline the general framework of Abduction and its applications to Knowledge Assimilation and Default Reasoning; and we introduce an argumentation-theoretic approach to the use of abduction as an interpretation for Negation as Failure. We also analyse the links between Abduction and the extension of Logic Programming obtained by adding a form of explicit negation. Finally we discuss the relation between Abduction and Truth Maintenance.  1 Introduction  This paper is a survey and analysis of work on the extension of logic programming to perform abductive reasoning. The purpose of the paper is to provide a critical overview of some of the main research results, in order to develop a common framework for evaluating these results, to identify the main unresolved problems, and to indicate directions for future work. The emphasis i...",
    "neighbors": [
      588,
      771,
      1091
    ],
    "mask": "Train"
  },
  {
    "node_id": 401,
    "label": 2,
    "text": "Scenario Customization for Information Extraction Information Extraction (IE) is an emerging NLP technology, whose function is to process unstructured, natural language text, to locate specific pieces of information, or facts, in the text, and to use these facts to fill a database. IE systems today are commonly based on pattern matching. The core IE engine uses a cascade of sets of patterns of increasing linguistic complexity. Each pattern consists of a regular expression and an associated mapping from syntactic to logical form. The pattern sets are customized for each new topic, as defined by the set of facts to be extracted. Construction of a pattern base for a new topic is recognized as a time-consuming and expensive process---a principal roadblock to wider use of IE technology in the large. An e#ective pattern base must be precise and have wide coverage. This thesis addresses the portability probl...",
    "neighbors": [
      391,
      855
    ],
    "mask": "Train"
  },
  {
    "node_id": 402,
    "label": 3,
    "text": "Algorithms for Temporal Query Operators in XML Databases The contents of an XML database or XML/Web data warehouse is seldom static. New documents  are created, documents are deleted, and more important: documents are updated. In many  cases, we want to be able to search in historical versions, retrieve documents valid at a certain  time, query changes to documents, etc. This can be supported by extending the system with temporal  database features. In this paper we describe the new query operators needed in order to  support an XML query language which supports temporal operations. We also describe the algorithms  which can make efficient implementation of these query operators possible.  Keywords: XML, temporal databases, query processing  1",
    "neighbors": [
      705,
      912,
      1025,
      1069
    ],
    "mask": "Train"
  },
  {
    "node_id": 403,
    "label": 2,
    "text": "Probabilistic Hierarchical Clustering with Labeled and Unlabeled Data . This paper presents hierarchical probabilistic clustering methods for unsupervised and supervised learning in datamining applications, where supervised learning is performed using both labeled and unlabeled examples. The probabilistic clustering is based on the previously suggested Generalizable Gaussian Mixture model and is extended using a modified Expectation Maximization procedure for learning with both unlabeled and labeled examples. The proposed hierarchical scheme is agglomerative and based on probabilistic similarity measures. Here, we compare a L 2 dissimilarity measure, error confusion similarity, and accumulated posterior cluster probability measure. The unsupervised and supervised schemes are successfully tested on artificially data and for e-mails segmentation.  1",
    "neighbors": [
      609,
      1153
    ],
    "mask": "Train"
  },
  {
    "node_id": 404,
    "label": 2,
    "text": "Theme-Based Retrieval of Web News Efficient information retrieval of highly dynamic information, such as Web news, is a complex task. As a result, search and retrieval environments for continuously updated news from other sources than the largest media conglomerates are almost absent on the Internet. Global search engines do not index or classify news information from smaller network communities. To address this problem, I developed NewsSearch, a news information management environment designed to improve retrieval efficiency of online news for the smaller networked communities.  NewsSearch search achieves its goal through a combination of techniques:  . Multiple indexing queues, defining multiple gathering schedules, to deal with different publication periodicities.  . Information Retrieval techniques to news, in order to classify them into a pre-defined set of themes.  . Support Vector Machines, which proved to be a fast and reliable classification technique.  NewsSearch proved to be a scalable solution with acceptable storage needs even while managing a fairly large collection of daily publications. A combination of fine tuning of training strategies, noise filtering of Web news documents and multiple classifications, enable NewsSearch to achieve a classification accuracy of 95%.  ACKNOWLEDGEMENTS  This work was supported in part by the PRAXIS project ARIADNE (Pblico Digital --  Praxis XXI, Medida 3.1b) and project SAGRES (Praxis/P/TIT/1676/95).  TABLE OF CONTENTS  CHAPTER I",
    "neighbors": [
      502,
      931,
      1003
    ],
    "mask": "Train"
  },
  {
    "node_id": 405,
    "label": 0,
    "text": "Towards Flexible Multi-Agent Decision-Making Under Time Pressure Abstract \u2014 Autonomous agents need considerable computational resources to perform rational decision-making. These demands are even more severe when other agents are present in the environment. In these settings, the quality of an agent\u2019s alternative behaviors depends not only on the state of the environment, but also on the actions of other agents, which in turn depend on the others \u2019 beliefs about the world, their preferences, and further on the other agents\u2019 beliefs about others, and so on. The complexity becomes prohibitive when large number of agents are present and when decisions have to be made under time pressure. In this paper we investigate strategies intended to tame the computational burden by using off-line computation in conjunction with on-line reasoning. We investigate two approaches. First, we use rules compiled off-line to constrain alternative actions considered during on-line reasoning. This method minimizes overhead, but is not sensitive to changes in realtime demands of the situation at hand. Second, we use performance profiles computed off-line and the notion of urgency (i.e., the value of time) computed on-line to choose the amount of information to be included during on-line deliberation. This method can adjust to various levels of real-time demands, but incurs some overhead associated with iterative deepening. We test our framework with experiments in a simulated anti-air defense domain. The experiments show that both procedures are effective in reducing computation time while offering good performance under time pressure.",
    "neighbors": [
      277,
      964
    ],
    "mask": "Train"
  },
  {
    "node_id": 406,
    "label": 0,
    "text": "Generating and Using State Spaces of Object-Oriented Petri Nets : The article discusses the notion of state spaces of object-oriented Petri nets associated to the tool called PNtalk and the role of identifiers of dynamically appearing and disappearing instances within these state spaces. Methods of working with identifiers based on sophisticated naming rules and mechanisms for abstracting names are described and compared. Some optimizations of state space generating algorithms for the context of object-oriented Petri nets are briefly mentioned, as well.  Key Words: Petri nets, object-orientation, state spaces, formal analysis and verification 1 Introduction  Methods of formal analysis and verification has been developed as an alternative to simulation approaches of examining properties of complex systems. Although we are not always able to fully verify the behaviour of a system, even partial analysis or verification can reveal some errors which tend to be different from the ones found by simulation due to the different nature of formal analysis and...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 407,
    "label": 2,
    "text": "Using Labeled and Unlabeled Data to Learn Drifting Concepts For many learning tasks, where data is collected  over an extended period of time, one has to cope  two problems. The distribution underlying the data  is likely to change and only little labeled training  data is available at each point in time. A typical  example is information filtering, i. e. the adaptive  classification of documents with respect to a particular  user interest. Both the interest of the user  and the document content change over time. A filtering  system should be able to adapt to such concept  changes. Since users often give little feedback,  a filtering system should also be able to achieve a  good performance, even if only few labeled training  examples are provided. This paper proposes a  method to recognize and handle concept changes  with support vector machines and to use unlabeled  data to reduce the need for labeled data. The  method maintains windows on the training data,  whose size is automatically adjusted so that the estimated  generalization error is minimized. The approach  is both theoretically well-founded as well  as effective and efficient in practice. Since it does  not require complicated parameterization, it is simpler  to use and more robust than comparable heuristics.  Experiments with simulated concept drift scenarios  based on real-world text data compare the  new method with other window management approaches  and show that it can effectively select an  appropriate window size in a robust way. In order  to achieve an acceptable performance with fewer  labeled training examples, the proposed method exploits  unlabeled examples in a transductive way.  1",
    "neighbors": [
      609,
      865,
      1133,
      1153
    ],
    "mask": "Train"
  },
  {
    "node_id": 408,
    "label": 1,
    "text": "Neuro-Mimetic Navigation Systems: A Computational Model of the Rat Hippocampus : We propose a bio-inspired approach to autonomous navigation  based on some of the components that rats use for navigation. A spatial  model of the environment is constructed by unsupervised Hebbian learning.  The representation consists of a population of localized overlapping place  elds, modeling place cell activity in the rat Hippocampus. Place elds  are established by extracting spatio-temporal properties of the environment  from visual sensory inputs. Visual ambiguities are resolved by means of  path integration. Reinforcement learning is applied to use place cell activity  for goal-oriented navigation. Experimental results obtained with a mobile  Khepera robot are presented.  Keywords: Autonomous robots, hippocampus, place elds, unsupervised  learning, reinforcement learning, population vector coding, path integration.  1. Introduction  The complexity of the autonomous navigation task is inherent in the concept of autonomy: Ideally, an autonomous agent should have a completely ...",
    "neighbors": [
      997
    ],
    "mask": "Train"
  },
  {
    "node_id": 409,
    "label": 1,
    "text": "Markov Techniques for Object Localization With Force-Controlled Robots This paper deals with object localization with forcecontrolled robots in the Bayesian framework [1]. It describes a method based on Markov Localization techniques with a Monte Carlo implementation applied for solving 3D (6 degrees of freedom) global localization problems with force-controlled robots. The approach was successfully applied to problems such as the recursive localization of a box by a robot manipulator.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 410,
    "label": 2,
    "text": "Web Document Clustering: A Feasibility Demonstration Abstract Users of Web search engines are often forced to sift through the long ordered list of document \u201csnippets\u201d returned by the engines. The IR community has explored document clustering as an alternative method of organizing retrieval results, but clustering has yet to be deployed on the major search engines. The paper articulates the unique requirements of Web document clustering and reports on the first evaluation of clustering methods in this domain. A key requirement is that the methods create their clusters based on the short snippets returned by Web search engines. Surprisingly, we find that clusters based on snippets are almost as good as clusters created using the full text of Web documents. To satisfy the stringent requirements of the Web domain, we introduce an incremental, linear time (in the document collection size) algorithm called Suffix Tree Clustering (STC). which creates clusters based on phrases shared between documents. We show that STC is faster than standard clustering methods in this domain, and argue that Web document clustering via STC is both feasible and potentially beneficial. 1",
    "neighbors": [
      84,
      235,
      447,
      851,
      893,
      947,
      1005,
      1247
    ],
    "mask": "Train"
  },
  {
    "node_id": 411,
    "label": 2,
    "text": "Discovering Informative Content Blocks from Web Documents In this paper, we propose a new approach to discover informative contents from a set of tabular documents (or Web pages) of a Web site. Our system, InfoDiscoverer, first partitions a page into several content blocks according to HTML tag <TABLE> in a Web page. Based on the occurrence of the features (terms) in the set of pages, it calculates entropy value of each feature. According to the entropy value of each feature in a content block, the entropy value of the block is defined. By analyzing the information measure, we propose a method to dynamically select the entropy-threshold that partitions blocks into either informative or redundant. Informative content blocks are distinguished parts of the page, whereas redundant content blocks are common parts. Based on the answer set generated from 13 manually tagged news Web sites with a total of 26,518 Web pages, experiments show that both recall and precision rates are greater than 0.956. That is, using the approach, informative blocks (news articles) of these sites can be automatically separated from semantically redundant contents such as advertisements, banners, navigation panels, news categories, etc. By adopting InfoDiscoverer as the preprocessor of information retrieval and extraction applications, the retrieval and extracting precision will be increased, and the indexing size and extracting complexity will also be reduced.",
    "neighbors": [
      112,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 412,
    "label": 2,
    "text": "RoadRunner: Towards Automatic Data Extraction from Large Web Sites The paper investigates techniques for extracting data from HTML sites through the use of automatically generated wrappers. To automate the wrapper generation and the data extraction process, the paper develops a novel technique to compare HTML pages and generate a wrapper based on their similarities and differences. Experimental results on real-life data-intensive Web sites confirm the feasibility of the approach. 1",
    "neighbors": [
      570,
      612,
      855,
      1215,
      1232
    ],
    "mask": "Train"
  },
  {
    "node_id": 413,
    "label": 3,
    "text": "The Diagnosis Frontend of the dlv System This paper presents the Diagnosis Frontend of dlv, which is a knowledge representation system under development at the Technische Universit\u00e4t Wien. The kernel language of the system is an extension of disjunctive logic programming (DLP) by integrity constraints; it offers frontends to several advanced knowledge representation formalisms. The formal model of diagnosis employed in the frontend includes both abductive diagnosis (over DLP theories) and consistency-based diagnosis. For each of the two diagnosis modalities, generic diagnoses, single error diagnoses, and subset minimal diagnoses are considered. We illustrate the use of the frontend by showing the dlv encodings of several diagnosis problems. Thereafter, we discuss implementation issues. Diagnostic reasoning is implemented on the dlv engine through suitable translations of diagnostic problems into disjunctive logic programs, such that their stable models correspond to diagnoses. For the six kinds of diagnostic reasoning problems emerging from above, such reductions are provided",
    "neighbors": [
      632,
      812
    ],
    "mask": "Train"
  },
  {
    "node_id": 414,
    "label": 1,
    "text": "Integrating Case Based Reasoning and Tabu Search for Solving Optimisation Problems Tabu search is an established heuristic optimisation technique for problems where exact algorithms are not available. It belongs to the same family as simulated annealing or genetic algorithms. It extends the basic iterative improvement scheme by adding control learning. A technique of this kind, intensification, captures experience established on a frequency-based analysis of past search. Experience is reused while the same optimisation process is going on in order to guide search to better solutions.  In this paper, we introduce a case-based reasoning approach for control learning in tabu search. Search experience concerns operator selection and is represented by cases. The aim of case reuse is to improve conflict resolution. While the proposed method is domain independent, we present its application to the NPhard uncapacitated facility location problem. Experimental results show that adding our approach to a basic tabu search optimisation significantly improves solution quality on t...",
    "neighbors": [
      611
    ],
    "mask": "Test"
  },
  {
    "node_id": 415,
    "label": 0,
    "text": "Information agents on the move: A survey on load-balancing with mobile agents Information agents process and integrate heterogeneous, distributed information. To achieve this task efficiently, some researchers promote the idea of mobile information agents [13, 53, 44, 20, 10], which migrate between a user's host and other hosts in the network. We outline the concepts behind mobile information agents and give a survey on load balancing, which aims to optimise distributed information processing.",
    "neighbors": [
      465,
      1038
    ],
    "mask": "Train"
  },
  {
    "node_id": 416,
    "label": 0,
    "text": "Formal ReSpecT Logic tuple centres have s own that logic-ba d languages can be e#ectively exploited not only for building individual agents and enabling interagent communication in multi-agent ssG ms butals for  ruli ng inter-agent communications as to builds cial behaviours In this paper, we formally define the notion of logic tuple centre as well as the operationals emantics of the logic-bas d language ReSpecT for the behaviours pecification of logic tuple centres . For this purpos e, we exploit a generals emantic framework for as ynchronous dis tributeds ys tems allowing a coordination medium to be formally denoted in as eparate and independent way with res pect to the whole coordinateds ys tem. This s hows that a logic-bas ed coordination medium does not limit agents and coordination languages to be logic-bas ed, but may ins  tead enable agents of di#erents orts and technologies to be combined and coordinated in an e#ective way by exploiting a logic-bas ed approach. 1 Coordinationm edia form ulti...",
    "neighbors": [
      47,
      119
    ],
    "mask": "Train"
  },
  {
    "node_id": 417,
    "label": 5,
    "text": "Second Order Sufficient Conditions for Optimal Control Problems with Free Final Time: The Riccati Approach . Second order sufficient conditions (SSC) for control problems with control--state constraints and free final time are presented. Instead of deriving such SSC de initio, the control problem with free final time is tranformed into an augmented control problem with fixed final time for which well-known SSC exist. SSC are then expressed as a condition on the positive definiteness of the second variation. A convenient numerical tool for verifying this condition is based on the Riccati approach where one has to find a bounded solution of an associated Riccati equation satisfying specific boundary conditions. The augmented Riccati equations for the augmented control problem are derived and their modifications on the boundary of the control--state constraint are discussed. Two numerical examples, (1) the classical Earth-Mars orbit transfer in minimal time, (2) the Rayleigh problem in electrical engineering, demonstrate that the Riccati equation approach provides a viable numerical test of SS...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 418,
    "label": 1,
    "text": "Rule Discovery with a Parallel Genetic Algorithm An important issue in data mining is scalability  with respect to the size of the dataset being  mined. In the paper we address this issue by  presenting a parallel GA for rule discovery. This  algorithm exploits both data parallelism, by  distributing the data being mined across all  available processors, and control parallelism, by  distributing the population of individuals across  all available processors.  1",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 419,
    "label": 3,
    "text": "WSQ/DSQ: A Practical Approach for Combined Querying of Databases and the Web www-db.stanford.edu We present WSQ/DSQ (pronounced \u201cwisk-disk\u201d), a new approach for combining the query facilities of traditional databases with existing search engines on the Web. WSQ, for Web-Supported (Database) Queries, leverages results from Web searches to enhance SQL queries over a relational database. DSQ, for Database-Supported (Web) Queries, uses information stored in the database to enhance and explain Web searches. This paper focuses primarily on WSQ, describing a simple, low-overhead way to support WSQ in a relational DBMS, and demonstrating the utility of WSQ with a number of interesting queries and results. The queries supported by WSQ are enabled by two virtual tables, whose tuples represent Web search results generated dynamically during query execution. WSQ query execution may involve many high-latency calls to one or more search engines, during which the query processor is idle. We present a lightweight technique called asynchronous iteration that can be integrated easily into a standard sequential query processor to enable concurrency between query processing and multiple Web search requests. Asynchronous iteration has broader applications than WSQ alone, and it opens up many interesting query optimization issues. We have developed a prototype implementation of WSQ by extending a DBMS with virtual tables and asynchronous iteration; performance results are reported. 1",
    "neighbors": [
      218,
      876,
      879,
      998
    ],
    "mask": "Train"
  },
  {
    "node_id": 420,
    "label": 3,
    "text": "Integrating Keyword Search into XML Query Processing Due to the popularity of the XML data format, several query languages for XML have been proposed, specially devised to handle data whose structure is unknown, loose, or absent. While these languages are rich enough to allow for querying the content and structure of an XML document, a varying or unknown structure can make formulating queries a very difficult task. We propose an extension to XML query languages that enables keyword search at the granularity of XML elements, that helps novice users formulate queries, and also yields new optimization opportunities for the query processor. We present an implementation of this extension on top of a commercial RDBMS; we then discuss implementation choices and performance results.  Keywords  XML query processing, full-text index  1 Introduction  There is no doubt that XML is rapidly becoming one of the most important data formats. It is already used for scientific data (e.g., DNA sequences), in linguistics (e.g., the Treebank database at the U...",
    "neighbors": [
      218,
      488,
      634,
      876
    ],
    "mask": "Train"
  },
  {
    "node_id": 421,
    "label": 0,
    "text": "On the Emergence of Macro Spatial Structures in Dissipative Cellular Automata, and its Implications for Agent-based Distributed Computing This paper describes the peculiar behavior observed in a class of cellular automata that we have defined as \"dissipative\", i.e., cellular automata that are \"open\" and makes it possible for the environment to influence the evolution of the automata. Peculiar in the dynamic evolution of this class of cellular automata is that stable macro-level spatial structures emerge from local interactions among cells, a behavior that does not emerge when the cellular automaton is \"closed\", i.e., when the state of a cell is not influenced by the external world. On this basis, the paper discusses the relations of the performed experiments with the area of open distributed computing, and in particular of agent-based distributed computing. The basic intuition is that dissipative cellular automata express characteristics that strongly resembles those of wide-area open distributed systems based on autonomous and situated active components -- as agents are. Accordingly, similar sorts of macrolevel behaviors are likely to emerge and need to be studied, controlled, and possibly fruitfully exploited.",
    "neighbors": [
      246
    ],
    "mask": "Train"
  },
  {
    "node_id": 422,
    "label": 3,
    "text": "Modeling Temporal Consistency in Data Warehouses Real-world changes are generally discovered delayed by computer systems. The typical update patterns for traditional data warehouses on an overnight or even weekly basis enlarge this propagation delay until the information is available to knowledge workers. The main contribution of the paper is the identification of two different temporal characterizations of the information appearing in a data warehouse: one is the classical description of the time instant when a given fact occurred, the other represents the instant when the information has been entered into the system. We present an approach for modeling conceptual time consistency problems and introduce a data model that deals with timely delays and supports knowledge workers to determine what the situation was in the past, knowing only the information available at a given instant of time. 1",
    "neighbors": [
      1243
    ],
    "mask": "Train"
  },
  {
    "node_id": 423,
    "label": 4,
    "text": "Layout Rules for Graphical Web Documents The number of companies, institutions, and individuals competing for attention in the World-Wide Web is growing exponentially. This makes designing informative, easy-to-grasp, and visually appealing documents not only important for userfriendly information presentation, but also the key to success for any information provider. In this paper, we present layout guidelines for textual and graphical, static and dynamic, 2-D and 3-D Web documents which are drawn from fields as diverse as typography, Gestalt psychology, architecture, hypertext authoring, and human-computer interaction. Web documents are classified into five basic types, and our layout rules are applied to each of these. Finally, we show how currently evolving standards (HTML 3.0 for text and still graphics, Java for 2-D animation, and VRML for 3-D worlds) support applying those rules. 1 Introduction  Whenever a new information-conveying technology is invented, it usually takes many years until authors develop new media that ...",
    "neighbors": [
      384
    ],
    "mask": "Validation"
  },
  {
    "node_id": 424,
    "label": 0,
    "text": "Implementing Incremental Code Migration with XML We demonstrate how XML and related technologies can be used for code mobility at any granularity, thus overcoming the restrictions of existing approaches. By not fixing a particular granularity for mobile code, we enable complete programs as well as individual lines of code to be sent across the network. We define the concept of incremental code mobility as the ability to migrate and add, remove, or replace code fragments (i.e., increments) in a remote program. The combination of fine-grained and incremental migration achieves a previously unavailable degree of flexibility. We examine the application of incremental and fine-grained code migration to a variety of domains, including user interface management, application management on mobile thin clients, for example PDAs, and management of distributed documents.  Keywords  Incremental Code Migration, XML Technologies  1 INTRODUCTION  The increasing popularity of Java and the spread of Webbased technologies are contributing to a growing ...",
    "neighbors": [
      757
    ],
    "mask": "Train"
  },
  {
    "node_id": 425,
    "label": 0,
    "text": "How to Avoid Knowing It All Beliefs have been formally modelled in the last decades using doxastic logics. The possible worlds model and its associated Kripke semantics provide an intuitive semantics for these logics, but they seem to commit us to model agents that are logically omniscient (they believe every classical tautology) and perfect reasoners (their beliefs are closed under classical deductive closure). Thus, this model would not be appropriate to model non-ideal agents, that have resource limitations that prevent them from attaining such levels of doxastic competence. This report contains a statement of these problems and a brief survey of some of the most interesting approaches that have been suggested to overcome them. Contents 1 Formal models of belief 3 1.1 Possible worlds and Kripke semantics . . . . . . . . . . . . . . . . . . . . . 3 1.2 Logical omniscience and perfect reasoning . . . . . . . . . . . . . . . . . . 5 2 Avoiding logical omniscience 7 2.1 Syntactic approaches . . . . . . . ...",
    "neighbors": [
      827,
      964
    ],
    "mask": "Train"
  },
  {
    "node_id": 426,
    "label": 2,
    "text": "Data Mining Models as Services on the Internet The goal of this article is to raise a debate on the usefulness of providing data mining models as services on the internet. These services can be provided by anyone with adequate data and expertise and made available on the internet for anyone to use. For instance, Yahoo or Altavista, given their huge categorized document collection, can train a document classifier and provide the model as a service on the internet. This way data mining can be made accessible to a wider audience instead of being limited to people with the data and the expertise. A host of practical problems need to be solved before this idea can be made to work. We identify them and close with an invitation for further debate and investigation.  1.",
    "neighbors": [
      440
    ],
    "mask": "Validation"
  },
  {
    "node_id": 427,
    "label": 2,
    "text": "The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. Among the many applications of this approach are information retrieval and search, topic identification, query disambiguation, focused web crawling, web authoring, and bibliometric analysis.",
    "neighbors": [
      216,
      247,
      722,
      774,
      1017,
      1068
    ],
    "mask": "Train"
  },
  {
    "node_id": 428,
    "label": 1,
    "text": "Mixtures of Linear Subspaces for Face Detection We present two methods using mixtures of linear subspaces for face detection in gray level images. One method uses a mixture of factor analyzers to concurrently perform clustering and, within each cluster, perform local dimensionality reduction. The parameters of the mixture model are estimated using an EM algorithm. A face is detected if the probability of an input sample is above a predened threshold. The other mixture of subspaces method uses Kohonen 's self-organizing map for clustering and Fisher Linear Discriminant to nd an optimal projection and a Gaussian distribution to model the class-conditional density function of the projected samples for each class. The parameters of the class-conditional density functions are maximum likelihood estimates and the decision rule is also based on maximum likelihood. A wide range of face images including ones in dierent poses, with dierent expressions and under dierent lighting conditions are used as the training set to capture the varia...",
    "neighbors": [
      664
    ],
    "mask": "Validation"
  },
  {
    "node_id": 429,
    "label": 4,
    "text": "Supporting Creativity with Advanced Information-Abundant User Interfaces A challenge for human-computer interaction researchers and user interface designers is to construct information technologies that support creativity. This ambitious goal can be attained if designers build on an adequate understanding of creative processes. This paper describes a model of creativity, the four-phase genex framework for generating excellence: - Collect: learn from previous works stored in digital libraries, the web, etc. - Relate: consult with peers and mentors at early, middle and late stages - Create: explore, compose, discover, and evaluate possible solutions - Donate: disseminate the results and contribute to the digital libraries, the web, etc. Within this integrated framework, there are eight activities that require human-computer interaction research and advanced user interface design. This paper concentrates on techniques of information visualization that support creative work by enabling users to find relevant information resources, identify desired items in a se...",
    "neighbors": [
      743,
      1167
    ],
    "mask": "Train"
  },
  {
    "node_id": 430,
    "label": 5,
    "text": "The CMUnited-97 Simulator Team . The Soccer Server system provides a rich and challenging multiagent, real-time domain. Agents must accurately perceive and act despite a quickly changing, largely hidden, noisy world. They must also act at several levels, ranging from individual skills to full-team collaborative and adversarial behaviors. This article presents the CMUnited-97 approaches to the above challenges which helped the team to the semifinals of the 29-team RoboCup-97 tournament. 1 Introduction  The Soccer Server system [5] used at RoboCup-97 [2] provides a rich and challenging multiagent, real-time domain. Sensing and acting is noisy, while interagent communication is unreliable and low-bandwidth. In order to be successful, each agent in a team must be able to sense and act in real time: sensations arrive at unpredictable intervals while actions are possible every 100ms. Furthermore, since the agents get local, noisy sensory information, they must have a method of converting their sensory inputs into a good w...",
    "neighbors": [
      127,
      155,
      394
    ],
    "mask": "Test"
  },
  {
    "node_id": 431,
    "label": 3,
    "text": "Query Rewriting for Semistructured Data We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q.  Our algorithm is based on appropriately generalizing containment mappings, the chase, and unification  -- techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries. We show that the algorithm is sound and complete for TSL, i.e., it always finds every TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use available structural constraints (such as DTDs) to find more opportunities for query rewriting. We currently incorporate the algorithm in the TSIMMIS system. 1 Introduction  Recently, many semistructured data models, query and view definition languages have been proposed [2...",
    "neighbors": [
      708
    ],
    "mask": "Train"
  },
  {
    "node_id": 432,
    "label": 4,
    "text": "An Anthropomorphic Agent for the Use of Spatial Language . In this paper we describe the communication with a responsive  virtual environment with the main emphasis on the processing of  spatial expressions in natural language instructions. This work is part  of the VIENA project in whichwechose interior design as an example  domain. A multiagent system acts as an intelligent mediator between the  user and a graphics system. To make the communication about spatial  relations more intuitive, we developed an anthropomorphic agent which  is graphically visualized in the scene. Considering the human-like #gure  we explain the use of qualitative spatial expressions, like #right of \" and  #there\".  1 Introduction  Interactive 3-dimensional graphics systems are more useful #e.g. in design#, when users can concentrate on their imaginations and be free from technical considerations. Therefore it is important to improveinteraction with the virtual environmentbyway of natural, intuitive communication forms.  In our work we consider a #virtual interface...",
    "neighbors": [
      90
    ],
    "mask": "Test"
  },
  {
    "node_id": 433,
    "label": 2,
    "text": "Towards a Highly-Scalable and Effective Metasearch Engine A metasearch engine is a system that supports unified access to multiple local search engines. Database selection is one of the main challenges in building a large-scale metasearch engine. The problem is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query. In order to enable accurate selection, metadata that reect the contents of each search engine need to be collected and used. In this paper, we propose a highly scalable and accurate database selection method. This method has several novel features. First, the metadata for representing the contents of all search engines are organized into a single integrated representative. Such a representative yields both computation efficiency and storage efficiency. Second, our selection method is based on a theory for ranking search engines optimally. Experimental results indicate that this new method is very effective. An operational prototype system has been built based on the proposed approach.",
    "neighbors": [
      224,
      241,
      271,
      477,
      510,
      696,
      792,
      931,
      1003,
      1134,
      1165
    ],
    "mask": "Train"
  },
  {
    "node_id": 434,
    "label": 0,
    "text": "Representing Coordination Relationships with Influence Diagrams It is well know the necessity of managing relationships among agents  in a multi-agent system to achieve coordinated behavior. One approach to manage  such relationships consists of using an explicit representation of them, allowing  each agent to choose its actions based on them. Previous work in the area have  considered ideal situations, such as fully known environments, static relationships  and shared mental states. In this paper we propose to represent relationships  among agents and entities in a multi-agent system by using influence diagrams.",
    "neighbors": [
      495,
      500
    ],
    "mask": "Test"
  },
  {
    "node_id": 435,
    "label": 2,
    "text": "Concept Hierarchy Based Text Database Categorization Document categorization as a technique to improve the retrieval of useful documents has been extensively investigated. One important issue in a large-scale metasearch engine is to select text databases that are likely to contain useful documents for a given query. We believe that database categorization can be a potentially effective technique for good database selection, especially in the Internet environment where short queries are usually submitted. In this paper, we propose and evaluate several database categorization algorithms. This study indicates that while some document categorization algorithms could be adopted for database categorization, algorithms that take into consideration the special characteristics of databases may be more effective. Preliminary experimental results are provided to compare the proposed database categorization algorithms. A prototype database categorization system based on one of the proposed algorithms has been developed.",
    "neighbors": [
      271,
      696,
      792,
      931,
      1003,
      1124
    ],
    "mask": "Train"
  },
  {
    "node_id": 436,
    "label": 0,
    "text": "The Organisation of Sociality: A Manifesto for a New Science of MultiAgent Systems . In this paper, we pose and motivate a challenge, namely the need for a new science of multiagent systems. We propose that this new science should be grounded, theoretically on a richer conception of sociality, and methodologically on the extensive use of computational modelling for real-world applications and social simulations. Here, the steps we set forth towards meeting that challenge are mainly theoretical. In this respect, we provide a new model of multi-agent systems that reflects a fully explicated conception of cognition, both at the individual and the collective level. Finally, the mechanisms and principles underpinning the model will be examined with particular emphasis on the contributions provided by contemporary organisation theory.  1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 437,
    "label": 2,
    "text": "Kernel Expansions With Unlabeled Examples Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance. We present a new tractable algorithm for exploiting unlabeled examples in discriminative classification. This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples. The resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution. We provide, in addition, a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework. We demonstrate that the proposed approach requires very few labeled examples for high classification accuracy. 1 Introduction In many modern classification problems such as text categorization, very few labeled examples are available but a...",
    "neighbors": [
      391,
      505,
      609,
      1133,
      1153
    ],
    "mask": "Validation"
  },
  {
    "node_id": 438,
    "label": 4,
    "text": "NEXUS - Distributed Data Management Concepts for Location Aware Applications Nowadays, mobile computers like subnotebooks or personal digital  assistants, as well as cellular phones can not only communicate wirelessly, but  they can also determine their position via appropriate sensors like DGPS. Socalled  location aware applications take advantage of this fact and structure information  according to the position of their users. In order to be able to assign  data to a certain location, these information systems have to refer to spatial  computer models. The NEXUS    project, which is supported by the Deutsche  Forschungsgemeinschaft (DFG, German Research Foundation), aims at the development  of a generic infrastructure that serves as a basis for location aware  applications. The central task of this platform deals with the data management.",
    "neighbors": [
      256
    ],
    "mask": "Train"
  },
  {
    "node_id": 439,
    "label": 2,
    "text": "Error-Correcting Output Coding for Text Classification This paper applies error-correcting output coding (ECOC) to the task of document categorization. ECOC, of recent vintage in the AI literature, is a method for decomposing a multiway classification problem into many binary classification tasks, and then combining the results of the subtasks into a hypothesized solution to the original problem. There has been much recent interest in the machine learning community about algorithms which integrate \"advice\" from many subordinate predictors into a single classifier, and error-correcting output coding is one such technique. We provide experimental results on several real-world datasets, extracted from the Internet, which demonstrate that ECOC can offer significant improvements in accuracy over conventional classification algorithms. 1 Introduction Error-correcting output coding is a recipe for solving multi-way classification problems. It works in two stages: first, independently construct many subordinate classifiers, each responsible for r...",
    "neighbors": [
      242,
      347
    ],
    "mask": "Train"
  },
  {
    "node_id": 440,
    "label": 1,
    "text": "Meta-Learning in Distributed Data Mining Systems: Issues and Approaches Data mining systems aim to discover patterns and extract useful information  from facts recorded in databases. A widely adopted approach to this  objective is to apply various machine learning algorithms to compute descriptive  models of the available data. Here, we explore one of the main  challenges in this research area, the development of techniques that scale up  to large and possibly physically distributed databases.  Meta-learning is a technique that seeks to compute higher-level classifiers  (or classification models), called meta-classifiers, that integrate in some principled  fashion multiple classifiers computed separately over different databases.  This study, describes meta-learning and presents the JAM system (Java Agents  for Meta-learning), an agent-based meta-learning system for large-scale data  mining applications. Specifically, it identifies and addresses several important  desiderata for distributed data mining systems that stem from their additional  complexity co...",
    "neighbors": [
      426,
      916
    ],
    "mask": "Train"
  },
  {
    "node_id": 441,
    "label": 0,
    "text": "Investigating Interactions Between Agent Conversations and Agent Control Components Exploring agent conversation in the context of fine-grained agent coordination research has raised several intellectual questions. The major issues pertain to interactions between different agent conversations, the representations chosen for different classes of conversations, the explicit modeling of interactions between the conversations, and how to address these interactions. This paper is not so ambitious as to attempt to address these questions, only frame them in the context of quantified, scheduling-centric multi-agent coordination. research. 1 Introduction  Based on a long history of work in agents and agent control components for building distributed AI and multi-agent systems, we are attempting to frame and address a set of intellectual questions pertaining to agent conversation. Interaction lies at the heart of the matter; the issue is interaction between different agent conversations, that possibly occur at different levels of abstraction, but also interaction between the m...",
    "neighbors": [
      367,
      495,
      500,
      724,
      847,
      945
    ],
    "mask": "Train"
  },
  {
    "node_id": 442,
    "label": 1,
    "text": "Applications of Machine Learning and Rule Induction An important area of application for machine learning is in automating the acquisition of knowledge bases required for expert systems. In this paper, we review the major paradigms for machine learning, including neural networks, instance-based methods, genetic learning, rule induction, and analytic approaches. We consider rule induction in greater detail and review some of its recent applications, in each case stating the problem, how rule induction was used, and the status of the resulting expert system. In closing, we identify the main stages in fielding an applied learning system and draw some lessons from successful applications. Introduction  Machine learning is the study of computational methods for improving performance by mechanizing the acquisition of knowledge from experience. Expert performance requires much domainspecific knowledge, and knowledge engineering has produced hundreds of AI expert systems that are now used regularly in industry. Machine learning aims to provide ...",
    "neighbors": [
      0,
      443,
      1246
    ],
    "mask": "Train"
  },
  {
    "node_id": 443,
    "label": 1,
    "text": "A Survey of Methods for Scaling Up Inductive Algorithms . One of the defining challenges for the KDD research community is to enable inductive learning algorithms to mine very large databases. This paper summarizes, categorizes, and compares existing work on scaling up inductive algorithms. We concentrate on algorithms that build decision trees and rule sets, in order to provide focus and specific details; the issues and techniques generalize to other types of data mining. We begin with a discussion of important issues related to scaling up. We highlight similarities among scaling techniques by categorizing them into three main approaches. For each approach, we then describe, compare, and contrast the different constituent techniques, drawing on specific examples from published papers. Finally, we use the preceding analysis to suggest how to proceed when dealing with a large problem, and where to focus future research.  Keywords: scaling up, inductive learning, decision trees, rule learning  1. Introduction  The knowledge discovery and data...",
    "neighbors": [
      251,
      442,
      916
    ],
    "mask": "Validation"
  },
  {
    "node_id": 444,
    "label": 4,
    "text": "The Structure of Object Transportation and Orientation in Human-Computer Interaction An experiment was conducted to investigate the relationship between object transportation and object orientation by the human hand in the context of humancomputer interaction (HCI). This work merges two streams of research: the structure of interactive manipulation in HCI and the natural hand prehension in human motor control. It was found that object transportation and object orientation have a parallel, interdependent structure which is generally persistent over different visual feedback conditions. The notion of concurrency and interdependence of multidimensional visuomotor control structure can provide a new framework for human-computer interface evaluation and design.  Keywords  Direct manipulation, input device, multi-dimensional control, visuomotor control, visual conditions, information processing, interface design, virtual reality.  INTRODUCTION  Object manipulation is a basic operation in humancomputer interaction (HCI). Modern computer technology advances towards affording m...",
    "neighbors": [
      260,
      1042
    ],
    "mask": "Train"
  },
  {
    "node_id": 445,
    "label": 0,
    "text": "From Active Objects to Autonomous Agents This paper studies how to extend the concept of active objects  into a structure of agents. It first discusses the requirements for autonomous  agents that are not covered by simple active objects. We  propose then the extension of the single behavior of an active object  into a set of behaviors with a meta-behavior scheduling their activities.  To make a concrete proposal based on these ideas we describe how we  extended a framework of active objects, named Actalk, into a generic  multi-agent platform, named DIMA. We discuss how this extension  has been implemented. We finally report on one application of DIMA  to simulate economic models.  Keywords: active object, agent, implementation, meta-behavior, modularity, re-usability, simulation.  1 Introduction  Object-oriented concurrent programming (OOCP) is the most appropriate and promising technology to implement agents. The concept of active object may be considered as the basic structure for building agents. Furthermore, the combinat...",
    "neighbors": [
      955
    ],
    "mask": "Validation"
  },
  {
    "node_id": 446,
    "label": 5,
    "text": "OBPRM: An Obstacle-Based PRM for 3D Workspaces this paper we consider an obstacle-based prm",
    "neighbors": [
      320,
      387,
      1052
    ],
    "mask": "Train"
  },
  {
    "node_id": 447,
    "label": 2,
    "text": "From Resource Discovery to Knowledge Discovery on the Internet More than 50 years ago, at a time when modern computers didn't exist yet, Vannevar Bush wrote about a multimedia digital library containing human collective knowledge and filled with \"trails\" linking materials of the same topic. At the end of World War II, Vannevar urged scientists to build such a knowledge store and make it useful, continuously extendable and more importantly, accessible for consultation. Today, the closest to the materialization of Vannevar's dream is the World-Wide Web hypertext and multimedia document collection. However, the ease of use and accessibility of the knowledge described by Vannevar is yet to be realized. Since the 60s, extensive research has been accomplished in the information retrieval field, and free-text search was finally adopted by many text repository systems in the late 80s. The advent of the World-Wide Web in the 90s helped text search become routine as millions of users use search engines daily to pinpoint resources on the Internet. However, r...",
    "neighbors": [
      9,
      410,
      897,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 448,
    "label": 1,
    "text": "Three Ways to Grow Designs: A Comparison of Evolved Embryogenies for a Design Problem This paper explores the use of growth processes, or embryogenies, to map genotypes to phenotypes within evolutionary systems. Following a summary of the significant features of embryogenies, the three main types of embryogenies in Evolutionary Computation are then identified and explained: external, explicit and implicit. An experimental comparison between these three different embryogenies and an evolutionary algorithm with no embryogeny is performed. The problem set to the four evolutionary systems is to evolve tessellating tiles. In order to assess the scalability of the embryogenies, the problem is increased in difficulty by enlarging the size of tiles to be evolved. The results are surprising, with the implicit embryogeny outperforming all other techniques by showing no significant increase in the size of the genotypes or decrease in accuracy of evolution as the scale of the problem is increased. 1. Introduction The use of computers to evolve solutions to problems has seen a dra...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 449,
    "label": 5,
    "text": "Implementing a Knowledge Date-a-Base Knowledge-based systems are very useful, but can be  dicult to design because of the complexity of the realworld  knowledge they represent. This paper compares  the experiences of building the same knowledge base by  hand in two dierent systems, Otter and CLIPS. The  knowledge base considered is that of people's preferences  towards others, in the interests of nding a \\dating  match.\" Finally, this paper considers Horn theorems  and their impact on the usefulness of knowledge  systems.  Introduction  Because technology and automation are increasingly becoming a part of everyday life, it is benecial to enable technology to \\understand\" its application area. An obvious way of doing this is to implement and embed a knowledge base in an application. However, designing a good knowledge base is not trivial. A good knowledge base needs to be general so it can be reused, complete to avoid bad models, and ecient in description and time.  This paper presents the authors' experiences implement...",
    "neighbors": [
      980
    ],
    "mask": "Train"
  },
  {
    "node_id": 450,
    "label": 4,
    "text": "Jazz: An Extensible Zoomable User Interface Graphics Toolkit in Java In this paper we investigate the use of scene graphs as a general approach for implementing two-dimensional (2D) graphical applications, and in particular Zoomable User Interfaces (ZUIs). Scene graphs are typically found in three-dimensional (3D) graphics packages such as Sun's Java3D and SGI's OpenInventor. They have not been widely adopted by 2D graphical user interface toolkits.  To explore the effectiveness of scene graph techniques, we have developed Jazz, a general-purpose 2D scene graph toolkit. Jazz is implemented in Java using Java2D, and runs on all platforms that support Java 2. This paper describes Jazz and the lessons we learned using Jazz for ZUIs. It also discusses how 2D scene graphs can be applied to other application areas.  Keywords  Zoomable User Interfaces (ZUIs), Animation, Graphics, User Interface Management Systems (UIMS), Pad++, Jazz.  INTRODUCTION  Today's Graphical User Interface (GUI) toolkits contain a wide range of built-in user interface objects (also kno...",
    "neighbors": [
      213,
      1021
    ],
    "mask": "Train"
  },
  {
    "node_id": 451,
    "label": 0,
    "text": "Designing Agent-Oriented Systems by Analysing Agent Interactions . We propose a preliminary methodology for agent-oriented software  engineering based on the idea of agent interaction analysis. This approach uses  interactions between undetermined agents as the primary component of analysis  and design. Agents as a basis for software engineering are useful because they  provide a powerful and intuitive abstraction which can increase the comprehensiblity  of a complex design. The paper describes a process by which the designer  can derive the interactions that can occur in a system satisfying the given requirements  and use them to design the structure of an agent-based system, including  the identification of the agents themselves. We suggest that this approach has the  flexibility necessary to provide agent-oriented designs for open and complex applications,  and has value for future maintenance and extension of these systems.  1",
    "neighbors": [
      140,
      573,
      941,
      957
    ],
    "mask": "Train"
  },
  {
    "node_id": 452,
    "label": 3,
    "text": "Searching Documents on the Intranet Searching for documents on the internet with today\u2019s search engines, which are mainly based on words in a document, is not satisfactory. Results can be improved by also taking the content of a document into account. The Extensible Markup Language (XML) enables us to do semantic tagging and to make the structure of a document explicit. But this describes a document only at the syntactical level. A more ideal situation would be when the XML tagging is also used to define the document at the semantical level. To realize this we allow an author of a document to describe the relevant concepts by means of tags like he would design an object-oriented database schema. In our approach a user searching for a particular document is presented a graphical description of such a schema, that describes the concepts defined for the webspace of an intranet. Via this interface the user can formulate OO-like queries or navigate to relevant web pages. To realize our ideas we are building an architecture based on the concept of an index-database. A prototype is up and running.",
    "neighbors": [
      78,
      161
    ],
    "mask": "Validation"
  },
  {
    "node_id": 453,
    "label": 0,
    "text": "CiteSeer: An Autonomous Web Agent for Automatic Retrieval and Identification of Interesting Publications Published research papers available on the World Wide Web (WWW or Web) are often poorly organized, often exist in non-text form (e.g. Postscript) documents, and increase in quantity daily. Significant amounts of time and effort are commonly needed to find interesting and relevant publications on the Web. We have developed a Web based information agent that assists the user in the process of performing a scientific literature search. Given a set of keywords, the agent uses Web search engines and heuristics to locate and download papers. The papers are parsed in order to extract information features such as the abstract and individually identified citations which are placed into an SQL database. The agent's Web interface can be used to find relevant papers in the database using keyword searches, or by navigating the links between papers formed by the citations. Links to both \"citing\" and \"cited\" publications can be followed. In addition to simple browsing and keyword searches, the agent ...",
    "neighbors": [
      43,
      70,
      382,
      561,
      596,
      1181
    ],
    "mask": "Train"
  },
  {
    "node_id": 454,
    "label": 4,
    "text": "Future Multimedia User Interfaces this article, we examine some of the work that has been done in these two fields and explore where they are heading. First, we review their often-confusing terminology and provide a brief historical overview. Since both fields rely largely on relatively unusual, and largely immature, hardware technologies, we next provide a high-level introduction to important hardware issues. This is followed by a description of the key approaches to system architecture used by current researchers. We then build on the background provided by these sections to lay out a set of current research issues and directions for future work. Throughout, we attempt to emphasize the many ways in which virtual environments and ubiquitous computing can complement each other, creating an exciting new form of multimedia computing that is far more powerful than either approach would make possible alone.",
    "neighbors": [
      325
    ],
    "mask": "Validation"
  },
  {
    "node_id": 455,
    "label": 3,
    "text": "The SDCC Framework For Integrating Existing Algorithms for Diverse Data Warehouse Maintenance Tasks Recently proposed view maintenance algorithms tackle the problem of concurrent data updates happening at different autonomous ISs, whereas the EVE system addresses the maintenance of a data warehouse after schema changes of ISs. The concurrency of schema changes and data updates still remains an unexplored problem however. This paper now provides a first solution that guarantees concurrent view definition evolution and view extent maintenance of a DW defined over distributed ISs. For this problem, we introduce a framework called SDCC (Schema change and Data update Concurrency Control) system. SDCC integrates  existing algorithms designed to address view maintenance subproblems, such as view extent maintenance after IS data updates, view definition evolution after IS schema changes, and view extent adaptation after view definition changes, into one system by providing  protocols that enable them to correctly co-exist and collaborate. SDCC tracks any potential faulty updates of the DW ca...",
    "neighbors": [
      637,
      993,
      1209
    ],
    "mask": "Train"
  },
  {
    "node_id": 456,
    "label": 0,
    "text": "Corporate Memory Management through Agents . The CoMMA project (Corporate Memory Management through Agents)  aims at developing an open, agent-based platform for the management of a corporate  memory by using the most advanced results on the technical, the content, and the  user interaction level. We focus here on methodologies for the set-up of multi-agent  systems, requirement engineering and knowledge acquisition approaches.  1. Introduction  How to improve access, share and reuse of both internal and external knowledge in a company? How to improve newcomers' learning and integration in a company? How to enhance technology monitoring in a company? Knowledge Management (KM) aims at solving such problems. Different research communities offer - partial - solutions for supporting KM. The integration of results from these different research fields seems to be a promising approach. This is the motivation of the CoMMA IST project-funded by the European Commission- which started February 2000. The main objective is to implement and ...",
    "neighbors": [
      934,
      1085
    ],
    "mask": "Validation"
  },
  {
    "node_id": 457,
    "label": 2,
    "text": "Intelligent Crawling on the World Wide Web with Arbitrary Predicates The enormous growth of the world wide web in recent years has made it important to perform resource discovery efficiently. Consequently, several new ideas have been proposed in recent years; among them a key technique is focused crawling which is able to crawl particular topical portions of the world wide web quickly without having to explore all web pages. In this paper, we propose the novel concept of intelligent crawling which actually learns characteristics of the linkage structure of the world wide web while performing the crawling. Specifically, the intelligent crawler uses the inlinking web page content, candidate URL structure, or other behaviors of the inlinking web pages or siblings in order to estimate the probability that a candidate is useful for a given crawl. This is a much more general framework than the focused crawling technique which is based on a pre-defined understanding of the topical structure of the web. The techniques discussed in this paper are applicable for crawling web pages which satisfy arbitrary user-defined predicates such as topical queries, keyword queries or any combinations of the above. Unlike focused crawling, it is not necessary to provide representative topical examples, since the crawler can learn its way into the appropriate topic. We refer to this technique as intelligent crawling because of its adaptive nature in adjusting to the web page linkage structure. The learning crawler is capable of reusing the knowledge gained in a given crawl in order to provide more efficient crawling for closely related predicates.",
    "neighbors": [
      1,
      53,
      382,
      649,
      774,
      1000,
      1059,
      1104
    ],
    "mask": "Train"
  },
  {
    "node_id": 458,
    "label": 3,
    "text": "Tractable Query Answering in Indefinite Constraint Databases: Basic Results and Applications to Querying Spatiotemporal Information . We consider the scheme of indefinite constraint databases  proposed by Koubarakis. This scheme can be used to represent indefinite  information arising in temporal, spatial and truly spatiotemporal  applications. The main technical problem that we address in this paper is  the discovery of tractable classes of databases and queries in this scheme.  We start with the assumption that we have a class of constraints C with  satisfiability and variable elimination problems that can be solved in  PTIME. Under this assumption, we show that there are several general  classes of databases and queries for which query evaluation can be done  with PTIME data complexity. We then search for tractable instances of  C in the area of temporal and spatial constraints. Classes of constraints  with tractable satisfiability problems can be easily found in the literature.  The largest class that we consider is the class of Horn disjunctive  linear constraints over the rationals. Because variable eliminati...",
    "neighbors": [
      481,
      881,
      1012
    ],
    "mask": "Train"
  },
  {
    "node_id": 459,
    "label": 5,
    "text": "Generating, Executing and Revising Schedules for Autonomous Robot Office Couriers Scheduling the tasks of an autonomous robot office courier and carrying out the scheduled tasks reliably and efficiently pose challenging problems for autonomous robot control. To carry out their jobs reliably and efficiently many autonomous mobile service robots acting in human working environments have to view their jobs as everyday activity: they should accomplish longterm efficiency rather than optimize problem-solving episodes. They should also exploit opportunities and avoid problems flexibly because often robots are forced to generate schedules based on partial information. We propose to implement the controller for scheduled activity by employing concurrent reactive plans that reschedule the course of action whenever necessary and while performing their actions. The plans are represented modularly and transparently to allow for easy transformation. Scheduling and schedule repair methods are implemented as plan transformation rules. Introduction  To carry out their jobs reliably...",
    "neighbors": [
      340
    ],
    "mask": "Train"
  },
  {
    "node_id": 460,
    "label": 4,
    "text": "Sensing Techniques for Mobile Interaction We describe sensing techniques motivated by unique aspects of human-computer interaction with handheld devices in mobile settings. Special features of mobile interaction include changing orientation and position, changing venues, the use of computing as auxiliary to ongoing, real-world activities like talking to a colleague, and the general intimacy of use for such devices. We introduce and integrate a set of sensors into a handheld device, and demonstrate several new functionalities engendered by the sensors, such as recording memos when the device is held like a cell phone, switching between portrait and landscape display modes by holding the device in the desired orientation, automatically powering up the device when the user picks it up the device to start using it, and scrolling the display using tilt. We present an informal experiment, initial usability testing results, and user reactions to these techniques.  Keywords  Input devices, interaction techniques, sensing, contextaware...",
    "neighbors": [
      189,
      1006
    ],
    "mask": "Train"
  },
  {
    "node_id": 461,
    "label": 3,
    "text": "Summary this paper. The main questions addressed in this setting deal with conditions under which it is possible to evaluate queries incrementally.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 462,
    "label": 0,
    "text": "Self-Adaptive Operator Scheduling using the Religion-Based EA The optimal choice of the variation operators mutation and crossover and their parameters can be decisive for the performance of evolutionary algorithms (EAs). Usually the type of the operators (such as Gaussian mutation) remains the same during the entire run and the probabilistic frequency of their application is determined by a constant parameter, such as a fixed mutation rate. However, recent studies have shown that the optimal usage of a variation operator changes during the EA run. In this study, we combined the idea of self-adaptive mutation operator scheduling with the Religion-Based EA (RBEA), which is an agent model with spatially structured and variable sized subpopulations (religions). In our new model (OSRBEA), we used a selection of different operators, such that each operator type was applied within one specific subpopulation only. Our results indicate that the optimal choice of operators is problem dependent, varies during the run, and can be handled by our self-adaptive OSRBEA approach. Operator scheduling could clearly improve the performance of the already very powerful RBEA and was superior compared to a classic and other advanced EA approaches.",
    "neighbors": [
      783
    ],
    "mask": "Validation"
  },
  {
    "node_id": 463,
    "label": 1,
    "text": "Dynamic on-line clustering and state extraction: An approach to symbolic learning Researchers often try to understand the representations that develop in the hidden layers of a neural network during training. Interpretation is difficult because the representations are typically highly distributed and continuous. By \"continuous,\" we mean that if one constructed a scatter plot over the hidden unit activity space of patterns obtained in response to various inputs, examination at any scale would reveal the patterns to be broadly distributed over the space. Such continuous representations are naturally obtained if the input space and activation dynamics are continuous. Continuous representations are not always appropriate. Many task domains might benefit from discrete representations -- representations selected from a finite set of alternatives. Example domains include finite-state machine emulation, data compression, language and higher cognition (involving discrete symbol processing), and categorization. In such domains, standard neural...",
    "neighbors": [
      166
    ],
    "mask": "Train"
  },
  {
    "node_id": 464,
    "label": 1,
    "text": "A Behavior-Based Intelligent Control Architecture with Application to Coordination of Multiple Underwater Vehicles The paper presents a behavior-based intelligent control architecture for designing controllers which, based on their observation of sensor signals, compute the discrete control actions. These control actions then serve as the \"set-points\" for the lower level controllers. The behavior-based approach yields an intelligent controller which is a cascade of a perceptor and a response controller. The perceptor extracts the relevant symbolic information from the incoming continuous sensor signals, which enables the execution of one of the behaviors. The response controller is a discrete event system that computes the discrete control actions by executing one of the enabled behaviors. The behavioral approach additionally yields a hierarchical two layered response controller, which provides better complexity management. The inputs from the perceptor are used to first compute the higher level activities, called behaviors, and next to compute the corresponding lower level activities, called actio...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 465,
    "label": 0,
    "text": "Hive: Distributed Agents for Networking Things Hive is a distributed agents platform, a decentralized system for building applications by networking local system resources. This paper presents the architecture of Hive, concentrating on the idea of an \"ecology of distributed agents\" and its implementation in a practical Java based system. Hive provides ad-hoc agent interaction, ontologies of agent capabilities, mobile agents, and a graphical interface to the distributed system. We are applying Hive to the problems of networking \"Things That Think,\" putting computation and communication in everyday places such as your shoes, your kitchen, or your own body. TTT shares the challenges and potentials of ubiquitous computing and embedded network applications. We have found that the flexibility of a distributed agents architecture is well suited for this application domain, enabling us to easily build applications and to reconfigure our systems on the fly. Hive enables us to make our environment and network more alive. This paper is dedic...",
    "neighbors": [
      258,
      415,
      701,
      978
    ],
    "mask": "Train"
  },
  {
    "node_id": 466,
    "label": 2,
    "text": "Crawling the Hidden Web Current-day crawlers retrieve content only from  the publicly indexable Web, i.e., the set of Web  pages reachable purely by following hypertext  links, ignoring search forms and pages that require  authorization or prior registration. In particular,  they ignore the tremendous amount of high quality  content \"hidden\" behind search forms, in large  searchable electronic databases. In this paper, we  address the problem of designing a crawler capable  of extracting content from this hidden Web.  We introduce a generic operational model of a  hidden Web crawler and describe how this model  is realized in HiWE (Hidden Web Exposer), a  prototype crawler built at Stanford. We introduce  a new Layout-based Information Extraction  Technique (LITE) and demonstrate its use in automatically  extracting semantic information from  search forms and response pages. We also present  results from experiments conducted to test and  validate our techniques.  1",
    "neighbors": [
      224,
      1134
    ],
    "mask": "Train"
  },
  {
    "node_id": 467,
    "label": 4,
    "text": "BUILD-IT: A Planning Tool for Construction and Design It is time to go beyond the established approaches in humancomputer interaction. With the Augmented Reality (AR) design strategy humans are able to behave as much as possible in a natural way: behavior of humans in the real world with other humans and/or real world objects. Following the fundamental constraints of natural way of interacting we derive a set of recommendations for the next generation of user interfaces: the Natural User Interface (NUI). The concept of NUI is presented in form of a runnable demonstrator: a computer vision-based interaction technique for a planning tool for construction and design tasks.  Keywords  augmented reality, digital desk, natural user interface, computer vision-based interaction",
    "neighbors": [
      238
    ],
    "mask": "Train"
  },
  {
    "node_id": 468,
    "label": 3,
    "text": "Normal Forms for Defeasible Logic Defeasible logic is an important logic-programming based nonmonotonic reasoning formalism which has an efficient implementation. It makes use of facts, strict rules, defeasible rules, defeaters, and a superiority relation. Representation results are important because they can help the assimilation of a concept by confining attention to its critical aspects. In this paper we derive some representation results for defeasible logic. In particular we show that the superiority relation does not add to the expressive power of the logic, and can be simulated by other ingredients in a modular way. Also, facts can be simulated by strict rules. Finally we show that we cannot simplify the logic any further in a modular way: Strict rules, defeasible rules, and defeaters form a minimal set of independent ingredients in the logic. 1 Introduction  Normal forms play an important role in computer science. Examples of areas where normal forms have proved fruitful include logic [10], where normal forms o...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 469,
    "label": 0,
    "text": "ITR: A Framework for Environment-Aware, Massively Distributed Computing physical environment in real-time, and the need to reason about emerging aggregate properties as opposed to individual component behavior. In this research we propose to develop theory, methods and tools for massively distributed, environment-aware computing (more succinctly referred to as swarm computing).  The state of swarm computing today is similar to that of sequential computing in the early 1950s. Developers painstakingly produce swarm programs by designing and programming the actions of individual devices, and converge on an acceptable program through extensive simulation and experimentation. In the pre-compiler era, skeptical programmers believed that a mechanical process could not possibly produce code of comparable quality to that produced by highly skilled machine coders and that the cost of machine time is high enough to outweigh any possible savings in programmer effort. The state of swarm programming today is similar: devices are still expensive enough an",
    "neighbors": [
      689
    ],
    "mask": "Test"
  },
  {
    "node_id": 470,
    "label": 3,
    "text": "The View Holder Approach: Utilizing Customized Materialized Views To Create Database Services Suitable For Mobile Database Applications among mobile devices (i.e., a laptop vs. a pager) and the amount of information available from today's database environments and the Internet.  To this end, this dissertation presents the development of customizable view maintenance services, called the View Holder approach, whose middleware mechanism within the fixed network dynamically maintains versions of the views so that to meet the data consistency and currency requirements of a particular mobile client. In a general form, a View Holder can support a community of mobile clients with common interests. The motivation for maintaining versions is to compensate for the data changes that occurred to the materialized views that were used during disconnection as well as to reduce the cost of wireless communication. In order to maintain these views, customized view maintenance is performed at the data sources by translating the mobile machine's request into a materialization program containing a triggering",
    "neighbors": [
      174,
      278,
      354,
      373
    ],
    "mask": "Train"
  },
  {
    "node_id": 471,
    "label": 2,
    "text": "Using Relevance Feedback In Contentbased Image Metasearch this article with a review of the issues in content-based visual query, then describe the current MetaSeek implementation. We present the results of experiments that evaluated the implementation in comparison to a previous version of the system and a baseline engine that randomly selects the individual search engines to query. We conclude by summarizing open issues for future research.",
    "neighbors": [
      1066
    ],
    "mask": "Train"
  },
  {
    "node_id": 472,
    "label": 0,
    "text": "JACK Intelligent Agents - Components for Intelligent Agents in Java This paper is organised as follows. Section 2 introduces JACK Intelligent Agents, presenting the approach taken by AOS to its design and outlining its major engineering characteristics. The BDI model is discussed briefly in Section 3. Section 4 gives an outline of how to build an application with JACK Intelligent Agents. Finally, in Section 5 we discuss how the use of this framework can be beneficial to both engineers and researchers. For brevity, we will refer to JACK Intelligent Agents simply as \"JACK\".",
    "neighbors": [
      182,
      885
    ],
    "mask": "Train"
  },
  {
    "node_id": 473,
    "label": 1,
    "text": "Applying Formal Concepts to Learning Systems Validation In the problem area of evaluating complex software systems, there are two distinguished areas of research, development, and application  identified by the two buzzwords validation and verification, respectively. From the perspective adopted by the authors, verification is usually  more formally based and, thus, can be supported by formal reasoning tools like theorem provers, for instance.  The scope of verification approaches is limited by the difficulty of finding a sufficiently complete formalization to built upon. In paramount realistic problem domains, validation seems to be more appropriate, although it is less stringent in character and, therefore, validation results are often less definite. The aim of this paper is to exemplify a validation approach based on a clear and thoroughly formal theory. In this way, validation and  verification should be brought closer to each other.  To allow for precise and sufficiently clear results, the authors have selected the applicatio...",
    "neighbors": [
      935,
      1268
    ],
    "mask": "Test"
  },
  {
    "node_id": 474,
    "label": 4,
    "text": "Design and Implementation of Expressive Footwear As an outgrowth of our interest in dense wireless sensing and expressive applications of wearable computing, we have developed the world's most versatile human-computer interface for the foot. By dense wireless sensing, we mean the remote acquisition of many different parameters with a compact, autonomous sensor cluster. We have developed such a low-power sensor card to measure over 16 continuous quantities and transmit them wirelessly to a remote base station, updating all variables at 50 Hz. We have integrated a pair of these devices onto the feet of dancers and athletes, measuring continuous pressure at 3 points near the toe, dynamic pressure at the heel, bidirectional bend of the sole, height of each foot off conducting strips in the stage, angular rate of each foot about the vertical, angular position of each foot about the Earth's local magnetic field, as well as their tilt and low-G acceleration, 3-axis shock acceleration (from kicks and jumps), and position (via an integrated s...",
    "neighbors": [
      979
    ],
    "mask": "Train"
  },
  {
    "node_id": 475,
    "label": 3,
    "text": "NiagaraCQ: A Scalable Continuous Query System for Internet Databases Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system\u2019s performance and scalability. 1.",
    "neighbors": [
      20,
      101,
      651
    ],
    "mask": "Train"
  },
  {
    "node_id": 476,
    "label": 1,
    "text": "CABINS: A Framework of Knowledge Acquisition and Iterative Revision for Schedule Improvement and Reactive Repair Practical scheduling problems generally require allocation of resources in the presence of a large, diverse and typically conflicting set of constraints and optimization criteria. The ill-structuredness of both the solution space and the desired objectives make scheduling problems difficult to formalize. This paper describes a case-based learning method for acquiring context-dependent user optimization preferences and tradeoffs and using them to incrementally improve schedule quality in predictive scheduling and reactive schedule management in response to unexpected execution events. The approach, implemented in the CABINS system, uses acquired user preferences to dynamically modify search control to guide schedule improvement. During iterative repair, cases are exploited for: (1) repair action selection, (2) evaluation of intermediate repair results and (3) recovery from revision failures. The method allows the system to dynamically switch between repair heuristic actions, each of whi...",
    "neighbors": [
      195
    ],
    "mask": "Train"
  },
  {
    "node_id": 477,
    "label": 2,
    "text": "Efficient and Effective Metasearch for a Large Number of Text Databases Metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources (text databases). In a metasearch engine, the contents of each local database is represented by a representative. Each user query is evaluated against the set of representatives of all databases in order to determine the appropriate databases to search. When the number of databases is very large, say in the order of tens of thousands or more, then a traditional metasearch engine may become inefficient as each query needs to be evaluated against too many database representatives. Furthermore, the storage requirement on the site containing the metasearch engine can be very large. In this paper, we propose to use a hierarchy of database representatives to improve the efficiency. We provide an algorithm to search the hierarchy. We show that the retrieval effectiveness of our algorithm is the same as that of evaluating the user query against all database representatives. We als...",
    "neighbors": [
      271,
      433,
      526,
      696,
      897,
      1124,
      1165
    ],
    "mask": "Validation"
  },
  {
    "node_id": 478,
    "label": 2,
    "text": "Contextual Rules for Text Analysis In this paper we describe a rule-based formalism for the analysis and  labelling of texts segments. The rules are contextual rewriting rules with a  restricted form of negation. They allow to underspecify text segments not  considered relevant to a given task and to base decisions upon context. A parser  for these rules is presented and consistence and completeness issues are  discussed. Some results of an implementation of this parser with a set of rules  oriented to the segmentation of texts in propositions are shown.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 479,
    "label": 1,
    "text": "Protein Structure Prediction With Evolutionary Algorithms Evolutionary algorithms have been successfully applied to a variety of molecular structure prediction problems. In this paper we reconsider the design of genetic algorithms that have been applied to a simple protein structure prediction problem. Our analysis considers the impact of several algorithmic factors for this problem: the conformational representation, the energy formulation and the way in which infeasible conformations are penalized. Further we empirically evaluate the impact of these factors on a small set of polymer sequences. Our analysis leads to specific recommendations for both GAs as well as other heuristic methods for solving PSP on the HP model. 1 INTRODUCTION  A protein is a chain of amino acid residues that folds into a specific native tertiary structure under certain physiological conditions. A protein's structure determines its biological function. Consequently, methods for solving protein structure prediction (PSP) problems are valuable tools for modern molecula...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 480,
    "label": 5,
    "text": "Characterizing Operating System Activity In Specjvm98 Benchmarks : Complete system simulation to understand the influence of architecture and  operating systems on application execution has been identified to be crucial for  systems design. This problem is particularly interesting in the context of Java  since it is not only the application that can invoke kernel services, but so does  the underlying Java Virtual Machine (JVM) implementation which runs these  programs. Further, the JVM style (JIT compiler or interpreter) and the manner in  which the different JVM components (such as the garbage collector and class  loader) are exercised, can have a significant impact on the kernel activities. To  investigate these issues, this chapter uses complete system simulation of the  SPECjvm98 benchmarks on the SimOS simulation platform. The execution of  these benchmarks on both JIT compilers and interpreters is profiled in detail.  The kernel activity of SPECjvm98 applications constitutes up to 17% of the  execution time in the large dataset and up to 31% i...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 481,
    "label": 3,
    "text": "The DEDALE System for Complex Spatial Queries This paper presents dedale, a spatial database system intended to overcome some limitations of current systems by providing an abstract and non-specialized data model and query language for the representation and manipulation of spatial objects. dedale relies on a logical model based on linear constraints, which generalizes the constraint database model of [KKR90]. While in the classical constraint model, spatial data is always decomposed into its convex components, in dedale holes are allowed to fit the need of practical applications. The logical representation of spatial data although slightly more costly in memory, has the advantage of simplifying the algorithms. dedale relies on nested relations, in which all sorts of data (thematic, spatial, etc.) are stored in a uniform fashion. This new data model supports declarative query languages, which allow an intuitive and efficient manipulation of spatial objects. Their formal foundation constitutes a basis for practical query optimizati...",
    "neighbors": [
      147,
      397,
      458
    ],
    "mask": "Train"
  },
  {
    "node_id": 482,
    "label": 2,
    "text": "SimRank: A Measure of Structural-Context Similarity The problem of measuring \"similarity\" of objects arises in  many applications, and many domain-specific measures  have been developed, e.g., matching text across documents  or computing overlap among item-sets. We propose  a complementary approach, applicable in any domain  with object-to-object relationships, that measures similarity  of the structural context in which objects occur, based  on their relationships with other objects. Effectively, we  compute a measure that says \"two objects are similar if  they are related to similar objects.\" This general similarity  measure, called SimRank, is based on a simple and intuitive  graph-theoretic model. For a given domain, SimRank  can be combined with other domain-specific similarity  measures. We suggest techniques for efficient computation  of SimRank scores, and provide experimental results  on two application domains showing the computational  feasibility and effectiveness of our approach.",
    "neighbors": [
      216,
      1189
    ],
    "mask": "Train"
  },
  {
    "node_id": 483,
    "label": 2,
    "text": "The THISL Broadcast News Retrieval System This paper described the THISL spoken document retrieval system for British and North American Broadcast News. The system is based on the ABBOT large vocabulary speech recognizer, using a recurrent network acoustic model, and a probabilistic text retrieval system. We discuss the development of a realtime British English Broadcast News system, and its integration into a spoken document retrieval system. Detailed evaluation is performed using a similar North American Broadcast News system, to take advantage of the TREC SDR evaluation methodology. We report results on this evaluation, with particular reference to the effect of query expansion and of automatic segmentation algorithms. 1. INTRODUCTION  THISL is an ESPRIT Long Term Research project in the area of speech retrieval. It is concerned with the construction of a system which performs good recognition of broadcast speech from television and radio news programmes, from which it can produce multimedia indexing data. The principal obj...",
    "neighbors": [
      604
    ],
    "mask": "Validation"
  },
  {
    "node_id": 484,
    "label": 3,
    "text": "A String-based Model for Infinite Granularities (Extended Abstract) )  Jef Wijsen  Universit'e de Mons-Hainaut  Jef.Wijsen@umh.ac.be  Abstract  In the last few years, the concept of time granularity  has been defined by several researchers, and a glossary  of time granularity concepts has been published. These  definitions often view a time granularity as a (mostly  infinite) sequence of time granules. Although this view  is conceptually clean, it is extremely inefficient or even  practically impossible to represent a time granularity  in this manner. In this paper, we present a practical  formalism for the finite representation of infinite granularities.  The formalism is string-based, allows symbolic  reasoning, and can be extended to multiple dimensions  to accommodate, for example, space.  Introduction  In the last few years, formalisms to represent and to reason about temporal and spatial granularity have been developed in several areas of computer science. Although several researchers have used different definitions of time granularity, they comm...",
    "neighbors": [
      1237
    ],
    "mask": "Test"
  },
  {
    "node_id": 485,
    "label": 0,
    "text": "A Multi-Agent Approach to Vehicle Monitoring in Motorway . This paper describes CaseLP, a prototyping environment for MultiAgent  Systems (MAS), and its adoption for the development of a distributed industrial  application. CaseLP employs architecture definition, communication,  logic and procedural languages to model a MAS from the top-level architecture  down to procedural behavior of each agent's instance. The executable specification  which is obtained can be employed as a rapid prototype which helps in  taking quick decisions on the best possible implementation solutions. Such capabilities  have been applied to a distributed application of Elsag company, in order  to assess the best policies for data communication and database allocation before  the concrete implementation. The application consists in remote traffic control  and surveillance over service areas on an Italian motorway, employing automatic  detection and car plate reading at monitored gates. CaseLP allowed to predict  data communication performance statistics under differe...",
    "neighbors": [
      106,
      521,
      1222
    ],
    "mask": "Test"
  },
  {
    "node_id": 486,
    "label": 0,
    "text": "Process- and Agent-Based Modelling Techniques for Dialogue Systems and Virtual Environments This text presents results of ongoing research, which is aimed at developing a framework for developing multimodal natural language dialogue systems operating within virtual environments. The aspects of multimodality and presence in a virtual environment are chosen as the main focus of this research. It may be argued that specification techniques would form the basis of such a framework. Therefore, a general overview and evaluation is given of existing specification techniques for interactive systems, based on both literature and previous research results. This includes the object-oriented model, process algebras, interactor models, and agent systems. Agent systems are further subdivided into intentional logics, production rule systems, agent communication languages, agent platforms, and agent architectures. A new agent system is proposed, which is based on update notification mechanisms as found in interactor models, and the `facilitator' function as found in some agent platfo...",
    "neighbors": [
      941
    ],
    "mask": "Train"
  },
  {
    "node_id": 487,
    "label": 3,
    "text": "Temporal Objects for Spatio-Temporal Data Models and a Comparison of Their Representations Abstract: Currently, there are strong efforts to integrate spatial and temporal database technology into spatio-temporal database systems. This paper views the topic from a rather fundamental perspective and makes several contributions. First, it reviews existing temporal and spatial data models and presents a completely new approach to temporal data modeling based on the very general notion of temporal object. The definition of temporal objects is centered around the observation that anything that changes over time can be expressed as a function over time. For the modeling of spatial objects the well known concept of spatial data types is employed. As specific subclasses, linear temporal and spatial objects are identified. Second, the paper proposes the database embedding of temporal objects by means of the abstract data type (ADT) approach to the integration of complex objects into databases. Furthermore, we make statements about the expressiveness of different temporal and spatial database embeddings. Third, we consider the combination of temporal and spatial objects into spatio-temporal objects in (relational) databases. We explain various alternatives for spatio-temporal data models and databases and compare their expressiveness. Spatio-temporal objects turn out to be specific instances of temporal objects. 1",
    "neighbors": [
      151
    ],
    "mask": "Train"
  },
  {
    "node_id": 488,
    "label": 3,
    "text": "On Supporting Containment Queries in Relational Database Management Systems Virtually all proposals for querying XML include a class of query we term \u201ccontainment queries\u201d. It is also clear that in the foreseeable future, a substantial amount of XML data will be stored in relational database systems. This raises the question of how to support these containment queries. The inverted list technology that underlies much of Information Retrieval is well-suited to these queries, but should we implement this technology (a) in a separate loosely-coupled IR engine, or (b) using the native tables and query execution machinery of the RDBMS? With option (b), more than twenty years of work on RDBMS query optimization, query execution, scalability, and concurrency control and recovery immediately extend to the queries and structures that implement these new operations. But all this will be irrelevant if the performance of option (b) lags that of (a) by too much. In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under certain conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementation in an RDBMS can support this class of query much more efficiently.",
    "neighbors": [
      306,
      420,
      634,
      1162
    ],
    "mask": "Test"
  },
  {
    "node_id": 489,
    "label": 4,
    "text": "ARQuake: An Outdoor/Indoor Augmented Reality First Person Application This pap er presents an outdoor/indoor augmented re- ality first person applic ationAR(2uake we have developal. ARQuake is an extension of the desktop game Quake, and as such we are investigating how to convert a desktop first person application into an outdoor/indoor mobile augmented reality application. We present an archire  cture for a low cost, moderately accurate six degrees of freedom tracking system based on GP$, digital compass, and fiducial vision-based tracking. Usability issues  such as monster selection, colour, and input devies are  discussed. A second application for AR architectural design visualisation is presented.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 490,
    "label": 2,
    "text": "Exploiting Geographical Location Information of Web Pages Many information resources on the web are relevant primarily to limited geographical communities.  For instance, web sites containing information on restaurants, theaters, and apartment  rentals are relevant primarily to web users in geographical proximity to these locations.  In contrast, other information resources are relevant to a broader geographical community. For  instance, an on-line newspaper may be relevant to users across the United States. Unfortunately,  the geographical scope of web resources is largely ignored by web search engines. We  make the case for identifying and exploiting the geographical location information of web sites  so that web search engines can rank resources in a geographically sensitive fashion, in addition  to using more traditional information-retrieval strategies. In this paper, we first consider how  to compute the geographical location of web pages. Subsequently, we consider how to exploit  such information in one specific \"proof-of-concept\" appl...",
    "neighbors": [
      1000,
      1017,
      1104
    ],
    "mask": "Train"
  },
  {
    "node_id": 491,
    "label": 4,
    "text": "MRML: Towards an extensible standard for multimedia querying and benchmarking In recent years, the need for databases which query multimedia data by content has  become apparent. Many commercial and non--commercial research groups are trying  to fulfill these needs.  The development of research can be described as moving in two directions  ffl search for new, useful query and interaction paradigms  ffl deeper research to improve the performance of systems that have adopted a given  query paradigm.  The search for new better performance given a query paradigm has led to \"clusters\" of  systems which are similar in their interaction with the user, and which give a certain  set of interaction capabilities to the user.  It is already visible, that research will move towards systems which enable the user  to formulate multi--paradigm queries in order to further improve results.  As a consequence of the above, there is the need for  ffl A common mechanism for shipping multi--paradigm queries and their results ,  which assures that the right query processor processes th...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 492,
    "label": 2,
    "text": "Alerting Services in a Digital Library Environment The classical paradigm of finding information in the WWW by initiating  retrieval and browsing becomes more and more ineffective. Other techniques  have to be considered. Automatic delivery of contents to the user according  to their needs and filtered by her profile of interests is required. Current  implementations of such Alerting Services at content providers side have several  drawbacks. In my research project I evaluate methods and techniques for  Alerting Services with special respect to the area of digital libraries. I intend  to provide a framework that supports design decisions in building alerting services  depending on the infrastructure and desired system parameters.  1 Introduction  Imagine one morning you just arrive at your office and switch on your computer to have a look at the recent news in your special field of research. Little pictures for each topic tell you that some interesting documents arrived. Behind one icon you find for instance the new announcements for c...",
    "neighbors": [
      314
    ],
    "mask": "Validation"
  },
  {
    "node_id": 493,
    "label": 3,
    "text": "Optimizing Queries with Object Updates Object-oriented databases (OODBs) provide powerful data abstractions and modeling facilities but they usually lack a suitable framework for query processing and optimization. Even though there is an increasing number of recent proposals on OODB query optimization, only few of them are actually focused on query optimization in the presence of object identity and destructive updates, features often supported by most realistic OODB languages. This paper presents a formal framework for optimizing object-oriented queries in the presence of side effects. These queries may contain object updates at any place and in any form. We present a language extension to the monoid comprehension calculus to express these object-oriented features and we give a formal meaning to these extensions. Our method is based on denotational semantics, which is often used to give a formal meaning to imperative programming languages. The semantics of our language extensions is expressed in terms of our monoid calculu...",
    "neighbors": [
      302,
      397,
      1047
    ],
    "mask": "Train"
  },
  {
    "node_id": 494,
    "label": 2,
    "text": "Using Text Elements by Context to Display Search Results in Information Retrieval Systems - Model and Research Results Information retrieval systems display search results by various methods. This paper focuses on a model for displaying a list of search results by means of textual elements that utilize a new information unit that replaces the currently used information unit. The paper includes a short description of several studies that support the model.  1. Introduction  Because of the growth in the number and scope of global databases, a special approach is required to locating information, from the perspective of the user interface. The Internet, as it exists today, is an outstanding example of a broad-base, unfocused database. Most Internet search engines display their information as a serially ordered list of results (with a partial attempt at ranking the results). In most cases, this list includes the document title, URL and, at times, the first few lines of the document. The information, as currently displayed to the user, is incomplete and insufficiently focused on the search query. This requi...",
    "neighbors": [
      1032
    ],
    "mask": "Train"
  },
  {
    "node_id": 495,
    "label": 0,
    "text": "Environment Centered Analysis and Design of Coordination Mechanisms Environment Centered Analysis and Design of Coordination Mechanisms  May 1995 KEITH S. DECKER B.S., Carnegie Mellon University M.S., Rensselaer Polytechnic Institute Ph.D., University of Massachusetts Amherst Directed by: Professor Victor R. Lesser Committee: Professor Paul R. Cohen Professor John A. Stankovic Professor Douglas L. Anderton Coordination, as the act of managing interdependencies between activities, is one of the central research issues in Distributed Artificial Intelligence. Many researchers have shown that there is no single best organization or coordination mechanism for all environments. Problems in coordinating the activities of distributed intelligent agents appear in many domains: the control of distributed sensor networks; multi-agent scheduling of people and/or machines; distributed diagnosis of errors in local-area or telephone networks; concurrent engineering; `software agents' for information gathering. The design of coordination mechanisms for groups of compu...",
    "neighbors": [
      200,
      434,
      441,
      513,
      847,
      1119
    ],
    "mask": "Validation"
  },
  {
    "node_id": 496,
    "label": 2,
    "text": "Context in Web Search Web search engines generally treat search requests  in isolation. The results for a given query  are identical, independent of the user, or the context  in which the user made the request. Nextgeneration  search engines will make increasing  use of context information, either by using explicit  or implicit context information from users, or by  implementing additional functionality within restricted  contexts. Greater use of context in web  search may help increase competition and diversity  on the web.",
    "neighbors": [
      224,
      279,
      347,
      502,
      587,
      595,
      627,
      696,
      774,
      845,
      855,
      931,
      1000,
      1003,
      1017,
      1104,
      1207,
      1233
    ],
    "mask": "Test"
  },
  {
    "node_id": 497,
    "label": 4,
    "text": "Symbiotic Interfaces For Wearable Face Recognition We introduce a wearable face detection method that exploits constraints in face scale and orientation imposed by the proximity of participants in near social interactions. Using this method we describe a wearable system that perceives \u201csocial engagement,\u201d i.e., when the wearer begins to interact with other individuals. One possible application is improving the interfaces of portable consumer electronics, such as cellular phones, to avoid interrupting the user during face-to-face interactions. Our experimental system proved> 90 % accurate when tested on wearable video data captured at a professional conference. Over three hundred individuals were captured, and the data was separated into independent training and test sets. A goal is to incorporate user interface in mobile machine recognition systems to improve performance. The user may provide real-time feedback to the system or may subtly cue the system through typical daily activities, such as turning to face a speaker, as to when conditions for recognition are favorable. 1",
    "neighbors": [
      307,
      334,
      665,
      738,
      987
    ],
    "mask": "Train"
  },
  {
    "node_id": 498,
    "label": 3,
    "text": "First-Order Queries On Finite Structures Over The Reals We investigate properties of finite relational structures over the  reals expressed by first-order sentences whose predicates are the relations  of the structure plus arbitrary polynomial inequalities, and  whose quantifiers can range over the whole set of reals. In constraint  programming terminology, this corresponds to Boolean real polynomial  constraint queries on finite structures. The fact that quantifiers  range over all reals seems crucial; however, we observe that each sentence  in the first-order theory of the reals can be evaluated by letting  each quantifier range over only a finite set of real numbers without  changing its truth value. Inspired by this observation, we then show  that when all polynomials used are linear, each query can be expressed  uniformly on all finite structures by a sentence of which the quantifiers  range only over the finite domain of the structure. In other words,  linear constraint programming on finite structures can be reduced to  ordinary query evaluation as usual in finite model theory and databases.  Moreover, if only \"generic\" queries are taken into consideration,  we show that this can be reduced even further by proving that such  Dept. Math. & Computer Sci., University of Antwerp (UIA), Universiteitsplein 1, B-2610 Antwerp, Belgium. E-mail: pareda@uia.ac.be.  y  Dept. WNI, University of Limburg (LUC), B-3590 Diepenbeek, Belgium. E-mail: vdbuss@luc.ac.be.  z  Computer Science Department, Indiana University, Bloomington, IN 47405-4101, USA. E-mail: vgucht@cs.indiana.edu.  1  queries can be expressed by sentences using as polynomial inequalities  only those of the simple form x ! y.  1",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 499,
    "label": 0,
    "text": "An Architecture for Mobile BDI Agents BDI (Belief, Desire, Intention) is a mature and commonly adopted architecture for Intelligent Agents. BDI Agents are autonomous entities able to work in teams and react to changing environmental conditions. However, the current computational model adopted by BDI has problems which, amongst other limitations, prevent the development of mobile agents. In this paper, we discuss an architecture,  TOMAS (Transaction Oriented Multi Agent System), that addresses these issues by combining BDI and the distributed nested transaction paradigms. An algorithm is presented which enable agents in TOMAS to become mobile.  1 Introduction  Intelligent Agents are a very active area of AI research [WJ95] [Sho93]. Of the various agent architectures which have been proposed, BDI (Belief, Desire, Intention) [RG92] is probably the most mature and has been adopted by a few industrial applications. BDI Agents are autonomous entities able to work in teams and react to changing environmental conditions. Mobile m...",
    "neighbors": [
      808,
      816
    ],
    "mask": "Train"
  },
  {
    "node_id": 500,
    "label": 5,
    "text": "Learning Quantitative Knowledge for Multiagent Coordination A central challenge of multiagent coordination is reasoning  about how the actions of one agent affect the  actions of another. Knowledge of these interrelationships  can help coordinate agents --- preventing conflicts  and exploiting beneficial relationships among actions.  We explore three interlocking methods that  learn quantitative knowledge of such non-local effects  in TAEMS, a well-developed framework for multiagent  coordination. The surprising simplicity and effectiveness  of these methods demonstrates how agents can  learn domain-specific knowledge quickly, extending the  utility of coordination frameworks that explicitly represent  coordination knowledge.  Introduction  A major challenge of designing effective multiagent systems is managing non-local effects --- situations where the actions of one agent impact the performance of other agents' actions. For example, one agent's action can enable, disable, facilitate, or hinder the actions of other agents. Poor accounting for ...",
    "neighbors": [
      434,
      441,
      1119
    ],
    "mask": "Train"
  },
  {
    "node_id": 501,
    "label": 3,
    "text": "Design and Implementation of the ROL System ROL is a deductive object-oriented database system developed at the University of Regina. It eectively integrates important features of deductive databases and object-oriented databases in a uniform framework and provides a uniform rule-based declarative language for dening, manipulating and querying a database. This paper describes the latest implementation of ROL.  1 Introduction  In the past decade a lot of interests arose in integrating deductive and object-oriented databases to gain the best of the two approaches such as recursion, declarative querying, and rm logical foundations from deductive approaches, and object identity, complex objects, classes, class hierarchy, property inheritance with overriding and schema from object-oriented approach. A number of deductive object-oriented database languages have been proposed, such as O-logic [17], revised O-logic [11], C-logic [8], IQL [2], IQL2[1], Flogic [10], LOGRES [7], LLO [16], LOL [6], CORAL++[19], Datalog  method  [3], DLT ...",
    "neighbors": [
      192,
      861
    ],
    "mask": "Train"
  },
  {
    "node_id": 502,
    "label": 2,
    "text": "STARTS: Stanford Proposal for Internet Meta-Searching Document sources are available everywhere, both within the internal networks of organizations and on the Internet. Even individual organizations use search engines from different vendors to index their internal document collections. These search engines are typically incompatible in that they support different query models and interfaces, they do not return enough information with the query results for adequate merging of the results, and finally, in that they do not export metadata about the collections that they index (e.g., to assist in resource discovery). This paper describes STARTS,  an emerging protocol for Internet retrieval and search that facilitates the task of querying multiple document sources.  STARTS has been developed in a unique way. It is not a standard, but a group effort coordinated by Stanford's Digital Library project, and involving over 11 companies and organizations. The objective of this paper is not only to give an overview of the STARTS protocol proposal, but...",
    "neighbors": [
      241,
      271,
      404,
      496,
      526,
      792,
      1124
    ],
    "mask": "Train"
  },
  {
    "node_id": 503,
    "label": 3,
    "text": "An Efficient Index Structure for OID Indexing in Parallel Temporal Object-Oriented Database Systems . In an object-oriented database system based on logical OIDs, an OID index (OIDX) is necessary to convert from logical OID to physical location. In a temporal objectoriented database system (TOODB), this OIDX also contains the timestamps of the object versions. We have in a previous paper studied OIDX performance with a relatively simple index. The studies have shown that OIDX maintenance can be quite costly, especially objects updates, because in a temporal OODB, the OIDX needs to be updated every time an object is updated. This has convinced us that a new index structure, particularly suitable to TOODB requirements, is necessary. In this report, we describe an efficient OID index structure for TOODBs, which we call The Vagabond Temporal OID Index (VTOIDX). The main goals of the VTOIDX is 1) support for temporal data, while still having index performance close to a non-temporal/one version database system, 2) efficient object-relational operation, and 3) easy tertiary storage migrati...",
    "neighbors": [
      630,
      760
    ],
    "mask": "Train"
  },
  {
    "node_id": 504,
    "label": 1,
    "text": "Simultaneous Learning of Negatively Correlated Neural Networks A new approach to designing neural network ensembles has been proposed recently [1]. Experimental studies on some regression tasks have shown that the new approach performs significantly better than previous ones [1]. This paper presents a new algorithm for designing neural network ensembles for classification problems with noise. This new algorithm is different from that used for regression tasks although the idea is similar. The idea behind this new algorithm is to encourage different individual networks in an ensemble to learn different parts or aspects of the training data so that the whole ensemble can learn the whole training data better. Negatively correlated networks are trained with a novel correlation penalty term in the error function to encourage such specialisation. In our algorithm, individual networks are trained simultaneously rather than sequentially. This provides an opportunity for different networks to interact with other and to specialise. Experiments on two real-w...",
    "neighbors": [
      108
    ],
    "mask": "Train"
  },
  {
    "node_id": 505,
    "label": 1,
    "text": "Unsupervised Learning from Dyadic Data Dyadic data refers to a domain with two finite sets of objects in which observations are made for dyads, i.e., pairs with one element from either set. This includes event co-occurrences, histogram data, and single stimulus preference data as special cases. Dyadic data arises naturally in many applications ranging from computational linguistics and information retrieval to preference analysis and computer vision. In this paper, we present a systematic, domain-independent framework for unsupervised learning from dyadic data by statistical mixture models. Our approach covers different models with flat and hierarchical latent class structures and unifies probabilistic modeling and structure discovery. Mixture models provide both, a parsimonious yet flexible parameterization of probability distributions with good generalization performance on sparse data, as well as structural information about data-inherent grouping structure. We propose an annealed version of the standard Expectation Maximization algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains.",
    "neighbors": [
      50,
      437,
      592,
      722,
      1141
    ],
    "mask": "Train"
  },
  {
    "node_id": 506,
    "label": 3,
    "text": "Fast Approximate Evaluation of OLAP Queries for Integrated Statistical Data We have developed a mediator architecture that integrates statistical information about energy products  from several government agencies, such as the Bureau of Labor Statistics, the Energy Information  Administration, and the California Energy Commission. Our architecture has a dual mode of operation.  First, our system can retrieve live data from databases and web sources from these agencies. This allows  the users to obtain completely up-to-date data. However, for complex analytical queries that typically  require large amounts of data and processing, live access does not offer the level of interactivity that  some users require. Second, our system can warehouse the information from the data sources to allow  for complex analytical queries to be executed much more efficiently. However, the data would be only  as recent as the last update to the data warehouse. In this paper we describe the architecture and focus on  how to perform analytical queries against the data warehouse very efficiently. We present results using  a fast wavelet-based technique for progressive evaluation of range-sum queries. This technique allows  for returning an approximate result to the query very efficiently and for fast convergence to the exact  result. We envision users exploring many complex queries using the very fast approximate results as  guidance and only obtaining the exact results for those queries that are deemed of interest. We present  experimental results showing the efficiency of both approximate and exact queries.  1",
    "neighbors": [
      392
    ],
    "mask": "Test"
  },
  {
    "node_id": 507,
    "label": 2,
    "text": "Towards Efficient Multi-Feature Queries in Heterogeneous Environments Applications like multimedia databases or enterprisewide information management systems have to meet the challenge of efficiently retrieving best matching objects from vast collections of data. We present a new algorithm Stream-Combine for processing multi-feature queries on heterogeneous data sources. Stream-Combine is selfadapting to different data distributions and to the specific kind of the combining function. Furthermore we present a new retrieval strategy that will essentially speed up the output of relevant objects.",
    "neighbors": [
      555,
      1144,
      1145
    ],
    "mask": "Train"
  },
  {
    "node_id": 508,
    "label": 0,
    "text": "Spatial Agents Implemented in a Logical Expressible Language In this paper, we present a multi-layered architecture  for spatial and temporal agents. The focus is  laid on the declarativity of the approach, which  makes agent scripts expressive and well understandable.  They can be realized as (constraint) logic  programs. The logical description language is able  to express actions or plans for one and more autonomous  and cooperating agents for the RoboCup  (Simulator League). The system architecture hosts  constraint technology for qualitative spatial reasoning,  but quantitative data is taken into account, too.  The basic (hardware) layer processes the agent's  sensor information. An interface transfers this lowlevel  data into a logical representation. It provides  facilities to access the preprocessed data and supplies  several basic skills. The second layer performs  (qualitative) spatial reasoning. On top of this, the  third layer enables more complex skills such as  passing, offside-detection etc. At last, the fourth  layer establishes acting as a team both by emergent  and explicit cooperation. Logic and deduction provide  a clean means to specify and also to implement  teamwork behavior.  1",
    "neighbors": [
      184,
      363
    ],
    "mask": "Train"
  },
  {
    "node_id": 509,
    "label": 1,
    "text": "Learning human arm movements by imitation: Evaluation of a biologically-inspired connectionist architecture . This paper is concerned with the evaluation of a model of human imitation of arm movements. The  model consists of a hierarchy of artificial neural networks, which are abstractions of brain regions involved in  visuo-motor control. These are the spinal cord, the primary and pre-motor cortexes (M1 & PM), the cerebellum,  and the temporal cortex. A biomechanical simulation is developed which models the muscles and the complete  dynamics of a 37 degree of freedom humanoid. Input to the model are data from human arm movements  recorded using video and marker-based tracking systems.  The model's performance is evaluated for reproducing reaching movements and oscillatory movements of the  two arms. Results show a high qualitative and quantitative agreement with human data. In particular, the model  reproduces the well known features of reaching movements in humans, namely the bell-shaped curves for the  velocity and quasi-linear hand trajectories. Finally, the model's performance is compar...",
    "neighbors": [
      1072,
      1143
    ],
    "mask": "Test"
  },
  {
    "node_id": 510,
    "label": 2,
    "text": "Detection of Heterogeneities in a Multiple Text Database Environment As the number of text retrieval systems (search engines) grows rapidly on the World Wide Web, there is an increasing need to build search brokers (metasearch engines) on top of them. Often, the task of building an effective and efficient metasearch engine is hindered by the heterogeneities among the underlying local search engines. In this paper, we first analyze the impact of various heterogeneities on building a metasearch engine. We then present some techniques that can be used to detect the most prominent heterogeneities among multiple search engines. Applications of utilizing the detected heterogeneities in building better metasearch engines will be provided.",
    "neighbors": [
      115,
      241,
      271,
      433,
      526,
      579,
      696,
      792,
      931,
      1003,
      1017,
      1253
    ],
    "mask": "Train"
  },
  {
    "node_id": 511,
    "label": 2,
    "text": "Probabilistic Question Answering on the Web Web-based search engines such as Google and NorthernLight return documents that are relevant to a user query, not answers to user questions. We have developed an architecture that augments existing search engines so that they support natural language question answering. The process entails five steps: query modulation, document retrieval, passage extraction, phrase extraction, and answer ranking. In this paper we describe some probabilistic approaches to the last three of these stages. We show how our techniques apply to a number of existing search en-1 Radev et al. 2 gines and we also present results contrasting three different methods for question answering. Our algorithm, probabilistic phrase reranking (PPR), uses proximity and question type features and achieves a total reciprocal document rank of.20 on the TREC8 corpus. Our techniques have been implemented as a Web-accessible system, called NSIR.",
    "neighbors": [
      595,
      653
    ],
    "mask": "Train"
  },
  {
    "node_id": 512,
    "label": 1,
    "text": "Using Case-Based Reasoning for Supporting Continuous Improvement Processes The goal of the IPQM project -- a collaboration of the Fraunhofer Institute for Manufacturing Engineering and Automation (IPA) in Stuttgart and the Fraunhofer Institute for Experimental Software Engineering (IESE) in  Kaiserslautern -- is to develop a technical infrastructure to support continuous improvement processes. We  describe the approach we took in some detail and focus on the implementation of the IPQM system and its currently ongoing evaluation in the healthcare sector. We also give an outlook on intended extensions of the  system and its application in other domains.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 513,
    "label": 0,
    "text": "Achieving Coordination through Combining Joint Planning and Joint Learning . There are two major approaches to activity coordination in multiagent systems. First, by endowing the agents with the capability to jointly plan, that is, to jointly generate hypothetical activity sequences. Second, by endowing the agents with the capability to jointly learn, that is, to jointly choose the actions to be executed on the basis of what they know from experience about the interdependencies of their actions. This paper describes a new algorithm called JPJL (\"Joint Planning and Joint Learning\") that combines both approaches. The primary motivation behind this algorithm is to bring together the advantages of joint planning and joint learning while avoiding their disadvantages. Experimental results are provided that illustrate the potential benefits and shortcomings of the JPJL algorithm. 1 Motivation  Multiagent Systems (MAS)---systems in which several interacting, intelligent and autonomous entities called agents pursue some set of goals or perform some set of tasks---have...",
    "neighbors": [
      495,
      945
    ],
    "mask": "Train"
  },
  {
    "node_id": 514,
    "label": 1,
    "text": "Guided Crossover: A New Operator for Genetic Algorithm Based Optimization Genetic algorithms (GAs) have been extensively used in different domains as a means of doing global optimization in a simple yet reliable manner. They have a much better chance of getting to global optima than gradient based methods which usually converge to local sub optima. However, GAs have a tendency of getting only moderately close to the optima in a small number of iterations. To get very close to the optima, the GA needs a very large number of iterations. Whereas gradient based optimizers usually get very close to local optima in a relatively small number of iterations. In this paper we describe a new crossover operator which is designed to endow the GA with gradient-like abilities without actually computing any gradients and without sacrificing global optimality. The operator works by using guidance from all members of the GA population to select a direction for exploration. Empirical results in two engineering design domains and across both binary and floating point representa...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 515,
    "label": 0,
    "text": "Jinni: Intelligent Mobile Agent Programming at the Intersection of Java and Prolog . Jinni (Java INference engine and Networked Interactor), is a lightweight, multi-threaded, logic programming language, intended to be used as a flexible scripting tool for gluing together knowledge processing components and Java objects in distributed applications. Jinni threads are coordinated through blackboards, local to each process. Associative search based on term unification (a variant of Linda) is used as the basic synchronization mechanism. Threads are controlled with tiny interpreters following a scripting language based on a subset of Prolog. Mobile threads, implemented by capturing first order continuations in a compact data structure sent over the network, allow Jinni to interoperate with remote high performance BinProlog servers for CPU-intensive knowledge processing and with other Jinni components over the Internet. The synergy of these features makes Jinni a convenient development platform for distributed AI, and in particular, for building intelligent autonomous agent...",
    "neighbors": [
      532
    ],
    "mask": "Train"
  },
  {
    "node_id": 516,
    "label": 0,
    "text": "Modelling and Design of Multi-Agent Systems Abstract. Agent technologies are now being applied to the development of large-scale commercial and industrial software systems. Such systems are complex, involving hundreds, perhaps thousands of agents, and there is a pressing need for system modelling techniques that permit their complexity to be e ectively managed, and principled methodologies to guide the process of system design. Without adequate techniques to support the design process, such systems will not be su ciently reliable, maintainable or extensible, will be di cult to comprehend, and their elements will not be re-usable. In this paper, we present techniques for modelling agents and multi-agent systems which adapt and extend existing Object-Oriented representation techniques, and a methodology which provides a clear conceptual framework to guide system design and speci cation. We have developed these techniques for systems of agents based upon a particular Belief-Desire-Intention architecture, but have soughttoprovide a framework for the description of agent systems that is su ciently general to be applicable to other agent architectures, and which may be extended in various ways. 1",
    "neighbors": [
      522,
      580,
      941,
      957
    ],
    "mask": "Train"
  },
  {
    "node_id": 517,
    "label": 2,
    "text": "Automatic Text Summarization of Multiple Documents Scientists have retrieved what appear to be normal human eggs from human ovarian tissue that was grafted onto research mice. This is the first research group to obtain mature, potentially fertilizable eggs. Results of the research are being presented today at the conference of the European Society of Human Reproduction and Embryology. A report published last year demonstrated that ovarian tissue which was frozen and then replaced into a woman's body resulted in ovulation and menstruation. Such methods are being considered for women being treated for cancer with methods that would severely diminish or destroy their reproductive chances. However, there is concern that the retransplanted tissue might contain cancer cells. The current study proposes to reduce that risk. This is yet another step toward enabling women to freeze ovarian tissue in their early 20's, when it is generally most productive, to delay reproduction until their later years.",
    "neighbors": [
      548,
      695
    ],
    "mask": "Train"
  },
  {
    "node_id": 518,
    "label": 3,
    "text": "Accurate Estimation of the Cost of Spatial Selections Optimizing queries that involve operations on spatial data requires estimating the selectivity and cost of these operations. In this paper, we focus on estimating the cost of spatial selections, or window queries, where the query windows and data objects are general polygons. Cost estimation techniques previously proposed in the literature only handle rectangular query windows over rectangular data objects, thus ignoring the very significant cost of exact geometry comparison (the refinement step in a \u201cfilter and refine\u201d query processing strategy). The cost of the exact geometry comparison depends on the selectivity of the filtering step and the average number of vertices in the candidate objects identified by this step. In this paper, we introduce a new type of histogram for spatial data that captures the complexity and size of the spatial objects as well as their location. Capturing these attributes makes this type of histogram useful for accurate estimation, as we experimentally demonstrate. We also investigate sampling-based estimation approaches. Sampling can yield better selectivity estimates than histograms for polygon data, but at the high cost of performing exact geometry comparisons for all the sampled objects. 1.",
    "neighbors": [
      992
    ],
    "mask": "Train"
  },
  {
    "node_id": 519,
    "label": 2,
    "text": "Programming by Demonstration for Information Agents this article we will refer to the user in the female form, while the agent will be referred to using male forms.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 520,
    "label": 4,
    "text": "Smart-Its Friends: A Technique for Users to Easily Establish Connections between Smart Artefacts . Ubiquitous computing is associated with a vision of everything being  connected to everything. However, for successful applications to emerge, it will  not be the quantity but the quality and usefulness of connections that will  matter. Our concern is how qualitative relations and more selective connections  can be established between smart artefacts, and how users can retain control  over artefact interconnection. We propose context proximity for selective  artefact communication, using the context of artefacts for matchmaking. We  further suggest to empower users with simple but effective means to impose the  same context on a number of artefacts. To prove our point we have  implemented Smart-Its Friends, small embedded devices that become  connected when a user holds them together and shakes them.  1",
    "neighbors": [
      701,
      869,
      1226
    ],
    "mask": "Train"
  },
  {
    "node_id": 521,
    "label": 0,
    "text": "CaseLP, A Rapid Prototyping Environment For Agent Based Software Intelligent agents and multi-agent systems are increasingly recognized as an innovative approach for analyzing, designing and implementing complex, heterogeneous and distributed software applications. The agent-based view offers a powerful and high level conceptualization that software engineers can exploit to considerably improve the way in which software is realized. Agent-based software engineering is a recent and very interesting research area. Due to its novelty, there is still no evidence of well-established practices for the development of agent-based applications and thus experimentation in this direction is very important. This dissertation",
    "neighbors": [
      485,
      573,
      884,
      957,
      1067
    ],
    "mask": "Test"
  },
  {
    "node_id": 522,
    "label": 0,
    "text": "Agent-Oriented Software Engineering Agent-oriented techniques represent an exciting new means of analysing, designing and building complex  software systems. They have the potential to significantly improve current practice in software  engineering and to extend the range of applications that can feasibly be tackled. Yet, to date, there have  been few serious attempts to cast agent systems as a software engineering paradigm. This paper seeks to  rectify this omission. Specifically, it will be argued that: (i) the conceptual apparatus of agent-oriented  systems is well-suited to building software solutions for complex systems and (ii) agent-oriented  approaches represent a genuine advance over the current state of the art for engineering complex systems.  Following on from this view, the major issues raised by adopting an agent-oriented approach to  software engineering are highlighted and discussed.",
    "neighbors": [
      264,
      516,
      964
    ],
    "mask": "Train"
  },
  {
    "node_id": 523,
    "label": 2,
    "text": "Discriminant-EM Algorithm with Application to Image Retrieval In many vision applications, the practice of supervised learning faces several difficulties, one of which is that insufficient labeled training data result in poor generalization. In image retrieval, we have very few labeled images from query and relevance feedback so that it is hard to automatically weight image features and select similarity metrics for image classification. This paper investigates the possibility of including an unlabeled data set to make up the insufficiency of labeled data. Different from most current research in image retrieval, the proposed approach tries to cast image retrieval as a transductive learning problem, in which the generalization of an image classifier is only defined on a set of images such as the given image database. Formulating this transductive problem in a probabilistic framework, the proposed algorithm, Discriminant-EM (D-EM), not only estimates the parameters of a generative model, but also finds a linear transformation to relax the assumption of pro...",
    "neighbors": [
      100,
      391,
      609,
      824,
      1133,
      1153,
      1203
    ],
    "mask": "Test"
  },
  {
    "node_id": 524,
    "label": 2,
    "text": "Combining Collaborative Filtering with Personal Agents for Better Recommendations Information filtering agents and collaborative filtering both  attempt to alleviate information overload by identifying  which items a user will find worthwhile. Information  filtering (IF) focuses on the analysis of item content and  the development of a personal user interest profile.  Collaborative filtering (CF) focuses on identification of  other users with similar tastes and the use of their opinions  to recommend items. Each technique has advantages and  limitations that suggest that the two could be beneficially  combined.  This paper shows that a CF framework can be used to  combine personal IF agents and the opinions of a  community of users to produce better recommendations  than either agents or users can produce alone. It also  shows that using CF to create a personal combination of a  set of agents produces better results than either individual  agents or other combination mechanisms. One key  implication of these results is that users can avoid having  to select among ag...",
    "neighbors": [
      937,
      1068
    ],
    "mask": "Test"
  },
  {
    "node_id": 525,
    "label": 2,
    "text": "Using Software Agents to Support Evolution of Distributed Workflow Models This paper outlines a high-level design of how software agents can be used combined with an existing CAGIS Process Centred Environment to deal with evolution of distributed, fragmented workflow models. Our process centred environment allows process fragments of the same workflow model to be located in workspaces that are geographically distributed. These process fragments can be changed independently in local workspaces causing consistency problems. We propose to use software mobile agents, offering awareness services solving conflicting updates of process fragment. Our solution is illustrated using some scenarios.  Keywords: Process centred environments, software agents, workflow model consistency, workflow model evolution, distribution, fragmentation.  1 Introduction  Dealing with evolution of workflow processes is not a trivial matter. One simple solution to this problem is to have one centralised workflow model, that cannot be changed after it is instanciated. In practice, it is ho...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 526,
    "label": 2,
    "text": "Building Efficient and Effective Metasearch Engines Frequently a user's information needs are stored in the databases of multiple search engines. It is inconvenient and inefficient for an ordinary user to invoke multiple search engines and identify useful documents from the returned results. To support unified access to multiple search engines, a metasearch engine can be constructed. When a metasearch engine receives a query from a user, it invokes the underlying search engines to retrieve useful information for the user. Metasearch engines have other benefits as a search tool such as increasing the search coverage of the Web and improving the scalability of the search. In this article, we survey techniques that have been proposed to tackle several underlying challenges for building a good metasearch engine. Among the main challenges, the database selection problem is to identify search engines that are likely to return useful documents to a given query. The document selection problem is to determine what documents to retrieve from each identified search engine. The result merging problem is to combine the documents returned from multiple search engines. We will also point out some problems that need to be further researched.",
    "neighbors": [
      216,
      224,
      241,
      271,
      477,
      502,
      510,
      579,
      696,
      792,
      931,
      1003,
      1017
    ],
    "mask": "Validation"
  },
  {
    "node_id": 527,
    "label": 3,
    "text": "Knowledge Discovery from Client-Server Databases . The subject of this paper is the implementation of knowledge discovery in databases. Specifically, we assess the requirements for interfacing tools to client-server database systems in view of the architecture of those systems and of \"knowledge discovery processes\". We introduce the concept of a query frontier of an exploratory process, and propose a strategy based on optimizing the current query frontier rather than individual knowledge discovery algorithms. This approach has the advantage of enhanced genericity and interoperability. We demonstrate a small set of query primitives, and show how one example tool, the well-known decision tree induction algorithm C4.5, can be rewritten to function in this environment. 1 Introduction  Relational databases are the current dominant database technology in industry, and many organizations have collected large amounts of data in so-called  data warehouses expressly for the purpose of decision support and data mining. In general the data must ...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 528,
    "label": 5,
    "text": "Sensor Fault Detection and Identification in a Mobile Robot Multiple model adaptive estimation (MMAE) is used to detect and identify sensor failures in a mobile robot. Each estimator is a Kalman filter with a specific embedded failure model. The filter bank also contains one filter which has the nominal model embedded within it. The filter residuals are postprocessed to produce a probabilistic interpretation of the operation of the system. The output of the system at any given time is the confidence in the correctness of the various embedded models. As an additional feature the standard assumption that the measurements are available at a constant, common frequency, is relaxed. Measurements are assumed to be asynchronous and of varying frequency. The particularly difficult case of 'soft' sensor failure is also handled successfully. A system architecture is presented for the general problem of failure detection and identification in mobile robots. As an example, the MMAE algorithm is demonstrated on a Pioneer I robot in the case of three different sensor failures.",
    "neighbors": [
      554
    ],
    "mask": "Test"
  },
  {
    "node_id": 529,
    "label": 3,
    "text": "Efficient Support for P-HTTP in Cluster-Based Web Servers This paper studies mechanisms and policies for supporting HTTP/1.1 persistent connections in cluster-based Web servers that employ content-based request distribution. We present two mechanisms for the efficient, content-based distribution of HTTP/1.1 requests among the back-end nodes of a cluster server. A trace-driven simulation shows that these mechanisms, combined with an extension of the locality-aware request distribution (LARD) policy, are effective in yielding scalable performance for HTTP/1.1 requests. We implemented the simpler of these two mechanisms, back-end forwarding. Measurements of this mechanism in connection with extended LARD on a prototype cluster, driven with traces from actual Web servers, confirm the simulation results. The throughput of the prototype is up to four times better than that achieved by conventional weighted round-robin request distribution. In addition, throughput with persistent connections is up to 26% better than without.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 530,
    "label": 3,
    "text": "Integrating Light-Weight Workflow Management Systems within Existing Business Environments Workflow management systems support the efficient, largely au-  tomated execution of business processes. However, using a workflow management system typically requires implementing the  application's control flow exclusively by the workflow management  system. This approach is powerful if the control flow is specified and implemented from scratch, but it has severe drawbacks if a workflow management system is to be integrated within environments  with existing solutions for implementing control flow. Usual-  ly, the existing solutions are too complex to be substituted by the workflow management system at once. Hence, the workflow management system must support an incremental integration, i.e. the reuse of existing implementations of control flow as well as their in-  cremental substitution. Extending the workflow management system's functionality ac-  cording to future application needs, e.g. by worklist and history management, must also be possible. In particular, at the beginning  of...",
    "neighbors": [
      315
    ],
    "mask": "Train"
  },
  {
    "node_id": 531,
    "label": 4,
    "text": "Realtime Personal Positioning System for Wearable Computers Context awareness is an important functionality for wearable computers. In particular, the computer should know where the person is in the environment. This paper proposes an image sequence matching technique for the recognition of locations and previously visited places. As in single word recognition in speech recognition, a dynamic programming algorithm is proposed for the calculation of the similarity of different locations. The system runs on a stand alone wearable computer such as a Libretto PC. Using a training sequence a dictionary of locations is created automatically. These locations are then be recognized by the system in realtime using a hatmounted camera.  1. Introduction  Obtaining user location is one of the important functions for wearable computers in two applications. One is automatic self-summary, and the other is contextaware user interface. In self-summary, the user is wearing a small camera and a small computer, capturing and recording every event of his/her daily ...",
    "neighbors": [
      307,
      680,
      738,
      1129
    ],
    "mask": "Validation"
  },
  {
    "node_id": 532,
    "label": 0,
    "text": "Computational Logic and Multi-Agent Systems: a Roadmap Agent-based computing is an emerging computing paradigm that has  proved extremely successful in dealing with a number of problems arising  from new technological developments and applications. In this paper we  report the role of computational logic in modeling intelligent agents, by  analysing existing agent theories, agent-oriented programming languages  and applications, as well as identifying challenges and promising directions  for future research.  1 Introduction  In the past ten years the eld of agent-based computing has emerged and greatly expanded, due to new technological developments such as ever faster and cheaper computers, fast and reliable interconnections between them as well as the emergence of the world wide web. These developments have at the same time opened new application areas, such as electronic commerce, and posed new problems, such as that of integrating great quantities of information and building complex software, embedding legacy code. The establishment o...",
    "neighbors": [
      288,
      515,
      588,
      1236
    ],
    "mask": "Validation"
  },
  {
    "node_id": 533,
    "label": 1,
    "text": "Simulating the Evolution of 2D Pattern Recognition on the CAM-Brain Machine, an Evolvable Hardware Tool for Building a 75 Million Neuron Artificial Brain This paper presents some simulation results of the evolution of 2D visual pattern recognizers to be implemented  very shortly on real hardware, namely the \"CAM-Brain Machine\" (CBM), an FPGA based piece of evolvable hardware  which implements a genetic algorithm (GA) to evolve a 3D cellular automata (CA) based neural network circuit  module, of approximately 1,000 neurons, in about a second, i.e. a complete run of a GA, with 10,000s of circuit  growths and performance evaluations. Up to 65,000 of these modules, each of which is evolved with a humanly  specified function, can be downloaded into a large RAM space, and interconnected according to humanly specified  artificial brain architectures. This RAM, containing an artificial brain with up to 75 million neurons, is then updated  by the CBM at a rate of 130 billion CA cells per second. Such speeds will enable real time control of robots and  hopefully the birth of a new research field that we call \"brain building\". The first such artif...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 534,
    "label": 1,
    "text": "Genetic Algorithms Based Systems For Conceptual Engineering Design this paper we try to integrate methods of preferences and scenarios with Genetic Algorithms used to perform multi--objective optimisation. The goal is to make a system that will be able to work together with the designer during the conceptual design phase, where interaction and designer knowledge are sometimes more important than accuracy. MODULE OPTIMISATION CONSTRAINT HANDLING MODULE FUZZY RULES HANDLING MODULE",
    "neighbors": [
      1136
    ],
    "mask": "Train"
  },
  {
    "node_id": 535,
    "label": 3,
    "text": "TEMPOS: A Temporal Database Model Seamlessly Extending ODMG This paper presents Tempos, a set of models and languages intended to seamlessly extend the ODMG object database standard with temporal functionalities. The proposed models exploit object-oriented technology to meet some important, yet traditionally neglected design criteria, related to legacy code migration and representation independence.  Tempos has been fully formalized both at the syntactical and the semantical level and implemented on top of the O 2 DBMS. Its suitability in regard to applications' requirements has been validated through concrete case studies from various contexts.  Keywords: temporal databases, temporal data models, temporal query languages, time representation, upward compatibility, object-oriented databases, ODMG R'esum'e  Ce document pr'esente Tempos : un ensemble de mod`eles et de langages con\u00b8cus pour 'etendre le standard pour Bases de Donn'ees `a objets ODMG, par des fonctionnalit'es temporelles. Les mod`eles d'ecrits exploitent les possibilit'es de la tech...",
    "neighbors": [
      59,
      1058
    ],
    "mask": "Train"
  },
  {
    "node_id": 536,
    "label": 2,
    "text": "Finding Related Pages in the World Wide Web When using traditional search engines, users have to formulate queries to describe their information need. This paper discusses a different approach toweb searching where the input to the search process is not a set of query terms, but instead is the URL of a page, and the output is a set of related web pages. A related web page is one that addresses the same topic as the original page. For example, www.washingtonpost.com is a page related to www.nytimes.com, since both are online newspapers. We describe two algorithms to identify related web pages. These algorithms use only the connectivity information in the web (i.e., the links between pages) and not the content of pages or usage information. We haveimplemented both algorithms and measured their runtime performance. To evaluate the e ectiveness of our algorithms, we performed a user study comparing our algorithms with Netscape's \\What's Related \" service [12]. Our study showed that the precision at 10 for our two algorithms are 73 % better and 51 % better than that of Netscape, despite the fact that Netscape uses both content and usage pattern information in addition to connectivity information.",
    "neighbors": [
      235,
      322,
      578,
      763,
      774,
      867,
      990,
      1000,
      1005,
      1017,
      1099,
      1228
    ],
    "mask": "Train"
  },
  {
    "node_id": 537,
    "label": 1,
    "text": "Evolving Rule-Based Trading Systems In this study, a market trading rulebase is optimised using genetic programming  (GP). The rulebase is comprised of simple relationships between technical  indicators, and generates signals to buy, sell short, and remain inactive. The  methodology is applied to prediction of the Standard & Poor's composite index  (02-Jan-1990 to 18-Oct-2001). Two potential market systems are inferred: a simple  system using few rules and nodes, and a more complex system. Results are  compared with a benchmark buy-and-hold strategy. Neither trading system was  found capable of consistently outperforming this benchmark. More complicated  rulebases, in addition to being difficult to understand, are susceptible to overfitting.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 538,
    "label": 0,
    "text": "Knowledge Base Support For Design And Synthesis Of Multiagent Systems",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 539,
    "label": 3,
    "text": "External Memory Algorithms and Data Structures Data sets in large applications are often too massive to fit completely inside the computer's internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this paper, we survey the state of the art in the design and analysis of external memory algorithms and data structures (which are sometimes referred to as \"EM\" or \"I/O\" or \"out-of-core\" algorithms and data structures). EM algorithms and data structures are often designed and analyzed using the parallel disk model (PDM). The three machine-independent measures of performance in PDM are the number of I/O operations, the CPU time, and the amount of disk space. PDM allows for multiple disks (or disk arrays) and parallel CPUs, and it can be generalized to handle tertiary storage and hierarchical memory. We discuss several important paradigms for how to solve batched and online problems efficiently in external memory. Programming tools and environments are available for simplifying the programming task. The TPIE system (Transparent Parallel I/O programming Environment) is both easy to use and efficient in terms of execution speed. We report on some experiments using TPIE in the domain of spatial databases. The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.",
    "neighbors": [
      929
    ],
    "mask": "Train"
  },
  {
    "node_id": 540,
    "label": 0,
    "text": "Hierarchical Agent Interface for Animation Asynchronous, Hierarchical Agents (AHAs) provide a vertically structured multilevel abstraction hierarchy. In this paper, we argue that this multilevel hierarchy is a convenient way to create a human-agent interface at multiple levels of abstraction. In this way, the agent has several layers of specification (input) and visualization  (output) which facilitates users with problem solving, because such an interface parallels the hierarchical and iterative nature of human creative thought processes. The AHA interface presents an intuitive, intimate interface which supports interactions on a scale from direct manipulation to delegation, depending on the user's choice. Another feature of this interface is its two modes of interaction: direct device interaction (mouse clicking) and interpretive, command line or scripting mode. This way, agents can be \"forced\" to perform certain activities via mouse clicks (direct control), or they can be programmed via scripts on the fly. We present example...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 541,
    "label": 4,
    "text": "Partial Replication in the Vesta Software Repository The Vesta repository is a special-purpose replicated file system, developed as part of the Vesta software configuration management system. One of the major goals of Vesta is to make all software builds reproducible. To this end, the repository provides an append-only name space; new names can be inserted, but once a name exists, its meaning cannot change. More concretely, all files and some designated directories are immutable, while the remaining directories are appendable, allowing new names to be defined but not allowing existing names to be redefined. The data stored",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 542,
    "label": 0,
    "text": "Hierarchical Optimization of Policy-Coupled Semi-Markov Decision Processes One general strategy for approximately solving large Markov decision processes is \"divide-and-conquer\": the original problem is decomposed into sub-problems which interact with each other, but yet can be solved independently by taking into account the nature of the interaction. In this paper we focus on a class of \"policy-coupled\" semi-Markov decision processes (SMDPs), which arise in many nonstationary real-world multi-agent tasks, such as manufacturing and robotics. The nature of the interaction among sub-problems (agents) is more subtle than that studied previously: the components of a sub-SMDP, namely the available states and actions, transition probabilities and rewards, depend on the policies used in solving the \"neighboring\" sub-SMDPs. This \"strongly-coupled\" interaction among subproblems causes the approach of solving each sub-SMDP in parallel to fail. We present a novel approach whereby many variants of each sub-SMDP are solved, explicitly taking into account the different mod...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 543,
    "label": 3,
    "text": "Knowledge Management through Ontologies Most enterprises agree that knowledge is an essential asset for success and survival on a increasingly competitive and global market. This awareness is one of the main reasons for the exponential growth of knowledge management in the past decade. Our approach to knowledge management is based on ontologies, and makes knowledge assets intelligently accessible to people in organizations. Most company-vital knowledge resides in the heads of people, and thus successful knowledge management does not only consider technical aspects, but also social ones. In this paper, we describe an approach to intelligent knowledge management that explicitly takes into account the social issues involved. The proof of concept is given by a large-scale initiative involving knowledge management of a virtual organization. 1 Introduction  According to Information Week (Angus et al., 1998) \"the business problem that knowledge management is designed to solve is that knowledge acquired through experience doesn't ge...",
    "neighbors": [
      347,
      767
    ],
    "mask": "Test"
  },
  {
    "node_id": 544,
    "label": 3,
    "text": "Indexing Techniques for Continuously Evolving Phenomena The management of spatial, temporal, and  spatiotemporal data is becoming increasingly  important in a wide range of applications.  This ongoing Ph.D. project focuses on applications  where spatial or temporal aspects of  objects are continuously changing and there  is a need for indexing techniques that \"track\"  the changing data, even in-between explicit  updates. In spatiotemporal applications, there  is a need to record and efficiently query the  history, the current state, and the predicted  future behavior of continuously moving objects,  such as vehicles, mobile telephones, and  people. Likewise, in temporal applications  and spatiotemporal applications with discrete  change, time intervals may be naturally related  to the current time, which continuously  progresses. The paper outlines the research  agenda of the Ph.D. project and describes  briefly two access methods developed so far  in this project.  1 Introduction  Recent years have shown both an increase in the amounts of ...",
    "neighbors": [
      1012
    ],
    "mask": "Train"
  },
  {
    "node_id": 545,
    "label": 2,
    "text": "Document Categorization and Query Generation on the World Wide Web Using WebACE We present WebACE, an agent for exploring and categorizing documents on the World Wide Web based on a user profile. The heart of the agent is an unsupervised categorization of a set of documents, combined with a process for generating new queries that is used to search for new related documents and for filtering the resulting documents to extract the ones most closely related to the starting set. The document categories are not given a priori. We present the overall architecture and describe two novel algorithms which provide significant improvement over Hierarchical Agglomeration Clustering and AutoClass algorithms and form the basis for the query generation and search component of the agent. We report on the results of our experiments comparing these new algorithms with more traditional clustering algorithms and we show that our algorithms are fast and scalable. y  Authors are listed alphabetically.  1 Introduction  The World Wide Web is a vast resource of information and services t...",
    "neighbors": [
      291,
      599,
      616,
      893,
      947,
      1126
    ],
    "mask": "Train"
  },
  {
    "node_id": 546,
    "label": 3,
    "text": "Practical Lineage Tracing in Data Warehouses We consider the view data lineage problem in a warehousing environment: For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formalize the problem and present a lineage tracing algorithm for relational views with aggregation. Based on our tracing algorithm, we propose a number of schemes for storing auxiliary views that enable consistent and efficient lineage tracing in a multisource data warehouse. We report on a performance study of the various schemes, identifying which schemes perform best in which settings. Based on our results, we have implemented a lineage tracing package in the WHIPS data warehousing system prototype at Stanford. With this package, users can select view tuples of interest, then efficiently \"drill down\" to examine the source data that produced them. 1 Introduction Data warehousing systems collect data from multiple distributed sources, integrate the information as materialized v...",
    "neighbors": [
      37
    ],
    "mask": "Train"
  },
  {
    "node_id": 547,
    "label": 5,
    "text": "Experience with EMERALD to Date After summarizing the EMERALD architecture and the evolutionary process from which EMERALD has evolved, this paper focuses on our experience to date in designing, implementing, and applying EMERALD to various types of anomalies and misuse. The discussion addresses the fundamental importance of good software engineering practice and the importance of the system architecture -- in attaining detectability, interoperability, general applicability, and future evolvability. It also considers the importance of correlation among distributed and hierarchical instances of EMERALD, and needs for additional detection and analysis components. 1. Introduction EMERALD (Event Monitoring Enabling Responses to Anomalous Live Disturbances) [6, 8, 9] is an environment for anomaly and misuse detection and subsequent analysis of the behavior of systems and networks. EMERALD is being developed under DARPA/ITO Contract number F30602-96-C-0294 and applied under DARPA/ISO Contract number F30602-98-C-0059. EMER...",
    "neighbors": [
      300,
      567
    ],
    "mask": "Train"
  },
  {
    "node_id": 548,
    "label": 2,
    "text": "Selecting Text Spans for Document Summaries: Heuristics and Metrics Human-quality text summarization systems are difficult to design, and even more difficult to evaluate, in part because documents can differ along several dimensions, such as length, writing style and lexical usage. Nevertheless, certain cues can often help suggest the selection of sentences for inclusion in a summary. This paper presents an analysis of news-article summaries generated by sentence extraction. Sentences are ranked for potential inclusion in the summary using a weighted combination of linguistic features -- derived from an analysis of news-wire summaries. This paper evaluates the relative effectiveness of these features. In order to do so, we discuss the construction of a large corpus of extraction-based summaries, and characterize the underlying degree of difficulty of summarization at different compression levels on articles in this corpus. Results on our feature set are presented after normalization by this degree of difficulty.",
    "neighbors": [
      517,
      695
    ],
    "mask": "Train"
  },
  {
    "node_id": 549,
    "label": 2,
    "text": "Assessment Methods for Information Quality Criteria Information quality (IQ) is one of the most important aspects of information integration on the Internet. Many projects realize and address this fact by gathering and classifying IQ criteria. Hardly ever do the projects address the immense difficulty of assessing scores for the criteria. This task must precede any usage of criteria for qualifying and integrating information.  After reviewing previous attempts to classify IQ criteria, in this paper we also classify criteria, but in a new, assessment-oriented way. We identify three sources for IQ scores and thus, three IQ criterion classes, each with different general assessment possibilities. Additionally, for each criterion we give detailed assessment methods. Finally, we consider confidence measures for these methods. Confidence expresses the accuracy, lastingness, and credibility of the individual assessment methods.  1 Introduction  Low information quality is one of the most pressing problems for consume rs of information that is di...",
    "neighbors": [
      553,
      970
    ],
    "mask": "Train"
  },
  {
    "node_id": 550,
    "label": 2,
    "text": "Automatic Text Detection and Tracking in Digital Video Text which appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this paper we present algorithms for detecting and tracking text in digital video. Our system implements a scalespace feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: an SSD (Sum of Squared Difference)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly. Keywords Text Detection, Text Tracking, Video Indexing, Digital Libraries, Neural Network I. Introduction The continued proliferation of large amounts of digital video has increased demand for true content based indexing and retrieval systems. Traditionally, content has been indexed primaril...",
    "neighbors": [
      8,
      840,
      1173
    ],
    "mask": "Test"
  },
  {
    "node_id": 551,
    "label": 4,
    "text": "Collections - Adapting The Display of Personal Objects for Different Audiences Although current networked systems and online applications provide new opportunities for displaying and sharing personal information, they do not account for the underlying social  contexts that frame such interactions. Existing categorization and management mechanisms for digital content have been designed to focus on the data they handle without much  regard for the social circumstances within which their content is shared. As we share large collections of personal information over mediated environments, our tools need to account  for the social scenarios that surround our interactions.  This thesis presents Collections: an application for the management  of digital pictures according to their intended audiences.  The goal is to create a graphical interface that  supports the creation of fairly complex privacy decisions concerning  the display of digital photographs. Simple graphics  are used to enable the collector to create a wide range of  audience arrangements for her digital pho...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 552,
    "label": 4,
    "text": "Graspable interfaces: Establishing design principles PhD Research Plan for Morten Fjeld. Topic: Design of Tangible User Interfaces",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 553,
    "label": 3,
    "text": "An Extensible Framework for Data Cleaning Data integration solutions dealing with large amounts of data have been strongly required in the last few years.  Besides the traditional data integration problems (e.g. schema integration, local to global schema mappings),  three additional data problems have to be dealt with: (1) the absence of universal keys across dierent databases  that is known as the object identity problem, (2) the existence of keyboard errors in the data, and (3) the presence  of inconsistencies in data coming from multiple sources. Dealing with these problems is globally called the data  cleaning process. In this work, we propose a framework which oers the fundamental services required by this  process: data transformation, duplicate elimination and multi-table matching. These services are implemented  using a set of purposely designed macro-operators. Moreover, we propose an SQL extension for specifying  each of the macro-operators. One important feature of the framework is the ability of explicitly includ...",
    "neighbors": [
      549,
      697,
      970
    ],
    "mask": "Train"
  },
  {
    "node_id": 554,
    "label": 5,
    "text": "Circumventing Dynamic Modeling: Evaluation of the Error-State Kalman Filter applied to Mobile Robot Localization The mobile robot localization problem is treated as a two-stage iterative estimation process. The attitude is estimated first and is then available for position estimation. The indirect (error state) form of the Kalman filter is developed for attitude estimation when applying gyro modeling. The main benefit of this choice is that complex dynamic modeling of the mobile robot and its interaction with the environment is avoided. The filter optimally combines the attitude rate information from the gyro and the absolute orientation measurements. The proposed implementation is independent of the structure of the vehicle or the morphology of the ground. The method can easily be transfered to another mobile platform provided it carries an equivalent set of sensors. The 2D case is studied in detail first. Results of extending the approach to the 3D case are presented. In both cases the results demonstrate the efficacy of the proposed method.  1 Introduction  On July 4th 1997, the Mars Pathfinde...",
    "neighbors": [
      528
    ],
    "mask": "Test"
  },
  {
    "node_id": 555,
    "label": 2,
    "text": "An XML-based Multimedia Middleware for Mobile Online Auctions Pervasive Internet services today promise to provide users with a quick and convenient access  to a variety of commercial applications. However, due to unsuitable architectures and  poor performance user acceptance is still low. To be a major success mobile services have  to provide device-adapted content and advanced value-added Web services. Innovative enabling  technologies like XML and wireless communication may for the first time provide a  facility to interact with online applications anytime anywhere. We present a prototype implementing  an efficient multimedia middleware approach towards ubiquitous value-added  services using an auction house as a sample application. Advanced multi-feature retrieval  technologies are combined with enhanced content delivery to show the impact of modern  enterprise information systems on today's e-commerce applications.  Keywords: mobile commerce, online auctions, middleware architectures, pervasive  Internet technology, multimedia database appli...",
    "neighbors": [
      507,
      1145
    ],
    "mask": "Train"
  },
  {
    "node_id": 556,
    "label": 0,
    "text": "Categorization of Software Errors that led to Security Breaches A set of errors known to have led to security breaches in computer systems was analyzed. The analysis led to a categorization of these errors. After examining several proposed schemes for the categorization of software errors a new scheme was developed and used. This scheme classifies errors by their cause, the nature of their impact, and the type of change, or fix, made to remove the error. The errors considered in this work are found in a database maintained by the COAST laboratory. The categorization is the first step in the investigation of the effectiveness of various measures of code coverage in revealing software errors that might lead to security breaches. 1 Introduction  We report the outcome of an effort to categorize errors in software that are known to have led to security breaches. The set of errors used in this study came from a database of errors developed in the COAST laboratory [10]. Several existing schemes for the categorization of software errors were evaluated for ...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 557,
    "label": 0,
    "text": "Agents That Reason and Negotiate By Arguing The need for negotiation in multi-agent systems stems from the requirement  for agents to solve the problems posed by their interdependence upon one another.  Negotiation provides a solution to these problems by giving the agents  the means to resolve their conflicting objectives, correct inconsistencies in their  knowledge of other agents' world views, and coordinate a joint approach to domain  tasks which benefits all the agents concerned. We propose a framework,  based upon a system of argumentation, which permits agents to negotiate in  order to establish acceptable ways of solving problems. The framework provides  a formal model of argumentation-based reasoning and negotiation, details  a design philosophy which ensures a clear link between the formal model and  its practical instantiation, and describes a case study of this relationship for a  particular class of architectures (namely those for belief-desire-intention agents).  1 Introduction  An increasing number of software app...",
    "neighbors": [
      222,
      263,
      597,
      724,
      852,
      953,
      964,
      1051,
      1166,
      1204,
      1208
    ],
    "mask": "Train"
  },
  {
    "node_id": 558,
    "label": 1,
    "text": "How Developmental Psychology and Robotics Complement Each Other This paper presents two complementary ideas relating  the study of human development and the  construction of intelligent artifacts. First, the use  of developmental models will be a critical requirement  in the construction of robotic systems that  can acquire a large repertoire of motor, perceptual,  and cognitive capabilities. Second, robotic  systems can be used as a test-bed for evaluating  models of human development much in the same  way that simulation studies are currently used  to evaluate cognitive models. To further explore  these ideas, two examples from the author's own  work will be presented: the use of developmental  models of hand-eye coordination to simplify the  task of learning to reach for a visual target and  the use of a humanoid robot to evaluate models  of normal and abnormal social skill development.  Introduction  Research on human development and research on the construction of intelligent artifacts can and should be complementary. Studies of human developm...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 559,
    "label": 1,
    "text": "Parameter Learning of Logic Programs for Symbolic-statistical Modeling We propose a logical/mathematical framework for statistical parameter learning of parameterized logic programs, i.e. de nite clause programs containing probabilistic facts with a parameterized distribution. It extends the traditional least Herbrand model semantics in logic programming to distribution semantics, possible world semantics with a probability distribution which is unconditionally applicable to arbitrary logic programs including ones for HMMs, PCFGs and Bayesian networks. We also propose a new EM algorithm, the graphical EM algorithm, thatrunsfora class of parameterized logic programs representing sequential decision processes where each decision is exclusive and independent. It runs on a new data structure called support graphs describing the logical relationship between observations and their explanations, and learns parameters by computing inside and outside probability generalized for logic programs. The complexity analysis shows that when combined with OLDT search for all explanations for observations, the graphical EM algorithm, despite its generality, has the same time complexity as existing EM algorithms, i.e. the Baum-Welch algorithm for HMMs, the Inside-Outside algorithm for PCFGs, and the one for singly connected Bayesian networks that have beendeveloped independently in each research eld. Learning experiments with PCFGs using two corpora of moderate size indicate that the graphical EM algorithm can signi cantly outperform the Inside-Outside algorithm. 1.",
    "neighbors": [
      39
    ],
    "mask": "Validation"
  },
  {
    "node_id": 560,
    "label": 1,
    "text": "Structure Identification of Fuzzy Classifiers For complex and high-dimensional problems, data-driven  identification of classifiers has to deal with structural issues like the selection of the relevant features and effective initial partition of the input domain. Therefore, the identification of fuzzy classifiers is a challenging topic. Decision-tree (DT) generation algorithms are effective in feature selection and extraction of crisp classification rules, hence they can be used for the initialization of fuzzy systems. Because fuzzy classifiers have much flexible decision boundaries than DTs, fuzzy models can be more parsimonious than DTs. Hence, to get compact, easily interpretable and transparent classification system, a new structure identification algorithm is proposed, where genetic algorithm (GA) based parameter optimization of the DT initialized fuzzy sets is combined with similarity based rule base simplification algorithms. The performance of the approach is studied on a specially designed artificial data. An application to the Cancer classification problem is also shown.",
    "neighbors": [
      828,
      1202
    ],
    "mask": "Train"
  },
  {
    "node_id": 561,
    "label": 2,
    "text": "A System For Automatic Personalized Tracking of Scientific Literature on the Web We introduce a system as part of the CiteSeer digital library project for automatic tracking of scientific literature that is relevant to a user\u2019s research interests. Unlike previous systems that use simple keyword matching, CiteSeer is able to track and recommend topically relevant papers even when keyword based query profiles fail. This is made possible through the use of a heterogenous profile to represent user interests. These profiles include several representations, including content based relatedness measures. The CiteSeer tracking system is well integrated into the search and browsing facilities of CiteSeer, and provides the user with great flexibility in tuning a profile to better match his or her interests. The software for this system is available, and a sample database is online as a public service.",
    "neighbors": [
      43,
      70,
      314,
      453,
      596
    ],
    "mask": "Train"
  },
  {
    "node_id": 562,
    "label": 3,
    "text": "Reclustering of HEP Data in Object-Oriented Databases The Large Hadron Collider (LHC), build at CERN, will enter operation in 2005. The experiments at the LHC will generate some 5 PB of data per year, which are stored in an ODBMS.  A good object clustering on the disk drives will be critical to achieve a high data throughput required by future analysis scenarios. This paper presents a new reclustering algorithm for HEP data that maximizes the read transfer rate for objects contained in multiple overlapping collections. It works by decomposing the stored objects into a number of chunks and rearranging them by means of heuristics solving the traveling salesman problem with Hamming distance. Furthermore experimental results of a prototype are presented.  Keywords: object-oriented databases, scientific databases, object clustering, query optimisation  1 Introduction  The ATLAS experiment [1] at CERN, due to take data in the year 2005 will store approximately 1 PB (10  15  bytes) of data per year. Data taking is expected to last 15 or more yea...",
    "neighbors": [
      1019
    ],
    "mask": "Train"
  },
  {
    "node_id": 563,
    "label": 0,
    "text": "Towards UML-based Analysis and Design of Multi-Agent Systems The visual modeling facilities of the UML do not provide sufficient means to support the design of multi-agent systems. In this paper, we are investigating the development phases of requirements analysis, design, and code generation for multi agent systems. In the requirements analysis phase, we are using extended use case diagrams to identify agents and their relationship to the environment. In the design phase, we are using stereotyped class and object diagrams to model different agent types and their related goals and strategies. While these diagrams define the static agent system architecture, dynamic agent behavior is modeled in statecharts with respect to the BDI  1 agent approach. Concerning code generation, we show how the used diagrams can be taken to generate code for CASA, our executable agent specification language that is integrated into an existing multi-agent framework.  1",
    "neighbors": [
      941,
      1249
    ],
    "mask": "Train"
  },
  {
    "node_id": 564,
    "label": 1,
    "text": "Improvement in a Lazy Context: An Operational Theory for Call-By-Need The standard implementation technique for lazy functional languages is call-by-need, which ensures that an argument to a function in any given call is evaluated at most once. A significant problem with call-by-need is that it is difficult --- even for compiler writers --- to predict the effects of program transformations. The traditional theories for lazy functional languages are based on call-by-name models, and offer no help in determining which transformations do indeed optimize a program. In this article we present an operational theory for callby -need, based upon an improvement ordering on programs:  M is improved by N if in all program-contexts C,  when C[M ] terminates then C[N ] terminates at least as cheaply. We show that this improvement relation satisfies a \"context lemma\", and supports a rich inequational theory, subsuming the call-by-need lambda calculi of Ariola et al.  [AFM  +  95]. The reduction-based call-by-need calculi are inadequate as a theory of lazy-program tran...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 565,
    "label": 4,
    "text": "Maintaining the Illusion of Interacting Within a 3D Virtual Space It is widely thought to more or less a degree, that a sense of presence may be induced in users of new and emerging media technologies, such as, the Internet, digital television and cinema (supporting interaction), teleconferencing and 3D virtual reality systems. In this paper, it is argued that presence presupposes that participants are absorbed in the illusion of interacting within the visual spaces created by these media. That is, prior to the possibility of any inducement of presence, participants need to be absorbed in the illusion conveyed by the media. Without this, participants' attention is broken and the illusion is lost. Hence, the potential to induce presence in participants ceases. To encourage participants to lose sight of the means of representation and be drawn into the illusion conveyed by these media, this paper proposes the development of design principles to increase participants' experience. In an attempt to inform design principles, this paper focuses on another artificial although highly successful visual medium - film. By way of example, this paper concentrates on one medium, virtual reality, and proposes design principles that attempt to maintain the illusion of interacting within 3D virtual space. This attempts to provide a platform through the resourceful blend of hardware and software Virtual Reality (VR) enabling technologies on which to support a well designed virtual environment and hence, from which the inducement of presence in participants may develop.",
    "neighbors": [
      678,
      786
    ],
    "mask": "Train"
  },
  {
    "node_id": 566,
    "label": 1,
    "text": "On Concept Space and Hypothesis Space in Case-Based Learning Algorithms . In order to learn more about the behaviour of case-based reasoners as learning systems, we formalise a simple case-based learner as a PAC learning algorithm. We show that the case-based representation  hCB; oei is rich enough to express any boolean function. We define a family of simple case-based learning algorithms which use a single, fixed similarity measure and we give necessary and sufficient conditions for the consistency of these learning algorithms in terms of the chosen similarity measure. Finally, we consider the way in which these simple algorithms, when trained on target concepts from a restricted concept space, often output hypotheses which are outside the chosen concept space. A case study investigates this relationship between concept space and hypothesis space and concludes that the case-based algorithm studied is a less than optimal learning algorithm for the chosen, small, concept space. 1 Introduction  The performance of a case-based reasoning system [13] will chan...",
    "neighbors": [
      1149,
      1259
    ],
    "mask": "Test"
  },
  {
    "node_id": 567,
    "label": 0,
    "text": "Intrusion Detection: A Bibliography This document contains more than 600 references, dated from 1980 to 2001. We undoubtedly have forgotten some important citations, either through oversight or ignorance. Moreover, errors may remain in the citations. Thus, we ask for your indulgence and, more importantly, for your help. Send us a note if you nd any errors and let us know of any omissions",
    "neighbors": [
      547,
      620
    ],
    "mask": "Validation"
  },
  {
    "node_id": 568,
    "label": 0,
    "text": "Analysis and Design using MaSE and agentTool This paper provides an overview of the work being done at the Air Force Institute of Technology on the Multiagent Systems Engineering methodology and the associated agentTool environment. Our research is focused on discovering methods and techniques for engineering practical multiagent systems. It uses the abstraction provided by multiagent systems for developing intelligent, distributed software systems.",
    "neighbors": [
      573
    ],
    "mask": "Train"
  },
  {
    "node_id": 569,
    "label": 0,
    "text": "ATNoSFERES: a Model for Evolutive Agent Behaviors This paper introduces ATNoSFEERS, a model aimed at designing evolutive and adaptive behaviors for agents or multi-agent systems. We first discuss briefly the main problems raised by classical evolutionary models, which are not intended to produce agents or behaviors but to solve problems. Then we provide detailed explanations about the model we propose and its components. We also show through a simple example how the system works, and give some experimental results. Finally, we discuss the features of our model and propose extensions.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 570,
    "label": 2,
    "text": "Finite-state approaches to Web information extraction Introduction  An information agent is a distributed system that receives a goal through its user interface, gathers information relevant to this goal from a variety of sources, processes this content as appropriate, and delivers the results to the users. We focus on the second stage in this generic architecture. We survey a variety of information extraction techniques that enable information agents to automatically gather information from heterogeneous sources.  For example, consider an agent that mediates package-delivery requests. To satisfy such requests, the agent might need to retrieve address information from geographic services, ask an advertising service for freight forwarders that serve the destination, request quotes from the relevant freight forwarders, retrieve duties and legal constraints from government sites, get weather information to estimate transportation delays, etc.  Information extraction (IE) is a form of shallow document processing that involves populating",
    "neighbors": [
      279,
      412,
      855,
      1215,
      1233
    ],
    "mask": "Test"
  },
  {
    "node_id": 571,
    "label": 2,
    "text": "Developing Language Processing Components with GATE (a User Guide) Contents  1 Introduction 3 1.1 How to Use This Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Structure of the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2 How To. . . 14 2.1 Download GATE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2 Install and Run GATE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.3 [D,F] Configure GATE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.4 Build GATE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.5 [D,F] Create a New CREOLE Resource . . . . . . . . . . . . . . . . . . . . 18 2.6<F11",
    "neighbors": [
      323,
      1083
    ],
    "mask": "Train"
  },
  {
    "node_id": 572,
    "label": 2,
    "text": "Visual Ranking of Link Structures (Extended Abstract) Methods for ranking World Wide Web resources according to their position in the link structure of the Web are receiving considerable attention, because they provide the first effective means for search engines to cope with the explosive growth and diversification of the Web.",
    "neighbors": [
      774,
      819,
      1000,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 573,
    "label": 0,
    "text": "A Survey of Agent-Oriented Methodologies . This article introduces the current agent-oriented methodologies. It discusseswhat approacheshave been followed (mainly extending existing objectoriented and knowledge engineering methodologies), the suitability of these approaches for agent modelling, and some conclusions drawn from the survey. 1 Introduction Agent technology has received a great deal of attention in the last few years and, as a result, the industry is beginning to get interested in using this technology to develop its own products. In spite of the different developed agent theories, languages, architectures and the successful agent-based applications, very little work for specifying (and applying) techniques to develop applications using agent technology has been done. The role of agent-oriented methodologies is to assist in all the phases of the life cycle of an agent-based application, including its management. This article reviews the current approaches to the development of an agent-oriented (AO) methodology. ...",
    "neighbors": [
      140,
      451,
      521,
      568,
      622,
      885,
      1248
    ],
    "mask": "Train"
  },
  {
    "node_id": 574,
    "label": 1,
    "text": "Hierarchical Models for Screening of Iron Deficiency Anemia We investigate the problem of classifying individuals based on estimated density functions for each individual. The problem is similar to conventional classification in that there is labelled training data, but different in that the underlying measurements are not feature vectors but histograms or density estimates. We describe a general framework based on probabilistic hierarchical models for modelling such data and illustrate how the model lends itself to classification. We contrast this approach with two other alternatives: (1) directly defining distance between densities using a cross-entropy distance measure, and (2) using parameters of the estimated densities as feature vectors for a standard discriminative classification framework. We evaluate all three methods on a realworld medical diagnosis problem. The hierarchical modeling and density-distance approaches are most accurate, yielding crossvalidated error rates in the range of 1 to 2%. We conclude by discussing the relative me...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 575,
    "label": 3,
    "text": "Planned Disconnections for Mobile Databases As mobility permeates todays computing environment, we envision application infrastructures that will increasingly use mobile technologies. Traditional database applications will need to integrate mobile entities: people and computers. In this paper, we develop a distributed database framework for mobile environments. A key requirement in such an environment is to support frequent connection and disconnection of database sites.  1 Introduction  As mobility permeates into todays computing and communication arena, we envision application infrastructures that will increasingly rely on mobile technologies. Current mobility applications tend to have a large central server and use mobile platforms only as caching devices. We want to elevate the role of mobile computers to first class entities in the sense that they allow the mobile user work/update capabilities independent of a central server. In such an environment, several mobile computers may collectively form the entire distributed syste...",
    "neighbors": [
      894
    ],
    "mask": "Validation"
  },
  {
    "node_id": 576,
    "label": 4,
    "text": "Roomware: Towards the next generation of human-computer interaction based on an integrated design of real and virtual worlds In the past, a central mainframe computer provided terminals for many users. In the current age of the personal desktop computer, there is one computer for one person. Observation of early adopters and predictions about the future point to an era where each person will have multiple devices and computational power will be ubiquitous. Against this background, we present a vision for the workspaces of the future and a user-centered approach for an integrated design of virtual information spaces and real architectural spaces. The resulting environments are called cooperative buildings. The design approach is based on the roomware concept. By roomware, we mean computer-augmented objects resulting from the integration of room elements, e.g., walls, doors, furniture (tables, chairs, etc.) with computer-based information devices. They are part of the vision that the world around us will be the interface to information -- where the computer as a device will disappear and people's interaction w...",
    "neighbors": [
      316
    ],
    "mask": "Test"
  },
  {
    "node_id": 577,
    "label": 5,
    "text": "HYMES: A HYbrid Modular Expert System with Efficient Inference and Explanation A HYbrid Modular Expert System, called HYMES, is presented. HYMES provides a dual representation scheme: a symbolic one, based on conventional symbolic rules, and a hybrid one, based on neumles, a kind of rules that combine a symbolic and a connectionist representation. Symbolic rules are internally converted into neumles, for efficiency reasons. In this way, hybrid modular knowledge bases can be constructed.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 578,
    "label": 2,
    "text": "An Approach to Relate the Web Communities Through Bipartite Graphs The Web harbors a large number of community structures. Early detection of community structures has many purposes such as reliable searching and selective advertising. In this paper we investigate the problem of extracting and relating the web community structures from a large collection of Web-pages by performing hyper-link analysis. The proposed algorithm extracts the potential community signatures by extracting the corresponding dense bipartite graph (DBG) structures from the given data set of web pages. Further, the proposed algorithm can also be used to relate the extracted community signatures. We report the experimental results conducted on 10 GB TREC (Text REtrieval Conference) data collection that contains 1.7 million pages and 21.5 million links. The results demonstrate that the proposed approach extracts meaningful community signatures and relates them.",
    "neighbors": [
      536,
      774,
      1000,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 579,
    "label": 2,
    "text": "Discovery of Similarity Computations of Search Engines Two typical situations in which it is of practical interest to determine the similarities of text documents to a query due to a search engine are: (1) a global search engine, constructed on top of a group of local search engines, wishes to retrieve the set of local documents globally most similar to a given query; and (2) an organization wants to compare the retrieval performance of search engines. The dot-product function is a widely used similarity function. For a search engine using such a function, we can determine its similarity computations if how the search engine sets the weights of terms is known, which is usually not the case. In this paper, techniques are presented to discover certain mathematical expressions of these formulas and the values of embedded constants when the dot-product similarity function is used. Preliminary results from experiments on the WebCrawler search engine are given to illustrate our techniques.  1  Categories and Subject Descriptors  H.3 [Information...",
    "neighbors": [
      241,
      510,
      526,
      696
    ],
    "mask": "Train"
  },
  {
    "node_id": 580,
    "label": 0,
    "text": "The EASE Actor Development Environment In interactive simulations it is often desirable to have intelligent actors  playing the roles of humans. Drawing on a wide range of previous  work this paper presents a system that is intended to reduce some of the  diculties involved in the development of actors. We present a system  called EASE (End-user Actor Specication Environment) that provides  tools and methods to support end user development of intelligent actors.  The tools support the whole development process from design to testing.  The EASE actor architecture is a multi-agent system where a process of  contract making and negotiation between agents determines the actions  of the actor.  1 Introduction  In modern, complex, interactive simulations it is often highly desirable to have intelligent actors playing the roles of humans. The actors' task is dicult { sensing the (simulated) environment, choosing a course of action that exibly and intelligently follows designer intentions and sending appropriate commands back to ...",
    "neighbors": [
      516,
      941
    ],
    "mask": "Train"
  },
  {
    "node_id": 581,
    "label": 0,
    "text": "Modeling Emotion-Based Decision-Making This paper presents a computational approach to EmotionBased Decision-Making that models important aspects of emotional processing and integrates these with other models of perception, motivation, behavior, and motor control. A particular emphasis is placed on using some of the mechanisms of emotions as building blocks for the acquisition of emotional memories that serve as biasing signals during the process of making decisions and selecting actions. We have successfully followed this approach to develop and control several different autonomous agents, including both synthetic agents and physical robots.  Introduction  Most theories of human reasoning and decision-making fall between two different positions. The first one argues that we make decisions in a way similar to that of solving problems in formal logic. According to this view, when faced with a problem, we form a list of all different options and their possible outcomes, and then we use logic in its best sense to perform a cos...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 582,
    "label": 5,
    "text": "Loglinear Models for First-Order Probabilistic Reasoning Recent work on loglinear models in probabilistic constraint logic programming is applied to first-order probabilistic reasoning. Probabilities are defined directly on the  proofs of atomic formulae, and by marginalisation on the atomic formulae themselves. We use Stochastic Logic Programs (SLPs) composed of labelled and unlabelled definite clauses to define the proof probabilities. We have a conservative extension of first-order reasoning, so that, for example, there is a one-one mapping between logical and random variables. We show how, in this framework, Inductive Logic Programming (ILP) can be used to induce the features of a loglinear model from data. We also compare the presented framework with other approaches to first-order probabilistic reasoning. Keywords: loglinear models, constraint logic programming, inductive logic programming  1 Introduction  A framework which merges first-order logical and probabilistic inference in a theoretically sound and applicable manner promises ma...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 583,
    "label": 2,
    "text": "Discovering Seeds of New Interest Spread from Premature Pages Cited by Multiple Communities The World Wide Web is a great source of new topics significant  for trend birth and creation. In this paper, we propose a method for  discovering topics, which stimulate communities of people into earnest  communications on the topics' meaning, and grow into a trend of popular  interest. Here, the obtained are web pages which absorb attentions of  people from multiple interest-communities. It is shown by a experiments  to a small group of people, that topics in such pages can trigger the  growth of peoples' interests, beyond the bounds of existing communities.",
    "neighbors": [
      1000,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 584,
    "label": 3,
    "text": "A Cost-Based Object Buffer Replacement Algorithm for Object-Oriented Database Systems Many object-oriented database systems manage object buffers to provide fast access to objects. Traditional buffer replacement algorithms based on fixed-length pages simply assume that the cost incurred by operating a buffer is proportional to the number of buffer faults. However, this assumption no longer holds in an object buffer where objects are of variable-lengths and the cost of replacing an object varies for each object. In this paper, we propose a cost-based replacement algorithm for object buffers. The proposed algorithm replaces the objects that have minimum costs per unit time and unit space. The cost model extends the previous page-based one to include the replacement costs and the sizes of objects. The performance tests show that the proposed algorithm is almost always superior to the LRU-2 algorithm and in some cases is more than twice as fast. The idea of cost-based replacement can be applied to any buffer management architectures that adopt earlier algorithms. It is espe...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 585,
    "label": 3,
    "text": "A Web Odyssey: from Codd to XML INTRODUCTION  The Web presents the database area with vast opportunities and commensurate challenges. Databases and the Web are organically connected at many levels. Web sites are increasingly powered by databases. Collections of linked Web pages distributed across the Internet are themselves tempting targets for a database. The emergence of XML as the lingua franca of the Web brings some much needed order and will greatly facilitate the use of database techniques to manage Web information.  This paper will discuss some of the developments related to the Web from the viewpoint of database theory. As we shall see, the Web scenario requires revisiting some of the basic assumptions of the area. To be sure, database theory remains as valid as ever in the classical setting, and the database industry will continue to representamulti-billion dollar target of applicability for the foreseeable future. But the Web represents an opportunityofanentirely di#erent scale. We are th",
    "neighbors": [
      17,
      23,
      88,
      673,
      681,
      685,
      1055,
      1162,
      1254
    ],
    "mask": "Validation"
  },
  {
    "node_id": 586,
    "label": 2,
    "text": "Constrained Nearest Neighbor Queries In this paper we introduce the notion of constrained nearest neighbor queries (CNN) and propose a  series of methods to answer them. This class of queries can be thought of as nearest neighbor queries with  range constraints. Although both nearest neighbor and range queries have been analyzed extensively in  previous literature, the implications of constrained nearest neighbor queries have not been discussed. Due  to their versatility, CNN queries are suitable to a wide range of applications from GIS systems to reverse  nearest neighbor queries and multimedia applications. We develop methods for answering CNN queries  with different properties and advantages. We prove the optimality (with respect to I/O cost) of one of the  techniques proposed in this paper. The superiority of the proposed technique is shown by a performance  analysis.  1 Introduction  Two dimensional range queries are used frequently in various applications such as spatial databases [Sam89,  GG98] and Geographic Infor...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 587,
    "label": 2,
    "text": "Context and Page Analysis for Improved Web Search NEC Research Institute has developed a metasearch engine that improves the efficiency of Web searches by downloading and analyzing each document and then displaying results that show the query terms in context. Several popular and useful search engines such as AltaVista, Excite, HotBot, Infoseek, Lycos, and Northern Light attempt to maintain full-text indexes of the World Wide Web. However, relying on a single standard search engine has limitations. The standard search engines have limited coverage, 1,2 outdated databases, and are sometimes unavailable due to problems with the network or the engine itself. The precision of standard engine results can also vary because they generally focus on handling queries quickly and use relatively simple ranking schemes. 3 Rankings can be further muddled by keyword spamming to increase a page's rank order. Often, the relevance of a particular page is obvious only after loading it and finding the query terms. Metasearch engines, such as MetaCrawler and SavvySearch, attempt to contend with the problem of limited coverage by submitting queries to several standard search engines at once. 4,5 The primary advantages of metasearch engines are that they combine the results of several search engines and present a consistent user interface. 5 However, most metasearch engines rely on the documents and summaries returned by standard search engines and so inherit their limited precision and vulnerability to keyword spamming. We developed the NEC Research Institute (NECI) metasearch engine [now called Inquirus] to improve the efficiency and precision of Web search by downloading and analyzing each document and then displaying results that show the query terms in",
    "neighbors": [
      219,
      233,
      496,
      595,
      653,
      1017,
      1031,
      1066,
      1134,
      1207,
      1264
    ],
    "mask": "Train"
  },
  {
    "node_id": 588,
    "label": 3,
    "text": "Executing Suspended Logic Programs . We present an extension of Logic Programming (LP) which, in addition to ordinary LP clauses, also includes integrity constraints, explicit representation of disjunction in the bodies of clauses and in goals, and suspension of atoms as in concurrent logic languages. The resulting framework aims to unify Constraint Logic Programming (CLP), Abductive Logic Programming (ALP) and Semantic Query Optimisation (SQO) in deductive databases. We present a proof procedure for the new framework, simplifying and generalising previously proposed proof procedures for ALP. We discuss applications of the framework, formulating traditional problems from LP, ALP, CLP and SQO. Keywords: Logic Programming (LP), Constraint Logic Programming (CLP), Abductive Logic Programming (ALP), Semantic Query Optimisation (SQO) in Deductive Databases.   The second author is supported by the EPSRC project \"Logic-based multi-agent systems\". The third author is supported by ONR grant N00014-96-1-1057. The authors are grat...",
    "neighbors": [
      400,
      532,
      1091
    ],
    "mask": "Test"
  },
  {
    "node_id": 589,
    "label": 0,
    "text": "Algorithm for Optimal Winner Determination in Combinatorial Auctions Combinatorial auctions, i.e. auctions where bidders can bid on com-binations of items, tend to lead to more e cient allocations than tra-ditional auctions in multi-item auctions where the agents ' valuations of the items are not additive. However, determining the winners so as to maximize revenue is NP-complete. First, existing approaches for tackling this problem are reviewed: exhaustive enumeration, dynamic programming, approximation algorithms, and restricting the allow-able combinations. Then we present our search algorithm for optimal winner determination. Experiments are shown on several bid distri-butions. The algorithm allows combinatorial auctions to scale up to signi cantly larger numbers of items and bids than prior approaches to optimal winner determination by capitalizing on the fact that the space of bids is necessarily sparsely populated in practice. The algo-rithm does this by provably su cient selective generation of children in the search tree, by using a secondary search for fast child genera-tion, by heuristics that are accurate and optimized for speed, and by four methods for preprocessing the search space. Patent pending. A highly optimized implementation of the algorithm is available for licensing both for research and commercial purposes. Please contact the author. 1 1",
    "neighbors": [
      380,
      716,
      1100
    ],
    "mask": "Train"
  },
  {
    "node_id": 590,
    "label": 3,
    "text": "A Graph Query Language and Its Query Processing Many new database applications involve querying of graph data. In this paper, we present an objectoriented graph data model, and an OQL like graph query language, GOQL. The data model and the language are illustrated in the application domain of multimedia presentation graphs. We then discuss the query processing techniques for GOQL, more specifically, the translation of GOQL into an operatorbased language, called O-Algebra, extended with operators to deal with paths and sequences. We also discuss different approaches for efficient implementations of algebra operators for paths and sequences. 1 Introduction  Many database applications such as hypertext applications, geographic information systems, world wide web searching, and heterogeneous information integration, etc., require modeling and querying of graph data ([Guti94, GBPV 94, MeMM 96, BDHS 96, AQMWW 96, AM 98, FFKLS 98, LSBBOO98]). In this paper, we present a data model, and an OQL-like query language GOQL, for querying graphs. ...",
    "neighbors": [
      813
    ],
    "mask": "Validation"
  },
  {
    "node_id": 591,
    "label": 0,
    "text": "Supporting Trust in Virtual Communities At any given time, the stability of a community depends on the right balance of trust and distrust. Trust is also the basis of economic activities, making such things as credit agreements, business contracts and customer confidence possible. In real-life, we are faced with increasingly complex decisions due to the increasingly daunting range of options provided by the Internet and uncertainty in the credibility of virtual entities. In short, information overload, increased uncertainty and risk taking a prominent feature of modern living. We as members of society cope with these complexities and uncertainties by relying on a vital social phenomenon, trust, which forms the basis of all social interaction. However, the ability to reason about trust is absent from the virtual medium, making virtual communities of real and artificial agents fragile. This is unsatisfactory as all `virtual' interactions are ultimately human-bound. Therefore we need a trust model to allow artificial agents to ...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 592,
    "label": 1,
    "text": "An Introduction to Variational Methods for Graphical Methods . This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.",
    "neighbors": [
      505,
      791
    ],
    "mask": "Test"
  },
  {
    "node_id": 593,
    "label": 0,
    "text": "Programming Satan's Agents Mobile agent security is still a young discipline and most naturally, the focus up to the time of writing was on inventing new cryptographic protocols for securing various aspects of mobile agents. However, past experience shows that protocols can be flawed, and flaws in protocols can remain unnoticed for a long period of time. The game of breaking and fixing protocols is a necessary evolutionary process that leads to a better understanding of the underlying problems and ultimately to more robust and secure systems. Although, to the best of our knowledge, little work has been published on breaking protocols for mobile agents, it is inconceivable that the multitude of protocols proposed so far are all flawless. As it turns out, the opposite is true. We identify flaws in protocols proposed by Corradi et al., Karjoth et al., and Karnik et al., including protocols based on secure coprocessors.",
    "neighbors": [
      252,
      1103
    ],
    "mask": "Train"
  },
  {
    "node_id": 594,
    "label": 3,
    "text": "Static and Dynamic Information Organization with Star Clusters In this paper we present a system for static and dynamic information organization and show our evaluations of this system on TREC data. We introduce the off-line and on-line star clustering algorithms for information organization. Our evaluation experiments show that the off-line star algorithm outperforms the single link and average link clustering algorithms. Since the star algorithm is also highly efficient and simple to implement, we advocate its use for tasks that require clustering, such as information organization, browsing, filtering, routing, topic tracking, and new topic detection.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 595,
    "label": 2,
    "text": "Improving Category Specific Web Search by Learning Query Modifications A user searching for documents within a specific category using a general purpose search engine might have a difficult time finding valuable documents. To improve category specific search, we show that a trained classifier can recognize pages of a specified category with high precision by using textual content, text location, and HTML structure. We show that query modifications to web search engines increase the probability that the documents returned are of the specific category.  We evaluate the effectiveness of several query modifications on real search engines, showing that the approach is highly effective for locating personal homepages and calls for papers.  1: Introduction  Typical web search engines index millions of pages across a variety of categories, and return results ranked by expected topical relevance. Only a small percentage of these pages may be of a specific category, for example, personal homepages, or calls for papers. A user may examine large numbers of pages abou...",
    "neighbors": [
      496,
      511,
      587,
      892,
      897,
      1134
    ],
    "mask": "Train"
  },
  {
    "node_id": 596,
    "label": 2,
    "text": "FEATURES: Real-time Adaptive Feature Learning and Document Learning for Web Search In this paper we report our research on building Features - an intelligent web search engine that is able to perform real-time adaptive feature (i.e., keyword) and document learning. Not only does Features learn from the user's document relevance feedback, but also automatically extracts and suggests indexing keywords relevant to a search query and learns from the user's keyword relevance feedback so that it is able to speed up its search process and to enhance its search performance. We design two efficient and mutual-benefiting learning algorithms that work concurrently, one for feature learning and the other for document learning. Features employs these algorithms together with an internal index database and a real-time meta-searcher so to perform adaptive real-time learning to find desired documents with as little relevance feedback from the user as possible. The architecture and performance of Features are also discussed. 1 Introduction As the world wide web rapidly evo...",
    "neighbors": [
      43,
      70,
      453,
      561,
      1000,
      1007,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 597,
    "label": 0,
    "text": "Social Mental Shaping: Modelling the Impact of Sociality on the Mental States of Autonomous Agents This paper presents a framework that captures how the social nature of agents that are situated in a multi-agent environment impacts upon their individual mental states. Roles and social relationships provide an abstraction upon which we develop the notion of social mental shaping. This allows us to extend the standard Belief-DesireIntention model to account for how common social phenomena (e.g. cooperation, collaborative problem-solving and negotiation) can be integrated into a unified theoretical perspective that reflects a fully explicated model of the autonomous agent's mental state.  Keywords: Multi-agent systems, agent interactions, BDI models, social influence.  3  1.",
    "neighbors": [
      557,
      724
    ],
    "mask": "Train"
  },
  {
    "node_id": 598,
    "label": 1,
    "text": "The Pareto Archived Evolution Strategy : A New Baseline Algorithm for Pareto Multiobjective Optimisation Most popular evolutionary algorithms for multiobjective optimisation maintain a population of solutions from which individuals are selected for reproduction. In this paper, we introduce a simpler evolution scheme for multiobjective problems, called the Pareto Archived Evolution Strategy (PAES). We argue that PAES may represent the simplest possible non-trivial algorithm capable of generating diverse solutions in the Pareto optimal set. The algorithm is identified as being a (1 + 1) evolution strategy, using local search from a population of one but using a reference archive of previously found solutions in order to identify the approximate dominance ranking of the current and candidate solution vectors. PAES is intended as a good baseline approach, against which more involved methods may be compared, and may also serve well in some real-world applications when local search seems superior to or competitive with population-based methods. The performance of the new algorithm is compared w...",
    "neighbors": [
      301
    ],
    "mask": "Train"
  },
  {
    "node_id": 599,
    "label": 2,
    "text": "Mining the Web to Create Minority Language Corpora The Web is a valuable source of language specific resources but the process of collecting, organizing and utilizing these resources is difficult. We describe CorpusBuilder, an approach for automatically generating Web-search queries for collecting documents in a minority language. It differs from pseudo-relevance feedback in that retrieved documents are labeled by an automatic language classifier as relevant or irrelevant, and this feedback is used to generate new queries. We experiment with various query-generation methods and query-lengths to find inclusion/exclusion terms that are helpful  for retrieving documents in the target language and find that using odds-ratio scores calculated over the documents acquired so far was one of the most consistently accurate query-generation methods. We also describe experiments using a handful of words elicited from a user instead of initial documents and show that the methods perform similarly. Experiments applying the same approach to multiple languages are also presented showing that our approach generalizes to a variety of languages.  1.",
    "neighbors": [
      43,
      241,
      545,
      1049
    ],
    "mask": "Test"
  },
  {
    "node_id": 600,
    "label": 4,
    "text": "Simulated 3D Painting This technical report looks at the motivation for simulating painting directly on 3D objects, and investigates the main issues faced by such systems. These issues include the provision of natural user interfaces, the reproduction of realistic brush effects and the surface parameterization for texture mapping so that the results of the painting can be stored on texture maps. The technical report further investigates the issues involved in using a haptic interface for simulating 3D painting, and the issues in surface parameterization for texture mapping with application to 3D painting. A survey of some work related to 3D painting, haptic rendering and surface parameterization for texture mapping is presented.  1",
    "neighbors": [
      691
    ],
    "mask": "Validation"
  },
  {
    "node_id": 601,
    "label": 2,
    "text": "Enhancing Supervised Learning with Unlabeled Data In many practical learning scenarios, there is  a small amount of labeled data along with  a large pool of unlabeled data. Many supervised  learning algorithms have been developed  and extensively studied. We present  a new \"co-training\" strategy for using unlabeled  data to improve the performance  of standard supervised learning algorithms.  Unlike much of the prior work, such as the  co-training procedure of Blum and Mitchell  (1998), we do not assume there are two redundant  views both of which are sufficient for  perfect classification. The only requirement  our co-training strategy places on each supervised  learning algorithm is that its hypothesis  partitions the example space into a set of  equivalence classes (e.g. for a decision tree  each leaf defines an equivalence class). We  evaluate our co-training strategy via experiments  using data from the UCI repository.  1. Introduction  In many practical learning scenarios, there is a small amount of labeled data along with a lar...",
    "neighbors": [
      391,
      609,
      643,
      891
    ],
    "mask": "Train"
  },
  {
    "node_id": 602,
    "label": 0,
    "text": "Spatial Agents Implemented in a Logical Expressible Language In this paper, we present a multi-layered architecture  for spatial and temporal agents. The focus is  laid on the declarativity of the approach, which  makes agent scripts expressive and well understandable.  They can be realized as (constraint) logic  programs. The logical description language is able  to express actions or plans for one and more autonomous  and cooperating agents for the RoboCup  (Simulator League). The system architecture hosts  constraint technology for qualitative spatial reasoning,  but quantitative data is taken into account, too.  The basic (hardware) layer processes the agent's  sensor information. An interface transfers this lowlevel  data into a logical representation. It provides  facilities to access the preprocessed data and supplies  several basic skills. The second layer performs  (qualitative) spatial reasoning. On top of this, the  third layer enables more complex skills such as  passing, offside-detection etc. At last, the fourth  layer establishes acting as a team both by emergent  and explicit cooperation. Logic and deduction provide  a clean means to specify and also to implement  teamwork behavior.  1",
    "neighbors": [
      184,
      363
    ],
    "mask": "Train"
  },
  {
    "node_id": 603,
    "label": 5,
    "text": "A Parametric Alternative to Grids for Occupancy-Based World Modeling In the paper, we consider an occupancy-based approach for range data fusion, as it is used in mobile robotics. We tackle the major problem of this approach, which is the redundancy of stored and processed data caused by using the grid representation of the occupancy function, by proposing a parametric piece-wise linear representation. When applied to the vision-based world exploration, the new representation is shown to have advantages over the former one, which include its suitability for radial range data, its efficiency in representing and fusing range data, and its convenience for navigation map extraction. The proposed technique is implemented on a mobile robot, Boticelli. The results obtained from running the robot are presented. 1 Introduction  In mobile robot world exploration, the occupancybased  approach is one of the most commonly used [7, 13, 3, 2, 8, 4, 11]. In this approach, the exploration policy is determined by the occupancy model of the world which is built from the r...",
    "neighbors": [
      1048
    ],
    "mask": "Train"
  },
  {
    "node_id": 604,
    "label": 2,
    "text": "The Cambridge University Spoken Document Retrieval System This paper describes the spoken document retrieval system that we have been developing and assesses its performance using automatic transcriptions of about 50 hours of broadcast news data. The recognition engine is based on the HTK broadcast news transcription system and the retrieval engine is based on the techniques developed at City University. The retrieval performance over a wide range of speech transcription error rates is presented and a number of recognition error metrics that more accurately reflect the impact of transcription errors on retrieval accuracy are defined and computed. The results demonstrate the importance of high accuracy automatic transcription. The final system is currently being evaluated on the 1998 TREC-7 spoken document retrieval task. 1.",
    "neighbors": [
      483,
      755,
      796
    ],
    "mask": "Validation"
  },
  {
    "node_id": 605,
    "label": 2,
    "text": "Automatic Text Detection and Tracking in Digital Video Text which either appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this paper we present algorithms for detecting and tracking text components that appear within digital video frames. Our system implements a scale-space feature extractor that feeds an artificial neural processor to extract textual regions and track their movement over time. The extracted regions can then be used as input to an appropriate Optical Character Recognition system which produces indexible keywords. Keywords: Text Detection, Text Tracking, Video Indexing, Digital Libraries, Neural Network, Wavelet The support of this effort by the Department of Defense under contract MDA 9049-6C-1250 is gratefully acknowledged.  1 Introduction  The increasing availability of online digital imagery and video has rekindled interest in the problems of how to index multimedia informa...",
    "neighbors": [
      8,
      840,
      1173
    ],
    "mask": "Validation"
  },
  {
    "node_id": 606,
    "label": 2,
    "text": "Evolving Personal Agent Environments To Reduce Internet Information Overload: Initial Considerations this paper we will address the other major information overload problem, arising from habitual surfing. We will present our proposed research programme and also outline and support a potential solution. So far this issue has been far less visited and researched, though with the ever increasing use of computers at home and in the workplace, as well as the imminent arrival of information appliances, this may well change [6].",
    "neighbors": [
      314,
      780,
      952
    ],
    "mask": "Train"
  },
  {
    "node_id": 607,
    "label": 4,
    "text": "Active User Interfaces For Building Decision-Theoretic Systems Knowledge elicitation/acquisition continues to be a bottleneck to constructing decisiontheoretic  systems. Methodologies and techniques for incremental elicitation/acquisition of  knowledge especially under uncertainty in support of users' current goals is desirable. This  paper presents PESKI, a probabilistic expert system development environment. PESKI provides  users with a highly interactive and integrated suite of intelligent knowledge engineering  tools for decision-theoretic systems. From knowledge acquisition, data mining, and verification  and validation to a distributed inference engine for querying knowledge, PESKI is based  on the concept of active user interfaces -- actuators to the human-machine interface. PESKI  uses a number of techniques to reduce the inherent complexity of developing a cohesive, realworld  knowledge-based system. This is accomplished by providing multiple communication  modes for human-computer interaction and the use of a knowledge representation endowed  with the ability to detect problems with the knowledge acquired and alert the user to these  possible problems. We discuss PESKI's use of these intelligent assistants to help users with  the acquisition of knowledge especially in the presence of uncertainty.",
    "neighbors": [
      1114
    ],
    "mask": "Validation"
  },
  {
    "node_id": 608,
    "label": 1,
    "text": "G-Algorithm for Extraction of Robust Decision Rules-Children's Postoperative Intra-Atrial Arrhythmia Case Study Clinical medicine is facing a challenge of knowledge discovery from the growing volume of data. In this paper, a data mining algorithm (G-algorithm) is proposed for extraction of robust rules that can be used in clinical practice for better understanding and prevention of unwanted medical events. The G-algorithm is applied to the data set obtained for children born with a malformation of the heart (univentricular heart). As the result of the Fontan surgical procedure, designed to palliate the children, 10%--35% of patients postoperatively develop an arrhythmia known as the intra-atrial reentrant tachycardia. There is an obvious need to identify the children that may develop the tachycardia before the surgery is performed. Prior attempts to identify such children with statistical techniques have been unrewarding. The G-algorithm discussed in this paper shows that there exists an unambiguous relationship between measurable features and the tachycardia. The data set used in this study shows that, for 78.08% of infants, the occurrence of tachycardia can be accurately predicted. The authors' prior computational experience with diverse medical data sets indicates that the percentage of accurate predictions may become even higher if data on additional features is collected for a larger data set.",
    "neighbors": [
      1081
    ],
    "mask": "Train"
  },
  {
    "node_id": 609,
    "label": 3,
    "text": "Text Classification from Labeled and Unlabeled Documents using EM  This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.  We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%.",
    "neighbors": [
      167,
      242,
      347,
      379,
      391,
      395,
      403,
      407,
      437,
      523,
      601,
      891,
      1090,
      1240
    ],
    "mask": "Train"
  },
  {
    "node_id": 610,
    "label": 3,
    "text": "Socratenon and its Application to the Learning of Italian Language tudents (the product was developed through a cooperation between universities in Salerno and Belgrade). This paper presents the basic elements of the Socratenon application and implementation philosophy, and discusses its possibilities in the general languagelearning environment. It describes three different experiments and explains the lessons learned. The stress is on the statistical analysis of success of those who used our Web-based product and those who relied on the classical approaches.  1 Introduction  Rapid growth of Internet as a medium and Internet technologies has led to the point when education can be detached from humans and books as the only possessors of knowledge. From the early days, Internet has been exploited in educational institutions for dissemination of research results, and knowledge in general. First shapes were unpolished and required a lot of attention from the users. Such sources of knowledge also included wrestling with several resources of",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 611,
    "label": 1,
    "text": "The Omnipresence of Case-Based Reasoning in Science and Application A surprisingly large number of research disciplines have contributed towards the development of knowledge on lazy problem solving, which ischaracterized by its storage of ground cases and its demand driven response to queries. Case-based reasoning (CBR) is an alternative, increasingly popular approach for designing expert systems that implements this approach. This paper lists pointers to some contributions in some related disciplines that o er insights for CBR research. We then outline a small number of Navy applications based on this approach that demonstrate its breadth of applicability. Finally, we list a few successful and failed attempts to apply CBR, and list some predictions on the future roles of CBR in applications. 1 Case-Based Reasoning Case-based reasoning (CBR) is a multi-disciplinary subject that focuses on the reuse of experiences (i.e., cases). It is di cult to nd consensus on more detailed de nitions of CBR because it means di erent things to di erent groups of people. For example, consider its interpretation by the following three groups: Cognitive Scientists: CBR is a plausible high-level model for cognitive processing (Kolodner,",
    "neighbors": [
      414,
      731,
      1075,
      1135
    ],
    "mask": "Train"
  },
  {
    "node_id": 612,
    "label": 2,
    "text": "Intelligent Anticipated Exploration of Web Sites In this paper we describe a web search agent, called Global Search Agent (hereafter  GSA for short). GSA integrates and enhances several search techniques in order to  achieve significant improvements in the user-perceived quality of delivered information as  compared to usual web search engines. GSA features intelligent merging of relevant documents  from different search engines, anticipated selective exploration and evaluation of  links from the current resuk set, automated derivation of refined queries based on user relevance  feedback. System architecture as well as experimental accounts are also illustrated.",
    "neighbors": [
      216,
      412,
      897
    ],
    "mask": "Train"
  },
  {
    "node_id": 613,
    "label": 1,
    "text": "From Regularization Operators to Support Vector Kernels We derive the correspondence between regularization operators used in Regularization Networks and Hilbert Schmidt Kernels appearing in Support Vector Machines. More specifically, we prove that the Green's Functions associated with regularization operators are suitable Support Vector Kernels with equivalent regularization properties. As a by--product we show that a large number of Radial Basis Functions namely conditionally positive definite functions may be used as Support Vector kernels. 1 INTRODUCTION  Support Vector (SV) Machines for pattern recognition, regression estimation and operator inversion exploit the idea of transforming into a high dimensional feature space where they perform a linear algorithm. Instead of evaluating this map explicitly, one uses Hilbert Schmidt Kernels k(x; y) which correspond to dot products of the mapped data in high dimensional space, i.e.  k(x; y) = (\\Phi(x) \\Delta \\Phi(y)) (1) with \\Phi : R  n  ! F denoting the map into feature space. Mostly, this m...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 614,
    "label": 4,
    "text": "Mining High-Speed Data Streams Many organizations today have more than very large databases; they have databases that grow without limit at a rate of several million records per day. Mining these continuous data streams brings unique opportunities, but also new challenges. This paper describes and evaluates VFDT, an anytime system that builds decision trees using constant memory and constant time per example. VFDT can incorporate tens of thousands of examples per second using o#-the-shelf hardware. It uses Hoe#ding bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner. We study VFDT's properties and demonstrate its utility through an extensive set of experiments on synthetic data. We apply VFDT to mining the continuous stream of Web access data from the whole University of Washington main campus.",
    "neighbors": [
      251
    ],
    "mask": "Validation"
  },
  {
    "node_id": 615,
    "label": 1,
    "text": "GTM: The Generative Topographic Mapping Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis which is based on a linear transformations between the latent space and the data space. In this paper we introduce a form of non-linear latent variable model called the Generative Topographic Mapping for which the parameters of the model can be determined using the EM algorithm. GTM provides a principled alternative to the widely used Self-Organizing Map (SOM) of Kohonen (1982), and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multi-phase oil pipeline.  GTM: The Generative Topographic Mapping 2 1 Introduction  Many data sets exhibit significant correlations between the variables. One way to capture such structure is to model the distribution of the data in term...",
    "neighbors": [
      50
    ],
    "mask": "Validation"
  },
  {
    "node_id": 616,
    "label": 2,
    "text": "Concept Indexing - A Fast Dimensionality Reduction Algorithm with Applications to Document Retrieval & Categorization In recent years, we have seen a tremendous growth in the volume of text documents available on the Internet, digital libraries, news sources, and company-wide intranets. This has led to an increased interest in developing meth-ods that can efficiently categorize and retrieve relevant information. Retrieval techniques based on dimensionality reduction, such as Latent Semantic Indexing (LSI), have been shown to improve the quality of the information being retrieved by capturing the latent meaning of the words present in the documents. Unfortunately, the high computa-tional requirements of LSI and its inability to compute an effective dimensionality reduction in a supervised setting limits its applicability. In this paper we present a fast dimensionality reduction algorithm, called concept indexing (CI) that is equally effective for unsupervised and supervised dimensionality reduction. CI computes a k-dimensional representation of a collection of documents by first clustering the documents into k groups, and then using the cen-troid vectors of the clusters to derive the axes of the reduced k-dimensional space. Experimental results show that the dimensionality reduction computed by CI achieves comparable retrieval performance to that obtained using LSI, while requiring an order of magnitude less time. Moreover, when CI is used to compute the dimensionality reduction in a supervised setting, it greatly improves the performance of traditional classification algorithms such as C4.5 and kNN. 1",
    "neighbors": [
      128,
      228,
      545,
      726,
      1049,
      1064
    ],
    "mask": "Validation"
  },
  {
    "node_id": 617,
    "label": 2,
    "text": "Data Visualization, Indexing and Mining Engine - A Parallel Computing Architecture for Information Processing Over the Internet ion : : : : : : : : : : : 9 3.2 Spatial Representation Of the System's Networks : : : : : : : : : : : : : : : : : : : : : : 10 3.3 Display And Interaction Mechanisms : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11 3.3.1 Overviews, orientation, and network abstraction : : : : : : : : : : : : : : : : : : 11 3.3.2 Other Navigation And Orientation Tools : : : : : : : : : : : : : : : : : : : : : : : 12 3.3.3 Head-tracked Stereoscopic Display : : : : : : : : : : : : : : : : : : : : : : : : : : 12 4 Web Access of Geographic Information System Data 13  4.1 Functions Provided by GIS2WEB : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 13 4.2 Structure : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 14 5 ParaCrawler --- Parallel Web Searching With Machine Learning 15  5.1 A Machine Learning Approach Towards Web Searching : : : : : : : : : : : : : : : : : : 15 6 DUSIE --- Interactive Content-based Web Structuring 16  6.1 Creating t...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 618,
    "label": 3,
    "text": "The Gateway System: Uniform Web Based Access to Remote Resources Exploiting our experience developing the WebFlow system, we designed the Gateway system to provide seamless and secure access to computational resources at ASC MSRC. The Gateway follows our commodity components strategy, and it is implemented as a modern three-tier system. Tier 1 is a highlevel front end for visual programming, steering, run-time data analysis and visualization that is built on top of the Web and OO commodity standards. Distributed object-based, scalable, and reusable Web server and Object broker middleware forms Tier 2. Back-end services comprise Tier 3. In particular, access to high-performance computational resources is provided by implementing the emerging standard for metacomputing API.  1. Introduction  The last few years have seen the growing power and capability of commodity computing and communication technologies largely driven by commercial distributed information systems. All of them can be abstracted to a three-tier model with largely independent clients c...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 619,
    "label": 0,
    "text": "Extending Agent UML Protocol Diagrams this paper is to present some new features that we propose. Several features deal with reliability such as triggering actions and exception handling. These new features are in fact proposed due to our work in electronic commerce and in supply chain management [16]. As much as possible, we apply these new features to needs in the supply chain management example",
    "neighbors": [
      264,
      874
    ],
    "mask": "Train"
  },
  {
    "node_id": 620,
    "label": 0,
    "text": "Automated Discovery of Concise Predictive Rules for Intrusion Detection This paper details an essential component of a multi-agent distributed knowledge network system  for intrusion detection. We describe a distributed intrusion detection architecture, complete with a  data warehouse and mobile and stationary agents for distributed problem-solving to facilitate building,  monitoring, and analyzing global, spatio-temporal views of intrusions on large distributed systems. An  agent for the intrusion detection system, which uses a machine learning approach to automated discovery  of concise rules from system call traces, is described.  We use a feature vector representation to describe the system calls executed by privileged processes.  The feature vectors are labeled as good or bad depending on whether or not they were executed during  an observed attack. A rule learning algorithm is then used to induce rules that can be used to monitor  the system and detect potential intrusions. We study the performance of the rule learning algorithm on  this task with an...",
    "neighbors": [
      197,
      567,
      962
    ],
    "mask": "Train"
  },
  {
    "node_id": 621,
    "label": 1,
    "text": "Reinforcement Learning for a Vision Based Mobile Robot Reinforcement learning systems improve behaviour based on scalar rewards from a critic. In this work vision based behaviours, servoing and wandering, are learned through a Q-learning method which handles continuous states and actions. There is no requirement for camera calibration, an actuator model, or a knowledgeable teacher. Learning through observing the actions of other behaviours improves learning speed. Experiments were performed on a mobile robot using a real-time vision system. 1 Introduction Collision free wandering and visual servoing are building blocks for purposeful robot behaviours such as foraging, target pursuit and landmark based navigation. Visual servoing consists of moving some part of a robot to a desired position using visual feedback [15]. Wandering is an environment exploration behaviour [6]. In this work we demonstrate real-time learning of wandering and servoing on a real robot. Learning eliminates the calibration process and leads to flexible behaviour....",
    "neighbors": [
      60
    ],
    "mask": "Train"
  },
  {
    "node_id": 622,
    "label": 0,
    "text": "Multiagent Systems Engineering: A Methodology And Language for Designing Agent Systems This paper overviews MaSE and provides a high-level introduction to one critical component used within MaSE, the Agent Modeling Language. Details on the Agent Definition Language and detailed agent design are left for a future paper.",
    "neighbors": [
      573,
      941,
      957
    ],
    "mask": "Train"
  },
  {
    "node_id": 623,
    "label": 5,
    "text": "On the Non-Linear Optimization of Projective Motion Using Minimal Parameters I address the problem of optimizing projective motion over a minimal  set of parameters. Most of the existing works overparameterize the problem.",
    "neighbors": [
      370
    ],
    "mask": "Test"
  },
  {
    "node_id": 624,
    "label": 0,
    "text": "Antisocial Bidding in Repeated Vickrey Auctions In recent years auctions have become more and more important in the eld  of multiagent systems as useful mechanisms for resource allocation and task  assignment. In many cases the Vickrey (second-price sealed-bid) auction is  used as a protocol that prescribes how the individual agents have to interact  in order to come to an agreement. The main reasons for choosing the Vickrey  auction are the low bandwidth and time consumption due to just one round  of bidding and the existence of a dominant bidding strategy under certain  conditions. We show that the Vickrey auction, despite its theoretical benets,  is inappropriate if \\antisocial\" agents participate in the auction process. More  specically, an antisocial attitude for economic agents that makes reducing  the prot of competitors their main goal besides maximizing their own prot  is introduced. Under this novel condition, agents need to deviate from the  dominant truth-telling strategy. This report presents a strategy for bidders...",
    "neighbors": [
      383,
      667
    ],
    "mask": "Train"
  },
  {
    "node_id": 625,
    "label": 0,
    "text": "A Multi-Agent Architecture for an Intelligent Website in Insurance . In this paper a multi-agent architecture for intelligent Websites is presented and applied in insurance. The architecture has been designed and implemented using the compositional development method for multi-agent systems DESIRE. The agents within this architecture are based on a generic broker agent model. It is shown how it can be exploited to design an intelligent Website for insurance, developed in co-operation with the software company Ordina Utopics and an insurance company. 1 Introduction  An analysis of most current business Websites from the perspectives of marketing and customer relations suggests that Websites should become more active and personalised, just as in the nonvirtual case where contacts are based on human servants. Intelligent agents provide the possibility to reflect at least a number of aspects of the nonvirtual situation in a simulated form, and, in addition, enables to use new opportunities for one-to-one marketing, integrated in the Website. The generic a...",
    "neighbors": [
      257
    ],
    "mask": "Train"
  },
  {
    "node_id": 626,
    "label": 1,
    "text": "Incremental Learning of Control Knowledge For Nonlinear Problem Solving In this paper we advocate a learning method where a deductive  and an inductive strategies are combined to efficiently learn control  knowledge. The approach consists of initially bounding the explanation  to a predetermined set of problem solving features. Since there is no  proof that the set is sufficient to capture the correct and complete explanation  for the decisions, the control rules acquired are then refined,  if and when applied incorrectly to new examples. The method is especially  significant as it applies directly to nonlinear problem solving, where  the search space is complete. We present hamlet, a system where we  implemented this learning method, within the context of the prodigy  architecture. hamlet learns control rules for individual decisions corresponding  to new learning opportunities offered by the nonlinear problem  solver that go beyond the linear one. These opportunities involve, among  other issues, completeness, quality of plans, and opportunistic decision  making. Finally, we show empirical results illustrating hamlet's learning  performance.",
    "neighbors": [
      137,
      690,
      988
    ],
    "mask": "Train"
  },
  {
    "node_id": 627,
    "label": 2,
    "text": "Server Selection on the World Wide Web We evaluate server selection methods in a Web environment, modeling a digital library which makes use of existing Web search servers rather than building its own index. The evaluation framework portrays the Web realistically in several ways. Its search servers index real Web documents, are of various sizes, cover different topic areas and employ different retrieval methods. Selection is based on statistics extracted from the results of probe queries submitted to each server. We evaluate published selection methods and a new method for enhancing selection based on expected search server effectiveness. Results show CORI to be the most effective of three published selection methods. CORI selection steadily degrades with fewer probe queries, causing a drop in early precision of as much as 0#05 (one relevant document out of 20). Modifying CORI selection based on an estimation of expected effectiveness disappointingly yields no significant improvement in effectiveness. However, modifying COR...",
    "neighbors": [
      241,
      496,
      931,
      1003
    ],
    "mask": "Train"
  },
  {
    "node_id": 628,
    "label": 4,
    "text": "Single Display Groupware: A Model for Co-present Collaboration We introduce a model for supporting collaborative work between people that are physically close to each other. We call this model Single Display Groupware (SDG). In this paper, we describe this model, comparing it to more traditional remote collaboration. We describe the requirements that SDG places on computer technology, and our understanding of the benefits and costs of SDG systems. Finally, we describe a prototype SDG system that we built and the results of a usability test we ran with 60 elementary school children.  Keywords  CSCW, Single Display Groupware, children, educational applications, input devices, Pad++, KidPad.  INTRODUCTION  In the early 1970's, researchers at Xerox PARC created an atmosphere in which they lived and worked with technology of the future. When the world's first personal computer, the Alto, was invented, it had only a single keyboard and mouse. This fundamental design legacy has carried through to nearly all modern computer systems. Although networks have...",
    "neighbors": [
      316,
      946,
      1130
    ],
    "mask": "Test"
  },
  {
    "node_id": 629,
    "label": 2,
    "text": "Proverb: The Probabilistic Cruciverbalist We attacked the problem of solving crossword puzzles by computer: given a set of clues and a crossword grid, try to maximize the number of words correctly filled in. After an analysis of a large collection of puzzles, we decided to use an open architecture in which independent programs specialize in solving specific types of clues, drawing on ideas from information retrieval, database search, and machine learning. Each expert module generates a (possibly empty) candidate list for each clue, and the lists are merged together and placed into the grid by a centralized solver. We used a probabilistic representation throughout the system as a common interchange language between subsystems and to drive the search for an optimal solution.  Proverb, the complete system, averages 95.3% words correct and 98.1% letters correct in under 15 minutes per puzzle on a sample of 370 puzzles taken from the New York Times and several other puzzle sources. This corresponds to missing roughly 3 words or 4 l...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 630,
    "label": 3,
    "text": "An Analytical Study of Object Identifier Indexing The object identifier index of an object-oriented database system is typically 20% of the size of the database itself, and for large databases, only a small part of the index fits in main memory. To avoid index retrievals becoming a bottleneck, efficient buffering strategies are needed to minimize the number of disk accesses. In this report, we develop analytical cost models which we use to find optimal sizes of index page buffer and index entry cache, for different memory sizes, index sizes, and access patterns. Because existing buffer hit estimation models are not applicable for index page buffering in the case of tree based indexes, we have also developed an analytical model for index page buffer performance. The cost gain from using the results in this report is typically in the order of 200-300%. Thus, the results should be of valuable use in optimizers and tools for configuration and tuning of object-oriented database systems. 1 Introduction  In a large OODB with logical object i...",
    "neighbors": [
      374,
      503,
      760
    ],
    "mask": "Train"
  },
  {
    "node_id": 631,
    "label": 2,
    "text": "Parsing As Information Compression By Multiple Alignment, Unification And Search: SP52 This article presents and discusses examples illustrating aspects of the proposition, described in the accompanying article (Wolff, 1998), that parsing may be understood as information compression by multiple alignment, unification and search (ICMAUS). The later examples show that the multiple alignment framework as described in the accompanying article has expressive power which is comparable with other `context sensitive' systems used to represent the syntax of natural languages. In all the examples, the SP52 model, described in the accompanying article, is capable of finding an alignment which is intuitively `correct' and assigning to it a `compression score' which is higher than for any other alignment. The congruance which has been found between this range of alignments produced by a system which is dedicated to information compression and what is judged to be `correct' in terms of linguistic intuition lends support to the hypothesis that linguistic intuition is itself a product o...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 632,
    "label": 3,
    "text": "Representing School Timetabling in a Disjunctive Logic Programming Language In this paper, we show how school timetabling problems with preferences originating from didactical, organisational and personal considerations can be represented in a highly declarative and natural way, using an extension of disjunctive datalog by strong and weak (integrity) constraints. 1 Introduction  Almost all people have come across school timetabling during their lives. For a long time almost all school timetables were created manually, a timeconsuming task, which often yielded suboptimal schedules. In the last thirty years, however, systems have been designed which automate timetable creation.  Timetabling in general is the problem of finding suitable combinations of two or more types of resources which have to be at the same place during several discrete periods of time and which have to satisfy various additional constraints. Some of these constraints are strict while some are not (the latter express preferences or desiderata). In the case of school timetabling problems these...",
    "neighbors": [
      413
    ],
    "mask": "Train"
  },
  {
    "node_id": 633,
    "label": 3,
    "text": "The Isomorphism Between a Class of Place Transition Nets and a Multi-Plane State Machine Agent Model Recently we introduced a multi-plane state machine model of an agent, released an implementation  of the model, and designed several applications of the agent framework. In this paper  we address the translation from the Petri net language to the Blueprint language used for agent  description as well as the translation from Blueprint to Petri Nets. The simulation of a class  of Place Transition Nets is part of an effort to create an agent--based workflow management  system.  Contents  1 Introduction 1 2 A Multi-Plane State Machine Agent Model 3 2.1 Bond Core . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Bond Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3 Bond Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.4 Using Planes to Implement Facets of Behavior . . . . . . . . . . . . . . . . . . . . . 5 3 Simulating a Class of Place-Transition Nets on the Bond ...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 634,
    "label": 3,
    "text": "Indexing and Querying XML Data for Regular Path Expressions With the advent of XML as a standard for data  representation and exchange on the Internet,  storing and querying XML data becomes more  and more important. Several XML query languages  have been proposed, and the common  feature of the languages is the use of regular  path expressions to query XML data. This  poses a new challenge concerning indexing and  searching XML data, because conventional approaches  based on tree traversals may not meet  the processing requirements under heavy access  requests. In this paper, we propose a  new system for indexing and storing XML data  based on a numbering scheme for elements.  This numbering scheme quickly determines the  ancestor-descendant relationship between elements  in the hierarchy of XML data. We also  propose several algorithms for processing regular  path expressions, namely, (1) ##-Join for  searching paths from an element to another,  (2) ##-Join for scanning sorted elements and  attributes to find element-attribute pairs, and  (3) ##-Join for finding Kleene-Closure on repeated  paths or elements. The ##-Join algorithm  is highly effective particularly for searching  paths that are very long or whose lengths  are unknown. Experimental results from our  prototype system implementation show that the  proposed algorithms can process XML queries  with regular path expressions by up to an or-  #  This work was sponsored in part by National Science Foundation CAREER Award (IIS-9876037) and Research Infrastructure program EIA-0080123. The authors assume all responsibility for the contents of the paper.  Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its...",
    "neighbors": [
      306,
      364,
      420,
      488,
      1162,
      1254
    ],
    "mask": "Train"
  },
  {
    "node_id": 635,
    "label": 4,
    "text": "Visual Interpretation of Hand Gestures for Human-Computer Interaction: A Review The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. We survey the literature on vision-based hand gesture recognition within the context of its role in HCI. The number of approaches to video-based hand gesture recognition has grown in recent years. Thus, the need for systematization and analysis of different aspects of gestural interaction has developed. We discuss a complete model of hand gestures that possesses both spatial and dynamic properties of human hand gestures and can accommodate for all their natural types. Two classes of models that have been employed for interpretation of hand gestures for HCI are considered. The first utilizes 3D models of the human hand, while the second relies on the appearance of the human hand in the image. Investigation of model parameters and analysis feat...",
    "neighbors": [
      54,
      175,
      234,
      699,
      833
    ],
    "mask": "Train"
  },
  {
    "node_id": 636,
    "label": 0,
    "text": "The BOID Architecture - Conflicts Between Beliefs, Obligations, Intentions and Desires In this paper we introduce the so-called Beliefs-Obligations-Intentions-Desires or BOID architecture. It contains feedback loops to consider all eects of actions before committing to them, and mechanisms to resolve conflicts between the outputs of its four components. Agent types such as realistic or social agents correspond to specific types of conflict resolution embedded in the BOID architecture.",
    "neighbors": [
      964,
      1269
    ],
    "mask": "Train"
  },
  {
    "node_id": 637,
    "label": 3,
    "text": "DyDa: Dynamic Data Warehouse Maintenance in a Fully Concurrent Environment Data warehouse is an emerging technology to support high-level decision making by gathering data from several distributed information sources into one repository. In dynamic environments, data warehouses must be maintained in order to stay consistent with the underlying sources. Recently proposed view maintenance algorithms tackle the problem of data warehouse maintenance under concurrent source data updates.While the view synchronization is to handle non-concurrent source schema changes. However, the concurrency between interleaved schema changes and data updates still remain unexplored problems.  In this paper, we propose a solution framework called DyDa that successfully addresses this problem. The DyDa framework detects concurrent schema changes by a broken query scheme and conicting concurrent data updates by a local timestamp scheme. A fundamental idea of the DyDa framework is the development of a two-layered architecture that separates the concerns for concurrent data updates and concurrent schema changes handling without imposing any restrictions on the sourse update transactions. At the lower level of the framework, it employs a local compensation algorithm to handle concurrent data updates, and a metadata name mapping strategy to handle concurrent source rename operations. At the higher level, it addresses the problem of concurrent source drop operations. For the latter problem, we design a strategy for the detection and correction of such concurrency and nd an executable plan for the aected updates. We then develop a new view adaption algorithm, called Batch-VA for execution of such plan to incrementally adapt the view. Put together, these algorithms are the rst to provide a complete solution to data warehouse management in a fully concurrent environment....",
    "neighbors": [
      455,
      718
    ],
    "mask": "Train"
  },
  {
    "node_id": 638,
    "label": 2,
    "text": "Using Common Hypertext Links to Identify the Best Phrasal Description of Target Web Documents This paper describes previous work which studied and compared the distribution of words in web documents with the distribution of words in \"normal\" flat texts. Based on the findings from this study it is suggested that the traditional IR techniques cannot be used for web search purposes the same way they are used for \"normal\" text collections, e.g. news articles. Then, based on these same findings, I will describe a new document description model which exploits valuable anchor text information provided on the web that is ignored by the traditional techniques.  The problem  Amitay (1997) has found, through a corpus analysis of a 1000 web pages that the lexical distribution in documents which were written especially for the web (home pages), is significantly different than the lexical distribution observed in a corpus of \"normal\" English language (the British National Corpus - 100,000,000 words). For example, in the web documents collection there were some HTML files which contained no v...",
    "neighbors": [
      990,
      1017,
      1099
    ],
    "mask": "Train"
  },
  {
    "node_id": 639,
    "label": 1,
    "text": "Speeding up Relational Reinforcement Learning Through the Use of an Incremental First Order Decision Tree Learner Relational reinforcement learning (RRL) is a learning technique  that combines standard reinforcement learning with inductive logic  programming to enable the learning system to exploit structural knowledge  about the application domain.",
    "neighbors": [
      158
    ],
    "mask": "Test"
  },
  {
    "node_id": 640,
    "label": 2,
    "text": "Searchable Words on the Web In designing data structures for text databases, it is valuable to know how many different words are likely to be encountered in a particular collection. For example, vocabulary accumulation is central to index construction for text database systems; it is useful to be able to estimate the space requirements and performance characteristics of the main-memory data structures used for this task. However, it is not clear how many distinct words will be found in a text collection or whether new words will continue to appear after inspecting large volumes of data. We propose practical definitions of a word, and investigate new word occurrences under these models in a large text collection. We inspected around two billion word occurrences in 45 gigabytes of world-wide web documents, and found just over 9.74 million different words in 5.5 million documents; overall, 1 word in 200 was new. We observe that new words continue to occur, even in very large data sets, and that choosing stricter definitions of what constitutes a word has only limited impact on the number of new words found.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 641,
    "label": 4,
    "text": "Perceptual User Interfaces For some time, graphical user interfaces (GUIs) have been the dominant platform for human computer interaction. The GUI-based style of interaction has made computers simpler and easier to use, especially for office productivity applications where computers are used as tools to accomplish specific tasks. However, as the way we use computers changes and computing becomes more pervasive and ubiquitous, GUIs will not easily support the range of interactions necessary to meet users \u2019 needs. In order to accommodate a wider range of scenarios, tasks, users, and preferences, we need to move toward interfaces that are natural, intuitive, adaptive, and unobtrusive. The aim of a new focus in HCI, called Perceptual User Interfaces (PUIs), is to make human-computer interaction more like how people interact with each other and with the world. This paper describes the emerging PUI field and then reports on three PUImotivated projects: computer vision-based techniques to visually perceive relevant information about the user. 1.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 642,
    "label": 1,
    "text": "The Chromatic Structure of Natural Scenes We applied Independent Component Analysis (ICA) to hyperspectral images in  order to learn an ecient representation of color in natural scenes. In the spectra of  single pixels, the algorithm found basis functions that had broadband spectra, as well  as basis functions that were similar to natural reectance spectra. When applied to  small image patches, the algorithm found basis functions that were achromatic and  others with overall chromatic variation along lines in color space, indicating color  opponency. The directions of opponency were not strictly orthogonal. Comparison  1  with Principal Component Analysis (PCA) on the basis of statistical measures such as  average mutual information, kurtosis and entropy, shows that the ICA transformation  results in much sparser coecients and gives higher coding eciency. Our ndings  suggest that non-orthogonal opponent encoding of photoreceptor signals leads to  higher coding eciency, and that ICA may be used to reveal the underlying stati...",
    "neighbors": [
      1151
    ],
    "mask": "Train"
  },
  {
    "node_id": 643,
    "label": 2,
    "text": "Using Unlabeled Data to Improve Text Classification One key difficulty with text classification learning algorithms is that they require many hand-labeled examples to learn accurately. This dissertation demonstrates that supervised learning algorithms that use a small number of labeled examples and many inexpensive unlabeled examples can create high-accuracy text classifiers. By assuming that documents are created by a parametric generative model, Expectation-Maximization (EM) finds local maximum a posteriori models and classifiers from all the data -- labeled and unlabeled. These generative models do not capture all the intricacies of text; however on some domains this technique substantially improves classification accuracy, especially when labeled data are sparse. Two problems arise from this basic approach. First, unlabeled data can hurt performance in domains where the generative modeling assumptions are too strongly violated. In this case the assumptions can be made more representative in two ways: by modeling sub-topic class structure, and by modeling super-topic hierarchical class relationships. By doing so, model probability and classification accuracy come into correspondence, allowing unlabeled data to improve classification performance. The second problem is that even with a representative model, the improvements given by unlabeled data do not sufficiently compensate for a paucity of labeled data. Here, limited labeled data provide EM initializations that lead to low-probability models. Performance can be significantly improved by using active learning to select high-quality initializations, and by using alternatives to EM that avoid low-probability local maxima.",
    "neighbors": [
      279,
      379,
      395,
      601,
      864,
      865,
      1240
    ],
    "mask": "Test"
  },
  {
    "node_id": 644,
    "label": 5,
    "text": "Using an Expressive Description Logic: FaCT or Fiction? Description Logics form a family of formalisms closely related to semantic networks but with the distinguishing characteristic that the semantics of the concept description language is formally defined, so that the subsumption relationship between two concept descriptions can be computed by a suitable algorithm. Description Logics have proved useful in a range of applications but their wider acceptance has been hindered by their limited expressiveness and the intractability of their subsumption algorithms. This paper addresses both these issues by describing a sound and complete tableaux subsumption testing algorithm for a relatively expressive Description Logic which, in spite of the logic's worst case complexity, has been shown to perform well in realistic applications. 1 INTRODUCTION Description Logics (DLs) form a family of formalisms which have grown out of knowledge representation techniques using frames and semantic networks",
    "neighbors": [
      82,
      173,
      188
    ],
    "mask": "Train"
  },
  {
    "node_id": 645,
    "label": 4,
    "text": "Putting the Feel in `Look and Feel' Haptic devices are now  commercially available and thus  touch has become a potentially  realistic solution to a variety of  interaction design challenges. We  reportonanexperimental  investigation of the use of touch as  a way of reducing visual overload in  the conventional desktop. In a  two-phase study, we investigated  the use of the PHANToM haptic  device as a means of interacting  with a conventional graphical user  interface. The first experiment  compared the effects of four  different haptic augmentations on  user performance in a simple  targeting task. The second  experiment involved a more  ecologically oriented searching and  scrolling task. Results indicated that  the haptic effects did not improve  users performance in terms of task  completion time. However, the  number of errors made was  significantly reduced. Subjective  workload measures showed that  participants perceived many aspects  of workload as significantly less  with haptics. The results are  described and the implications for  the use of haptics in user interface  design are discussed.  Subject  Areas:  Multimodal interaction  Augmented reality  Empirical (quantitative)  Evaluation  Input devices  Interaction technology  CHI 2000  Home Page  1of2 13/09/99 11:36  CHI 2000 Call For Papers http://msrconf.microsoft.com/sigchi/AuthorListEditProcess.asp  Tactile or gestural I/O  CLICK HERE to view the anonymous version of  your paper that we have received. Your paper  must be viewable by the standard Adobe Acrobat  viewer. If you cannot get this to work, then your  hardcopy version will be sent to reviewers.  Remember, in addition to your electronic  submission, you must submit your paper in  hardcopy form to CHI 2000 , c/o Mary  Czerwinski , Microsoft Research , One Microsoft  Way , Redmond, ...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 646,
    "label": 0,
    "text": "Second-Order Networks for Wall-Building Agents This paper describes robust neurocontrollers for groups of agents that perform construction tasks. They enable agents to balance multiple goals, perform sequences of actions and survive while building walls, corridors, intersections, and briar patches.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 647,
    "label": 0,
    "text": "A Formal Specification of dMARS   The Procedural Reasoning System (PRS) is the best established agent  architecture currently available. It has been deployed in many major industrial  applications, ranging from fault diagnosis on the space shuttle to air traffic management  and business process control. The theory of PRS-like systems has also  been widely studied: within the intelligent agents research community, the beliefdesire  -intention (BDI) model of practical reasoning that underpins PRS is arguably  the dominant force in the theoretical foundations of rational agency. Despite  the interest in PRS and BDI agents, no complete attempt has yet been made  to precisely specify the behaviour of real PRS systems. This has led to the development  of a range of systems that claim to conform to the PRS model, but which  differ from it in many important respects. Our aim in this paper is to rectify this  omission. We provide an abstract formal model of an idealised dMARS system  (the most recent implementation of the PRS...",
    "neighbors": [
      953,
      964
    ],
    "mask": "Test"
  },
  {
    "node_id": 648,
    "label": 4,
    "text": "Designing StoryRooms: Interactive Storytelling Spaces for Children Limited access to space, costly props, and complicated authoring technologies are among the many reasons why children can rarely enjoy the experience of authoring roomsized interactive stories. Typically in these kinds of environments, children are restricted to being story participants, rather than story authors. Therefore, we have begun the development of \"StoryRooms,\" room-sized immersive storytelling experiences for children. With the use of low-tech and high-tech storytelling elements, children can author physical storytelling experiences to share with other children. In the paper that follows, we will describe our design philosophy, design process with children, the current technology implementation and example StoryRooms.  KEYWORDS  Augmented Environments, Storytelling, Children, Educational Applications, Participatory Design, Cooperative Inquiry  INTRODUCTION  A child sits in a playroom. She tells a story to her dolls about her family. Another child sits at the dinner table wit...",
    "neighbors": [
      213,
      946,
      1074
    ],
    "mask": "Test"
  },
  {
    "node_id": 649,
    "label": 2,
    "text": "Topic-Driven Crawlers: Machine Learning Issues Topic driven crawlers are increasingly seen as a way to address the scalability limitations of universal  search engines, by distributing the crawling process across users, queries, or even client computers.",
    "neighbors": [
      1,
      216,
      224,
      281,
      457,
      662,
      774,
      968,
      1017,
      1264
    ],
    "mask": "Train"
  },
  {
    "node_id": 650,
    "label": 3,
    "text": "Database Replication Using Epidemic Update Due to severe performance penalties associated with synchronous replication, there is an increasing  interest in asynchronous replica management protocols in which database transactions are executed locally,  and the effects of these transactions are incorporated asynchronously on remote database copies.  However, the asynchronous protocols currently in use either do not guarantee consistency and serializability  as needed by transactional semantics or they impose restrictions on placement of data and on which  data objects can be updated. In this paper we investigate an epidemic update protocol that guarantees  consistency and serializability in spite of a write-anywhere capability. We conducted experiments on a detailed  simulation of a distributed, replicated database to evaluate this protocol. Our results establish that  this epidemic approach is indeed a viable alternative to traditional eager update protocols for a distributed  database environment where consistency and full seri...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 651,
    "label": 3,
    "text": "Pushing Reactive Services to XML Repositories using Active Rules Push technology, i.e., the ability of sending relevant information to clients in reaction to new events, is a fundamental aspect of modern information systems; XML is rapidly emerging as the widely adopted standard for information exchange and representation and hence, several XML-based protocols have been defined and are the object of investigation at W3C and throughout commercial organizations. In this paper, we propose the new concept of active XML rules for \"pushing\" reactive services to XML-enabled repositories. Rules operate on XML documents and deliver information to interested remote users in reaction to update events occurring at the repository site. The proposed mechanism assumes the availability of XML repositories supporting a standard XML query language, such as XQuery that is being developed by the W3C; for the implementation of the reactive components, it capitalizes on the use of standard DOM events and of the SOAP interchange standard to enable the remote installation of active rules. A simple protocol is proposed for subscribing and unsubscribing remote rules.",
    "neighbors": [
      101,
      475,
      745,
      1162
    ],
    "mask": "Test"
  },
  {
    "node_id": 652,
    "label": 1,
    "text": "State-Based SHOSLIF for Indoor Visual Navigation In this paper, we investigate vision-based navigation using the Self-organizing Hierarchical Optimal Subspace Learning and Inference Framework (SHOSLIF) that incorporates states and a visual attention mechanism. The problem is formulated as an observation-driven Markov model (ODMM) which is realized through recursive partitioning regression. A stochastic recursive partition tree (SRPT), which maps an preprocessed current input raw image and the previous state into the current state and the next control signal, is used for efficient recursive partitioning regression. The SRPT learns incrementally: each learning sample is learned or rejected \"onthe -fly\". The proposed scheme has been successfully applied to indoor navigation. Keywords: Vision-based navigation, incremental learning, eigen-subspace method, content-based retrieval, observation-driven Markov model, nearest neighbor regression.  1 1 Introduction  Much progress has been made in autonomous navigation of mobile robots, both ind...",
    "neighbors": [
      244
    ],
    "mask": "Train"
  },
  {
    "node_id": 653,
    "label": 2,
    "text": "Learning Search Engine Specific Query Transformations for Question Answering We introduce a method for learning query transformations that improves the ability to retrieve answers to questions from an information retrieval system. During the training stage the method involves automatically learning phrase features for classifying questions into different types, automatically generating candidate query transformations from a training set of question/answer pairs, and automatically evaluating the candidate transforms on target information retrieval systems such as real-world general purpose search engines. At run time, questions are transformed into a set of queries, and re-ranking is performed on the documents retrieved. We present a prototype search engine, Tritus, that applies the method to web search engines. Blind evaluation on a set of real queries from a web search engine log shows that the method significantly outperforms the underlying web search engines as well as a commercial search engine specializing in question answering. Keywords Web search, quer...",
    "neighbors": [
      511,
      587,
      1017,
      1134
    ],
    "mask": "Train"
  },
  {
    "node_id": 654,
    "label": 1,
    "text": "A Cellular Neural Associative Array for Symbolic Vision . A system which combines the descriptional power of symbolic representations with the parallel and distributed processing model of cellular automata and the speed and robustness of connectionist symbol processing is described. Following a cellular automata based approach, the aim of the system is to transform initial symbolic descriptions of patterns to corresponding object level descriptions in order to identify patterns in complex or noisy scenes. A learning algorithm based on a hierarchical structural analysis is used to learn symbolic descriptions of objects. The underlying symbolic processing engine of the system is a neural based associative memory (AURA) which use enables the system to operate in high speed. In addition, the use of distributed representations allow both efficient inter-cellular communications and compact storage of rules. 1 Introduction  One of the basic features of syntactic and structural pattern recognition systems is the use of the structure of the patterns...",
    "neighbors": [
      217
    ],
    "mask": "Test"
  },
  {
    "node_id": 655,
    "label": 2,
    "text": "Keyword Spices: A New Method for Building Domain-Specific Web Search Engines This paper presents a new method for building domain-specific web search engines. Previous methods eliminate irrelevant documents from the pages accessed using heuristics based on human knowledge about the domain in question. Accordingly, they are hard to build and can not be applied to other domains. The keyword spice method, in contrast, improves search performance by adding",
    "neighbors": [
      347
    ],
    "mask": "Train"
  },
  {
    "node_id": 656,
    "label": 2,
    "text": "Greedy strikes back: Improved Facility Location Algorithms A fundamental facility location problem is to choose the location of facilities, such as industrial plants and warehouses, to minimize the cost of satisfying the demand for some commodity. There are associated costs for locating the facilities, as well as transportation costs for distributing the commodities. We assume that the transportation costs form a metric. This problem is commonly referred to as the uncapacitated facility location (UFL) problem. Applications to bank account location and clustering, as well as many related pieces of work, are discussed by Cornuejols, Nemhauser and Wolsey [?]. Recently, the first constant factor approximation algorithm for this problem was obtained by Shmoys, Tardos and Aardal [?]. We show that a simple greedy heuristic combined with the algorithm by Shmoys, Tardos and Aardal, can be used to obtain an approximation guarantee of 2.408. We discuss a few variants of the problem, demonstrating better approximation factors for restricted versions of the...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 657,
    "label": 0,
    "text": "Transactions and Electronic Commerce . Electronic Commerce is a rapidly growing area that is gaining more and more importance not only in the interrelation of businesses (business--to--business Electronic Commerce) but also in the everyday consumption of individuals performed via the Internet (business--to-- customer Electronic Commerce). Since Electronic Commerce is a very interdisciplinary area, it has a lot of impacts to various communities. The goal of this paper is to identify and to summarize the impact of Electronic Commerce from a database transaction point of view and to highlight open problems in transaction management arising in Electronic Commerce applications by reflecting the discussions of the working group \"Transactions and Electronic Commerce\" held at the TDD Workshop. 1 Motivation The exchange of electronic data between companies has been an important issue in business interactions for quite a while. However, the recent proliferation of the Internet together with the rapid propagation of pers...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 658,
    "label": 3,
    "text": "Cost-based Query Scrambling for Initial Delays Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scramblin...",
    "neighbors": [
      842
    ],
    "mask": "Train"
  },
  {
    "node_id": 659,
    "label": 3,
    "text": "Consistent Query Answers in Inconsistent Databases In this paper we consider the problem of the logical characterization of the notion of consistent answer in a relational database that may violate given integrity constraints. This notion is captured in terms of the possible repaired versions of the database. A method for computing consistent answers is given and its soundness and completeness (for some classes of constraints and queries) proved. The method is based on an iterative procedure whose termination for several classes of constraints is proved as well. 1 Introduction  Integrity constraints capture an important normative aspect of every database application. However, it is often the case that their satisfaction cannot be guaranteed, allowing for the existence of inconsistent database instances. In that case, it is important to know which query answers are consistent with the integrity constraints and which are not. In this paper, we provide a logical characterization of consistent query answers in relational databases that may...",
    "neighbors": [
      29
    ],
    "mask": "Test"
  },
  {
    "node_id": 660,
    "label": 2,
    "text": "Fact or fiction: Content classification for digital libraries The World-Wide Web (WWW) is a vast repository of information, much of which is valuable  but very often hidden to the user. The anarchic nature of the WWW presents unique challenges  when it comes to information extraction and categorization. We view the WWW as a valuable  resource for the gathering of information for Digital Libraries. In this paper we will describe the  process of extracting and classifying information from the WWW for the purpose of integrating  it into digital libraries. Our eorts focus on ways to automatically classify news articles according  to whether they present opinions or reported facts. We describe and evaluate a system in  development that automatically classies and recommends Web news articles from sports and  politics domains.  1",
    "neighbors": [
      347
    ],
    "mask": "Train"
  },
  {
    "node_id": 661,
    "label": 5,
    "text": "Learned Models for Continuous Planning We are interested in the nature of activity -- structured behavior of nontrivial duration -- in intelligent agents. We believe that the development of activity is a continual process in which simpler activities are composed, via planning, to form more sophisticated ones in a hierarchical fashion. The success or failure of a planner depends on its models of the environment, and its ability to implement its plans in the world. We describe an approach to generating dynamical models of activity from real-world experiences and explain how they can be applied towards planning in a continuous state space. 1 Introduction  We are interested in the problem of how activity emerges in an intelligent agent. We believe that activity plays a critical role in the development of many high-level cognitive structures: classes, concepts, and language, to name a few. Thus, it is our goal to derive and implement a theory of the development of activity in intelligent agents and implement it using the Pioneer...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 662,
    "label": 2,
    "text": "Adaptive Retrieval Agents: Internalizing Local Context and Scaling up to the Web . This paper focuses on two machine learning abstractions springing from ecological models: (i) evolutionary adaptation by local selection, and (ii) selective query expansion by internalization of environmental signals. We first outline a number of experiments pointing to the feasibility and performance of these methods on a general class of graph environments. We then describe how these methods have been applied to the intelligent retrieval of information distributed across networked environments. In particular, the paper discusses a novel distributed evolutionary algorithm and representation used to construct populations of adaptive Web agents. These InfoSpiders search on-line for information relevant to the user, by traversing hyperlinks in an autonomous and intelligent fashion. They can adapt to the spatial and temporal regularities of their local context. Our results suggest that InfoSpiders could complement current search engine technology by starting up where search engines stop...",
    "neighbors": [
      1,
      53,
      235,
      255,
      649,
      1000,
      1005,
      1099,
      1264
    ],
    "mask": "Train"
  },
  {
    "node_id": 663,
    "label": 0,
    "text": "Semantics for an Agent Communication Language . We address the issue of semantics for an agent communication language. In particular, the semantics of Knowledge Query Manipulation Language (KQML) is investigated. KQML is a language and protocol to support communication between (intelligent) software agents. Based on ideas from speech act theory, we present a semantic description for KQML that associates \"cognitive\" states of the agent with the use of the language's primitives (performatives). We have used this approach to describe the semantics for the whole set of reserved KQML performatives. Our research offers a method for a speech act theory-based semantic description of a language of communication acts. Languages of communication acts address the issue of communication between software applications at a level of abstraction that could prove particularly useful to the emerging software agents paradigm of software design and development. 1 Introduction This research is concerned with communication between software agents [13]...",
    "neighbors": [
      34,
      296,
      367,
      765,
      962,
      1028,
      1067,
      1236
    ],
    "mask": "Test"
  },
  {
    "node_id": 664,
    "label": 1,
    "text": "A SNoW-Based Face Detector A novel learning approach for human face detection using a network of linear units is presented. The SNoW learning architecture is a sparse network of linear functions over a pre-defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of features. A wide range of face images in different poses, with different expressions and under different lighting conditions are used as a training set to capture the variations of human faces. Experimental results on commonly used benchmark data sets of a wide range of face images show that the SNoW-based approach outperforms methods that use neural networks, Bayesian methods, support vector machines and others. Furthermore, learning and evaluation using the SNoW-based method are significantly more efficient than with other methods.",
    "neighbors": [
      428
    ],
    "mask": "Test"
  },
  {
    "node_id": 665,
    "label": 4,
    "text": "Learning Visual Models of Social Engagement We introduce a face detector for wearable computers that exploits constraints in face scale and orientation imposed by the proximity of participants in near social interactions. Using this method we describe a wearable system that perceives \"social engagement,\" i.e., when the wearer begins to interact with other individuals. Our experimental system proved > 90% accurate when tested on wearable video data captured at a professional conference. Over 300 individuals were captured during social engagement, and the data was separated into independent training and test sets. A metric for balancing the performance of face detection, localization, and recognition in the context of a wearable interface is discussed.  Recognizing social engagement with a user's wearable computer provides context data that can be useful in determining when the user is interruptible. In addition, social engagement detection may be incorporated into a user interface to improve the quality of mobile face recognition software. For example, the user may cue the face recognition system in a socially graceful way by turning slightly away and then toward a speaker when conditions for recognition are favorable.  1",
    "neighbors": [
      307,
      334,
      497,
      738,
      987
    ],
    "mask": "Train"
  },
  {
    "node_id": 666,
    "label": 4,
    "text": "Detection and Tracking of Facial Features in Video Sequences . This work presents a real time system for detection and tracking of facial  features in video sequences. Such system may be used in visual communication  applications, such as teleconferencing, virtual reality, intelligent interfaces, humanmachine  interaction, surveillance, etc. We have used a statistical skin-color model to  segment face-candidate regions in the image. The presence or absence of a face in  each region is verified by means of an eye detector, based on an efficient template  matching scheme . Once a face is detected, the pupils, nostrils and lip corners are  located and these facial features are tracked in the image sequence, performing real  time processing.  1",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 667,
    "label": 0,
    "text": "Task Assignment in Multiagent Systems based on Vickrey-type Auctioning and Leveled Commitment Contracting . A key problem addressed in the area of multiagent systems  is the automated assignment of multiple tasks to executing agents. The  automation of multiagent task assignment requires that the individual  agents (i) use a common protocol that prescribes how they have to interact  in order to come to an agreement and (ii) x their nal agreement  in a contract that species the commitments resulting from the assignment  on which they agreed. The work reported in this paper is part of a  broader research eort aiming at the design and analysis of approaches to  automated multiagent task assignment that combine auction protocols  and leveled commitment contracts. The primary advantage of such approaches  is that they are applicable in a broad range of realistic scenarios  in which knowledge-intensive negotiation among agents is not feasible  and in which unforeseeable future environmental changes may require  agents to breach their contracts. Examples of standard auction protocols  are the...",
    "neighbors": [
      383,
      624
    ],
    "mask": "Train"
  },
  {
    "node_id": 668,
    "label": 4,
    "text": "A Novel User Interface for Group Collaboration Flexible user interfaces that can be customized to meet the needs of the task at hand are particularly important for real-time group collaboration. This paper presents the user interface of the DISCIPLE (DIstributed System for Collaborative Information Processing and LEarning) system for synchronous groupware along with the multimodal human-computer interface enhancement. DISCIPLE supports sharing of JavaBeans-compliant components [17], i.e., beans and applets, which at runtime get imported into the shared workspace and can be interconnected into more complex components. As a result, importing various components allows user tailoring of the human-computer interface. We present a software architecture for customization of both grouplevel and application-level interfaces. The applicationlevel interface includes a management system for sharing multiple modalities across concurrent applications. This multimodal management system is loadable on demand yet strongly embedded in the DISCIPLE f...",
    "neighbors": [
      764
    ],
    "mask": "Test"
  },
  {
    "node_id": 669,
    "label": 1,
    "text": "Machine Learning in Automated Text Categorisation The automated categorisation (or classification) of texts into topical categories has a long history, dating back at least to the early \u201960s. Until the late \u201980s, the most effective approach to the problem seemed to be that of manually building automatic classifiers by means of knowledgeengineering techniques, i.e. manually defining a set of rules encoding expert knowledge on how to classify documents under a given set of categories. In the \u201990s, with the booming production and availability of on-line documents, automated text categorisation has witnessed an increased and renewed interest, prompted by which the machine learning paradigm to automatic classifier construction has emerged and definitely superseded the knowledge-engineering approach. Within the machine learning paradigm, a general inductive process (called the learner) automatically builds a classifier (also called the rule, or the hypothesis) by \u201clearning\u201d, from a set of previously classified documents, the characteristics of one or more categories. The advantages of this approach are a very good effectiveness, a considerable savings in terms of expert manpower, and domain independence. In this survey we look at the main approaches that have been taken towards automatic text categorisation within the general machine learning paradigm. Issues pertaining to document indexing, classifier construction, and classifier evaluation, will be discussed in detail. A final section will be devoted to the techniques that have specifically been devised for an emerging application such as the automatic classification of Web pages into \u201cYahoo!-like \u201d hierarchically structured sets of categories. Categories and Subject Descriptors: H.3.1 [Information storage and retrieval]: Content analysis and indexing\u2014Indexing methods; H.3.3 [Information storage and retrieval]: Information search and retrieval\u2014Information filtering; H.3.3 [Information storage and retrieval]: Systems and software\u2014Performance evaluation (efficiency and effectiveness); I.2.3 [Artificial",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 670,
    "label": 1,
    "text": "Relational Learning Techniques for Natural Language Information Extraction The recent growth of online information available in the form of natural language documents creates a greater need for computing systems with the ability to process those documents to simplify access to the information. One type of processing appropriate for many tasks is information extraction, a type of text skimming that retrieves specific types of information from text. Although information extraction systems have existed for two decades, these systems have generally been built by hand and contain domain specific information, making them difficult to port to other domains. A few researchers have begun to apply machine learning to information extraction tasks, but most of this work has involved applying learning to pieces of a much larger system. This paper presents a novel rule representation specific to natural language and a learning system, Rapier, which learns information extraction rules. Rapier takes pairs of documents and filled templates indicating the information to be ext...",
    "neighbors": [
      1232
    ],
    "mask": "Test"
  },
  {
    "node_id": 671,
    "label": 1,
    "text": "The Global Dimensionality of Face Space Low-dimensional representations of sensory signals are key to solving many of the computational problems encountered in high-level vision. Principal Component Analysis (PCA) has been used in the past to derive such compact representations for the object class of human faces. Here, with an interpretation of PCA as a probabilistic model, we employ two objective criteria to study its generalization properties in the context of large frontal-pose face databases. We find that the eigenfaces, the eigenspectrum, and the generalization depend strongly on the ensemble composition and size, with statistics for populations as large as 5500, still not stationary. Further, the assumption of mirror symmetry of the ensemble improves the quality of the results substantially in the low-statistics regime, and is also essential in the high-statistics regime. We employ a perceptual criterion and argue that, even with large statistics, the dimensionality of the PCA subspace necessary for adequate represent...",
    "neighbors": [
      1108
    ],
    "mask": "Validation"
  },
  {
    "node_id": 672,
    "label": 0,
    "text": "Learning Network Designs for Asynchronous Teams . An asynchronous team (A-Team) is a network of agents (workers) and memories (repositories for the results of work). It is possible to design A-Teams to be effective in solving difficult computational problems. The main design issues are: \"What structure should the network have?\" and \"What should be the complement of agents?\" In the past, the structure-issue was resolved by intuition and experiment. This paper describes a procedure by which good structures can be learned from experience. The procedure is based on the use of regular expressions for encoding the capabilities of networks. 1 Introduction  An Asynchronous Team (A-Team) is a problem solving architecture consisting of collections of agents and memories connected into a strongly cyclic directed network. The memories form the nodes of the network, the agents form the arcs. Figure 1 below shows such a network. Each memory holds a population of trial solutions. The solutions are not necessarily solutions to the overall problem t...",
    "neighbors": [
      829
    ],
    "mask": "Test"
  },
  {
    "node_id": 673,
    "label": 3,
    "text": "Relational Transducers for Electronic Commerce Electronic commerce is emerging as one of the major Websupported applications requiring database support. We introduce and study high-level declarative specifications of business models, using an approach in the spirit of active databases. More precisely, business models are specified as  relational transducers that map sequences of input relations into sequences of output relations. The semantically meaningful trace of an input-output exchange is kept as a sequence of log relations. We consider problems motivated by electronic commerce applications, such as log validation, verifying temporal properties of transducers, and comparing two relational transducers. Positive results are obtained for a restricted class of relational transducers called Spocus transducers (for semi-positive outputs and cumulative state). We argue that despite the restrictions, these capture a wide range of practically significant business models.  1 Introduction  Electronic commerce is emerging as a major Web-s...",
    "neighbors": [
      585,
      745
    ],
    "mask": "Train"
  },
  {
    "node_id": 674,
    "label": 2,
    "text": "Boosting for Document Routing RankBoost is a recently proposed algorithm for learning ranking functions. It is simple to implement and has strong justifications from computational learning theory. We describe the algorithm and present initial experimental results on applying it to the document routing problem. The first set of results applies RankBoost to a text representation produced using modern term weighting methods. Performance of RankBoost is somewhat inferior to that of a state-of-the-art routing algorithm which is, however, more complex and less theoretically justified than RankBoost. RankBoost achieves comparable performance to the state-of-the-art algorithm when combined with feature or example selection heuristics. Our second set of results examines the behavior of RankBoost when it has to learn not only a ranking function but also all aspects of term weighting from raw data. Performance is usually, though not always, less good here, but the term weighting functions implicit in the resulting ranking fun...",
    "neighbors": [
      341,
      1007,
      1141
    ],
    "mask": "Train"
  },
  {
    "node_id": 675,
    "label": 0,
    "text": "Configuration Management for Multi-Agent Systems As heterogeneous distributed systems, multi-agent systems present some challenging configuration management issues. There are the problems of knowing how to allocate agents to computers, launch them on remote hosts, and once the agents have been launched, how to monitor their runtime status so as to manage computing resources effectively.  In this paper, we present the RETSINA Configuration Manager, RECoMa. We describe its architecture, how it uses agent infrastructure such as service discovery, to assist the multi-agent system administrator in allocating, launching, and monitoring a heterogeneous distributed agent system in a distributed and networked computing environment.  1  1.",
    "neighbors": [
      132
    ],
    "mask": "Validation"
  },
  {
    "node_id": 676,
    "label": 3,
    "text": "Semantics Of (disjunctive) Logic Programs Based On Partial Evaluation SEMANTICS AND TRANSFORMATIONS  In this paper, we consider allowed disjunctive DATALOG  :  programs over some fixed function-free finite signature \\Sigma. In fact, in the semantical part of this paper, we consider only the ground instantiation of the programs, because we claim that any sensible semantics should assign the same meaning to a program P and its instantiation ground(P ). So the variables are seen only as a shorthand for denoting ground programs in a more compact way. This means that in the semantical part, we could as well have worked with propositional programs. However, in the computational part, it would be very inefficient to compute first the ground instantiation of the given program. Here we make use of the allowedness condition: every variable of the rule must occur also in a positive body literal. This guarantees that in every rule application, all variables are bound to a constant. It is  5 true that in this way we again manage to consider only ground programs. But...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 677,
    "label": 4,
    "text": "An Introduction to 3-D User Interface Design Three-dimensional user interface design is a critical component of any virtual environment (VE) application. In this paper, we present a broad overview of 3-D interaction and user interfaces. We discuss the effect of common VE hardware devices on user interaction, as well as interaction techniques for generic 3-D tasks and the use of traditional 2-D interaction styles in 3-D environments. We divide most userinteraction tasks into three categories: navigation, selection/manipulation, and system control. Throughout the paper, our focus is on presenting not only the available techniques but also practical guidelines for 3-D interaction design and widely held myths. Finally, we briefly discuss two approaches to 3-D interaction design and some example applications with complex 3-D interaction requirements. We also present an annotated online bibliography as a reference companion to this article.",
    "neighbors": [
      756
    ],
    "mask": "Validation"
  },
  {
    "node_id": 678,
    "label": 4,
    "text": "Guiding User Navigation in Virtual Environments Using Awareness of Virtual Off-Screen Space Navigation in virtual environments can be difficult. One contributing factor is the problem of user disorientation. Two major causes of this are the lack of navigation cues in the environment and problems with navigating too close to or through virtual world objects. Previous work has developed guidelines, informed by cinematography conventions, for the construction of virtual environments to aid user comprehension of virtual \"space\" to reduce user disorientation. This paper describes the validation of these guidelines via a user study involving a navigation task in a virtual \"maze\". Results suggest that the use of the guidelines can help reduce the incidences of user disorientation. However, the guidelines seemed to have little impact on users' abilities to construct 'cognitive maps' of the environment.",
    "neighbors": [
      376,
      565,
      786
    ],
    "mask": "Train"
  },
  {
    "node_id": 679,
    "label": 1,
    "text": "Training Reinforcement Neurocontrollers Using The Polytope Algorithm A new training algorithm is presented for delayed reinforcement learning problems that does not assume the existence of a critic model and employs the polytope optimization algorithm to adjust the weights of the action network so that a simple direct measure of the training performance is maximized. Experimental results from the application of the method to the pole balancing problem indicate improved training performance compared with critic-based and genetic reinforcement approaches. Keywords: reinforcement learning, neurocontrol, optimization, polytope algorithm, pole balancing, genetic reinforcement.  TRAINING REINFORCEMENT NEUROCONTROLLERS USING THE POLYTOPE ALGORITHM  Abstract  A new training algorithm is presented for delayed reinforcement learning problems that does not assume the existence of a critic model and employs the polytope optimization algorithm to adjust the weights of the action network so that a simple direct measure of the training performance is maximized. Exper...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 680,
    "label": 4,
    "text": "Steps Toward Accommodating Variable Position Tracking Accuracy in a Mobile Augmented Reality System The position-tracking accuracy of a location-aware mobile system can change dynamically as a function of the user's location and other variables specific to the tracker technology used. This is especially problematic for mobile augmented reality systems, which ideally require extremely precise position tracking for the user's head, but which may not always be able to achieve the necessary level of accuracy. While it is possible to ignore variable positional accuracy in an augmented reality user interface, this can make for a confusing system; for example, when accuracy is low, virtual objects that are nominally registered with real ones may be too far off to be of use.",
    "neighbors": [
      307,
      531,
      738,
      1043
    ],
    "mask": "Train"
  },
  {
    "node_id": 681,
    "label": 3,
    "text": "Path Constraints on Semistructured and Structured Data We present a class of path constraints of interest in connection with both structured and semi-structured databases, and investigate their associated implication problems. These path constraints are capable of expressing natural integrity constraints that are not only a fundamental part of the semantics of the data, but are also important in query optimization. We show that in semistructured databases, despite the simple syntax of the constraints, their associated implication problem is r.e. complete and finite implication problem is co-r.e. complete. However, we establish the decidability of the implication problems for several fragments of the path constraint language, and demonstrate that these fragments suffice to express important semantic information such as inverse relationships and local database constraints commonly found in object-oriented databases. We also show that in the presence of types, the analysis of path constraint implication becomes more delicate. We demonstrate so...",
    "neighbors": [
      23,
      585,
      1254
    ],
    "mask": "Train"
  },
  {
    "node_id": 682,
    "label": 1,
    "text": "Employing EM and Pool-Based Active Learning for Text Classification This paper shows how a text classifier's need for labeled training data can be reduced by a combination of active learning and Expectation Maximization (EM) on a pool of unlabeled data. Query-by-Committee is used to actively select documents for labeling, then EM with a naive Bayes model further improves classification accuracy by concurrently estimating probabilistic labels for the remaining unlabeled documents and using them to improve the model. We also present a metric for better measuring disagreement among committee members; it accounts for the strength of their disagreement and for the distribution of the documents. Experimental results show that our method of combining EM and active learning requires only half as many labeled training examples to achieve the same accuracy as either EM or active learning alone. Keywords:  text classification active learning unsupervised learning information retrieval  1 Introduction  In many settings for learning text classifiers, obtaining lab...",
    "neighbors": [
      347,
      865
    ],
    "mask": "Test"
  },
  {
    "node_id": 683,
    "label": 0,
    "text": "A Classification Scheme for Negotiation in Electronic Commerce In the last few years we have witnessed a surge of business-to-consumer and business-to-business commerce operated on the Internet. However many of these systems are often nothing more than electronic catalogues on which the user can choose a product which is made available for a fixed price. This modus operandi is clearly failing to exploit the full potential of electronic commerce. Against this background, we argue here that in the next few years we will see a new generation of systems emerge, based on automatic negotiation. In this paper we identify the main parameters on which any automatic negotiation depends. This classification schema is then used to categorise the subsequent papers in this book that focus on automatic negotiation.",
    "neighbors": [
      329
    ],
    "mask": "Validation"
  },
  {
    "node_id": 684,
    "label": 4,
    "text": "Towards a Living Lab research facility and a ubiquitous computing research programme Introduction  My interest in the topic of this workshop stems from my current involvement in setting up a new research facility at the Eindhoven University of Technology. This facility is called the 'Living Lab', and is quite similar to related projects around the globe in that it aims to study how people experience a ubiquitous computing environment, when they will inhabit it and use it for prolonged periods of time. The slogan of this development, is 'Vacation on Campus'.  The project is currently at the initiation phase. We have proposed a white paper [1] describing the concept and the research programme, and we are currently working to involve stakeholders from different departments of the TU/e, e.g., Architecture, Electrical Engineering, and Technology Management, and of the local industry. (E.g., Philips).  In the remaining of this position paper, I outline our research concept and our programme.  Concept - Vacation on Campus  The Living Lab, will be a cros",
    "neighbors": [
      86
    ],
    "mask": "Train"
  },
  {
    "node_id": 685,
    "label": 3,
    "text": "Theory of Answering Queries Using Views The problem of answering queries using views is to nd ecient methods of answering a query using a set of previously materialized views over the database, rather than accessing the database relations. The problem has recently received signicant attention because of its relevance to a wide variety of data management problems, such as query optimization, the maintenance of physical data independence, data integration and data warehousing. This article surveys the theoretical issues concerning the problem of answering queries using views.  1",
    "neighbors": [
      585
    ],
    "mask": "Test"
  },
  {
    "node_id": 686,
    "label": 0,
    "text": "CAST: Collaborative Agents for Simulating Teamwork Psychological studies on teamwork have shown that an effective team often can anticipate information needs of teammates based on a shared mental model. Existing multi-agent models for teamwork are limited in their ability to support proactive information exchange among teammates. To address this issue, we have developed and implemented a multi-agent architecture called CAST that simulates teamwork and supports proactive information exchange in a dynamic environment. We present a formal model for proactive information exchange. Knowledge regarding the structure and process of a team is described in a language called MALLET. Beliefs about shared team processes and their states are represented using Petri Nets. Based on this model, CAST agents offer information proactively to those who might need it using an algorithm called DIARG. Empirical evaluations using a multi-agent synthetic testbed application indicate that CAST enhances the effectiveness of teamwork among agents without sacrificing a high cost for communications. 1",
    "neighbors": [
      375,
      724
    ],
    "mask": "Train"
  },
  {
    "node_id": 687,
    "label": 0,
    "text": "Model Checking Multiagent Systems Model checking is a very successful technique which has been applied in the design and verification of finite state concurrent reactive processes. In this paper we show how this technique can be lifted to be applicable to multiagent systems. Our approach allows us to reuse the technology and tools developed in model checking, to design and verify multiagent systems in a modular and incremental way, and also to have a very efficient model checking algorithm. 1 Introduction  Model checking is a very successful automatic technique which has been devised for the design and verification of finite state reactive systems, e.g., sequential circuit designs, communication protocols, and safety critical control systems (see, e.g., [2]). There is evidence that model checking, when applicable, is far more successful than the other approaches to formal methods and verification (e.g., first order or inductive theorem proving, tableau based reasoning about modal satisfiability). Nowadays many very eff...",
    "neighbors": [
      40,
      724,
      808,
      953
    ],
    "mask": "Train"
  },
  {
    "node_id": 688,
    "label": 4,
    "text": "The Open Agent Architecture: A Framework for Building Distributed Software Systems The Open Agent Architecture (OAA), developed and used for several years at SRI International, makes it possible for software services to be provided through the cooperative efforts of distributed collections of autonomous agents. Communication and cooperation between agents are brokered by one or more facilitators, which are responsible for matching requests, from users and agents, with descriptions of the capabilities of other agents. Thus, it is not generally required that a user or agent know the identities, locations, or number of other agents involved in satisfying a request. OAA is structured so as to minimize the effort involved in creating new agents and \"wrapping\" legacy applications, written in various languages and operating on various platforms; to encourage the reuse of existing agents; and to allow for dynamism and flexibility in the makeup of agent communities. Distinguishing features of OAA as compared with related work include extreme flexibility in using facilitator-b...",
    "neighbors": [
      920,
      942,
      1235
    ],
    "mask": "Train"
  },
  {
    "node_id": 689,
    "label": 2,
    "text": "Building Efficient Wireless Sensor Networks with Low-Level Naming In most distributed systems, naming of nodes for low-level communication leverages topological location (such as node addresses)  and is independent of any application. In this paper, we investigate an emerging class of distributed systems where low-level communication does not rely on network topological location. Rather, low-level communication is based on attributes that are external to the network topology and relevant to the application. When combined with dense deployment of nodes, this kind of named data enables in-network processing for data aggregation, collaborative signal processing, and similar problems. These approaches are essential for emerging applications such as sensor networks where resources such as bandwidth and energy are limited. This paper is the first description of the software architecture that supports  named data and in-network processing in an operational, multi-application sensor-network. We show that approaches such as in-network aggregation and nested queries can significantly affect network traffic. In one experiment aggregation reduces traffic by up to 42% and nested queries reduce loss rates by 30%. Although aggregation has been previously studied in simulation, this paper demonstrates nested queries as another form of in-network processing, and it presents the first evaluation of these approaches over an operational testbed.",
    "neighbors": [
      469
    ],
    "mask": "Train"
  },
  {
    "node_id": 690,
    "label": 1,
    "text": "Using Multi-Strategy Learning to Improve Planning Efficiency and Quality",
    "neighbors": [
      626
    ],
    "mask": "Train"
  },
  {
    "node_id": 691,
    "label": 4,
    "text": "DAB: Interactive Haptic Painting with 3D Virtual Brushes We present a novel painting system with an intuitive haptic interface, which serves as an expressive vehicle for interactively creating painterly works. We introduce a deformable, 3D brush model, which gives the user natural control of complex brush strokes. The force feedback enhances the sense of realism and provides tactile cues that enable the user to better manipulate the paint brush. We have also developed a bidirectional, two-layer paint model that, combined with a palette interface, enables easy loading of complex blends onto our 3D virtual brushes to generate interesting paint effects on the canvas. The resulting system, DAB, provides the user with an artistic setting, which is conceptually equivalent to a real-world painting environment. Several users have tested DAB and were able to start creating original art work within minutes.",
    "neighbors": [
      600
    ],
    "mask": "Validation"
  },
  {
    "node_id": 692,
    "label": 3,
    "text": "Incremental Maintenance of Materialized OQL Views The importance of materialized views has grown significantly with the advent of data warehousing and OLAP technology. This increases the relevance of solutions to the problem of incrementally maintaining materialized views. So far, most work on this problem has been confined to relational settings. Proposals that apply to object databases have either used non-standard models or fallen short of providing a comprehensive framework. This paper contributes a solution to the incremental view maintenance problem for a large class of views expressed in OQL, the query language of the ODMG standard for object databases. The solution applies to immediate update propagation, and works for any update operation on views de ned over a substantial subset of ODMG types. The approach presented has been fully implemented and preliminary performance results are reported.",
    "neighbors": [
      397,
      1047
    ],
    "mask": "Train"
  },
  {
    "node_id": 693,
    "label": 1,
    "text": "Why Unary and Binary Operations in Logic: General Result Motivated by Interval-Valued Logics Traditionally, in logic, only unary and binary operations are used as basic ones -- e.g., \"not\", \"and\", \"or\" -- while the only ternary (and higher order) operations are the operations which come from a combination of unary and binary ones. For the classical logic, with the binary set of truth values f0; 1g, the possibility to express an arbitrary operation in terms of unary and binary ones is well known: it follows, e.g., from the well known possibility to express an arbitrary operation in DNF form. A similar representation result for [0; 1]-based logic was proven in our previous paper. In this paper, we expand this result to finite logics (more general than classical logic) and to multi-D analogues of the fuzzy logic -- both motivated by interval-valued fuzzy logics.  1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 694,
    "label": 4,
    "text": "An Information Search Cost Perspective for Designing Interfaces for Electronic Commerce This research helps web developers apply knowledge on information search costs to the design of a web site for selling consumer products or services. The goal of the research is to predict how subtle changes in the user interface design influence information search costs. An empirical study compared 1,411 choices subjects made regarding a business to patronize from paper and electronic telephone directories. The choices were contingent upon information search costs imposed by the media. By providing a theoretical basis for predicting differences in information search costs, this research helps designers create more effective web sites for achieving their marketing objectives.  1  An Information Search Cost Perspective for Designing Interfaces for Electronic Commerce INTRODUCTION  Now more than ever, the promise of electronic commerce and on-line shopping will depend to a great extent upon the user interface and how people interact with the computer. In particular, success will depend ...",
    "neighbors": [
      806
    ],
    "mask": "Test"
  },
  {
    "node_id": 695,
    "label": 2,
    "text": "Generating Extraction-Based Summaries from Hand-Written Summaries by Aligning Text Spans Human-quality text summarization systems based on sentence extraction are difficult to design because documents can differ along several dimensions, such as length, writing style and lexical usage. The lack of suitable corpora of extraction-based summaries makes it difficult to evaluate and improve existing algorithms. However, there are a large number of hand-written (not extraction-based) summaries available for news-wire stories. This paper discusses our work on generating a corpus of approximately 25,000 extractionbased summaries from hand-written summaries. We discuss how text-span alignment can be applied to this problem and how this problem differs from previous work on aligning parallel texts. In addition, we briefly analyze differences between handwritten and extracted summaries. 1 Introduction  Human-quality text summarization systems based on sentence extraction are difficult to design, and even more difficult to evaluate, because documents can differ along several dimension...",
    "neighbors": [
      517,
      548
    ],
    "mask": "Train"
  },
  {
    "node_id": 696,
    "label": 2,
    "text": "Estimating the Usefulness of Search Engines In this paper, we present a statistical method to estimate the usefulness of a search engine for any given query. The estimates can be used by a metasearch engine to choose local search engines to invoke. For a given query, the usefulness of a search engine in this paper is defined to be a combination of the number of documents in the search engine that are sufficiently similar to the query and the average similarity of these documents. Experimental results indicate that the proposed estimation method is quite accurate. 1 Introduction  Many search engines have been created on the Internet to help ordinary users find desired data. Each search engine has a corresponding database that defines the set of documents that can be searched by the search engine. Usually, an index for all documents in the database is created and stored in the search engine to speed up query processing. The amount of data in the Internet is huge (it is believed that by the end of 1997, there were more than 300 mil...",
    "neighbors": [
      115,
      224,
      271,
      433,
      435,
      477,
      496,
      510,
      526,
      579,
      792,
      897,
      1124,
      1165,
      1253
    ],
    "mask": "Test"
  },
  {
    "node_id": 697,
    "label": 3,
    "text": "A Framework for Ontology Integration One of the basic problems in the development of techniques for the semantic  web is the integration of ontologies. Indeed, the web is constituted by a variety of  information sources, each expressed over a certain ontology, and in order to extract  information from such sources, their semantic integration and reconciliation in terms  of a global ontology is required. In this paper, we address the fundamental problem  of how to specify the mapping between the global ontology and the local ontologies.  We argue that for capturing such mapping in an appropriate way, the notion of query  is a crucial one, since it is very likely that a concept in one ontology corresponds  to a view (i.e., a query) over the other ontologies. As a result query processing in  ontology integration systems is strongly related to view-based query answering in data  integration.  1",
    "neighbors": [
      553,
      1055
    ],
    "mask": "Test"
  },
  {
    "node_id": 698,
    "label": 4,
    "text": "The Amulet Environment: New Models for Effective User Interface Software Development Abstract\u2014The Amulet user interface development environment makes it easier for programmers to create highly-interactive, graphical user interface software for Unix, Windows and the Macintosh. Amulet uses new models for objects, constraints, animation, input, output, commands, and undo. The object system is a prototype-instance model in which there is no distinction between classes and instances or between methods and data. The constraint system allows any value of any object to be computed by arbitrary code and supports multiple constraint solvers. Animations can be attached to existing objects with a single line of code. Input from the user is handled by \u201cinteractor \u201d objects which support reuse of behavior objects. The output model provides a declarative definition of the graphics and supports automatic refresh. Command objects encapsulate all of the information needed about operations, including support for various ways to undo them. A key feature of the Amulet design is that all graphical objects and behaviors of those objects are explicitly represented at run-time, so the system can provide a number of high-level built-in functions, including automatic display and editing of objects, and external analysis and control of interfaces. Amulet integrates these capabilities in a flexible and effective manner. Index Terms\u2014Toolkits, user interface tools, user interface development environments, user interface management systems (UIMSs). 1",
    "neighbors": [
      316
    ],
    "mask": "Train"
  },
  {
    "node_id": 699,
    "label": 4,
    "text": "Eyes in the Interface Computer vision has a significant role to play in the human-computer interaction (HCI) devices of the future. All computer input devices serve one essential purpose. They transduce some motion or energy from a human agent into machine useable signals. One may therefore think of input devices as the `perceptual organs' by which computers sense the intents of their human users. We outline the role computer vision will play, highlight the impediments to the development of vision-based interfaces, and propose an approach for overcoming these impediments. Prospective vision research areas for HCI include human face recognition, facial expression interpretation, lip reading, head orientation detection, eye gaze tracking, three-dimensional finger pointing, hand tracking, hand gesture interpretation, and body pose tracking. For vision-based interfaces to make any impact, we will have to embark on an expansive approach which begins with the study of the interaction modality we seek to implement...",
    "neighbors": [
      635
    ],
    "mask": "Validation"
  },
  {
    "node_id": 700,
    "label": 1,
    "text": "Neurobotics Lab Research: Learning, Vision and Sonar Recognition with Mobile Robots This article provides an overview of research projects undertaken in the Neurobotics Laboratory at Boston University. We focus on applications of neural networks and other biomimetic techniques in sensory processing, navigation, and other tasks using mobile robots. These applications share some central themes: the inclusion of minimal assumptions about the robots and the environment; cross-validation of modules on a variety of robotics platforms and environments; and real-time operation using real robots.  Keywords: Mobile robots, looming, mobile robots, robot learning, Neural networks, ARTMAP, sensor fusion 1 Introduction  The Neurobotics Laboratory was founded in 1996 with the goal of applying neural networks and other biomimetic techniques to the control and guidance of wheeled mobile robot. Research in the lab covers various problems in the general area of autonomous mobile robotics, with an emphasis on navigation and control using biomimetic algorithms that operate in real-time wi...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 701,
    "label": 4,
    "text": "Spatially aware local communication in the RAUM system In this paper, we propose a new paradigm for local communication between devices in Ubiquitous Computing environments, assuming a multitude of computerized everyday appliances communicating with each other to solve tasks. This paradigm is based on the concept that the location of devices is central for the communication in such a scenario. Devices define their communication scope by spatial criteria. In our paradigm no explicit addressing or identification of communication partners is used. In comparison to traditional communication methods the approach eases routing and discovery problems and can be deployed in a highly dynamic environment without centralized services. We use the term local communication as inter-device communication in a physically restricted local area. This is well distinguish from the terms telecommunication as communication over distance where location information is explicitly hidden. The communication model (RAUM) introduced is based on the observ...",
    "neighbors": [
      465,
      520,
      1226
    ],
    "mask": "Validation"
  },
  {
    "node_id": 702,
    "label": 1,
    "text": "The Genetic Algorithm as a Discovery Engine: Strange Circuits and New Principles This paper examines the idea of a genetic or evolutionary algorithm being an inspirational or discovery engine. This is illustrated in the particular context of designing electronic circuits. We argue that by connecting pieces of logic together and testing them to see if they carry out the desired function it may be possible to discover new principles of design, and new algebraic techniques. This is illustrated in the design of binary circuits, particularly arithmetic functions, where we demonstrate that by evolving a hierarchical series of examples, it becomes possible to re-discover the well known ripple-carry principle for building adder circuits of any size. We also examine the much harder case of multiplication. We show also that extending the work into the field of multiple-valued logic, the genetic algorithm is able to produce fully working circuits that lie outside conventional algebra. In addition we look at the issue of principle extraction from evolved data.  1 Introduction ...",
    "neighbors": [
      1265
    ],
    "mask": "Train"
  },
  {
    "node_id": 703,
    "label": 0,
    "text": "Contextual Deontic Logic: Normative Agents, Violations and Independence this paper we discuss when and how to use deontic logic in multi agent systems",
    "neighbors": [
      1269
    ],
    "mask": "Validation"
  },
  {
    "node_id": 704,
    "label": 4,
    "text": "Preliminary Investigation of Wearable Computers for Task Guidance in Aircraft Inspection This paper describes a preliminary investigation of how the capabilities of wearable computers may be used to provide task guidance in mobile environments. Specifically, this study examined how the capabilities of wearable computers may be used to aid a user in an inspection task, using as a case study the procedural task of preflight inspection of a general aviation aircraft. Two different configurations of a computer-based, voiceactivated task guidance system and the current method of preflight inspection were compared and evaluated. Initial results demonstrate an over reliance on the computer by the pilots and indicate the importance of the user interface design to the performance of the inspectors. The paper concludes with recommendations on promising directions of research.  Keywords  task guidance, procedural tasks, aircraft inspection, computerized procedures, decision aiding, wearable computers  1. Introduction  Wearable computers combine portable, voiceactivated, wireless-netw...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 705,
    "label": 3,
    "text": "Temporal Aspects of Semistructured Data In many applications, information about the history of data and their dynamic aspects are just as important as static information. During the last years the increasing amount of information accessible through the Web has presented new challenges to academic and industrial research on database. In this context, data are either structured, when coming from relational or object-oriented databases, or partially or completely unstructured, when they consist of simple collections of text or image files. In the context of semistructured data, model and query languages must be extended in order to consider dynamic aspects. We present a model based on labeled graphs for representing changes in semistructured data and a SQL-like query language for querying it.",
    "neighbors": [
      336,
      402,
      1069
    ],
    "mask": "Test"
  },
  {
    "node_id": 706,
    "label": 3,
    "text": "Scalable Algorithms for Large Temporal Aggregation The ability to model time-varying natures is essential to many database applications such as data warehousing and mining. However, the temporal aspects provide many unique characteristics and challenges for query processing and optimization. Among the challenges is computing temporal aggregates, which is complicated by having to compute temporal grouping. In this paper, we introduce a variety of temporal aggregation algorithms that overcome major drawbacks of previous work. First, for small-scale aggregations, both the worst-case and average-case processing time have been improved significantly. Second, for large-scale aggregations, the proposed algorithms can deal with a database that is substantially larger than the size of available memory.",
    "neighbors": [
      205,
      389,
      795
    ],
    "mask": "Train"
  },
  {
    "node_id": 707,
    "label": 0,
    "text": "Intention Reconsideration Reconsidered Abstract. In this paper, we consider the issue of designing agents that successfully balance the amount of time spent in reconsidering their intentions against the amount of time spent acting to achieve them. Following a brief review of the various ways in which this problem has previously been analysed, we motivate and introduce a simple formal model of agents, which is closely related to the well-known belief-desire-intention model. In this model, an agent is explicitly equipped with mechanisms for deliberation and action selection, as well as a meta-level control function, which allows the agent to choose between deliberation and action. Using the formal model, we define what it means for an agent to be optimal with respect to a task environment, and explore how various properties of an agent\u2019s task environment can impose certain requirements on its deliberation and meta-level control components. We then show how the model can capture a number of interesting practical reasoning scenarios, and illustrate how our notion of meta-level control can easily be extended to encompass higherorder meta-level reasoning. We conclude with a discussion and pointers to future work. 1",
    "neighbors": [
      1244
    ],
    "mask": "Test"
  },
  {
    "node_id": 708,
    "label": 3,
    "text": "Incremental Maintenance for Materialized Views over Semistructured Data Semistructured data is not strictly typed like relational or object-oriented data and may be irregular or incomplete. It often arises in practice, e.g., when heterogeneous data sources are integrated or data is taken from the World Wide Web. Views over semistructured data can be used to filter the data and to restructure (or provide structure to) it. To achieve fast query response time, these views are often materialized. This paper studies incremental maintenance techniques for materialized views over semistructured data. We use the graph-based data model OEM and the query language Lorel, developed at Stanford, as the framework for our work. We propose a new algorithm that produces a set of queries that compute the changes to the view based upon a change to the source. We develop an analytic cost model and compare the cost of executing our incremental maintenance algorithm to that of recomputing the view. We show that for nearly all types of database updates, it is more efficient to a...",
    "neighbors": [
      431,
      745,
      1187
    ],
    "mask": "Train"
  },
  {
    "node_id": 709,
    "label": 3,
    "text": "Answering Queries by Semantic Caches There has been growing interest in semantic query caches to  aid in query evaluation. Semantic caches are simply the results of previously  asked queries, or selected relational information chosen by an evaluation  strategy, that have been cached locally. For complex environments  such as distributed, heterogeneous databases and data warehousing, the  use of semantic caches promises to help optimize query evaluation, increase  turnaround for users, and reduce network load and other resource  usage. We present a general logical framework for semantic caches. We  consider the use of all relational operations across the caches for answering  queries, and we consider the various ways to answer, and to partially  answer, a query by cache. We address when answers are in cache, when  answers in cache can be recovered, and the notions of semantic overlaps,  semantic independence, and semantic query remainder.  While there has been much work relevant to the use of semantic caches,  no one has addressed in conjunction the issues pertinent to the effective  use of semantic caches to evaluate queries. In some cases, this is due to  overly simplified assumptions, and in other cases to the lack of a formal  framework. We attempt to establish some of that framework here. Within  that framework, we illustrate the issues involved in using semantic caches  for query evaluation. We show various applications for semantic caches,  and relate the work to relevant areas.  1",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 710,
    "label": 4,
    "text": "Phidgets: Easy Development of Physical Interfaces through Physical Widgets Physical widgets or phidgets are to physical user interfaces what widgets are to graphical user interfaces. Similar to widgets, phidgets abstract and package input and output devices: they hide implementation and construction details, they expose functionality through a well-defined API, and they have an (optional) on-screen interactive interface for displaying and controlling device state. Unlike widgets, phidgets also require: a connection manager to track how devices appear on-line; a way to link a software phidget with its physical counterpart; and a simulation mode to allow the programmer to develop, debug and test a physical interface even when no physical device is present. Our evaluation shows that everyday programmers using phidgets can rapidly develop physical interfaces.",
    "neighbors": [
      1046
    ],
    "mask": "Train"
  },
  {
    "node_id": 711,
    "label": 3,
    "text": "Using LDAP Directory Caches this paper, we consider the problem of reusing cached LDAP directory entries for answering (declarative) LDAP queries. We develop a suite of query transformations that capture the semantics of LDAP queries, and design a sound and complete algorithm for determining whether a conjunctive LDAP query is cacheanswerable  using positive query templates. We demonstrate the practicality of our algorithm for real applications with a preliminary performance evaluation, based on sample queries from a directory enabled application at AT&T Labs. Cache-answerability is related to the problem of finding complete rewritings of a query using views (see, e.g., [8]). A key conceptual difference arises due to the fact that we do not seek a rewriting of the original query in terms of the (views in the) semantic cache description, but want to evaluate the original query against the cached directory instance. Further, the differences of the LDAP data model and query language make the previous results inapplicable; for details, see Section 2.",
    "neighbors": [
      304,
      752,
      1201
    ],
    "mask": "Train"
  },
  {
    "node_id": 712,
    "label": 2,
    "text": "A Mediation Infrastructure for Digital Library Services Digital library mediators allow interoperation between diverse information services. In this paper we describe a flexible and dynamic mediator infrastructure that allows mediators to be composed from a set of modules (\"blades\"). Each module implements a particular mediation function, such as protocol translation, query translation, or result merging. All the information used by the mediator, including the mediator logic itself, is represented by an RDF graph. We illustrate our approach using a mediation scenario involving a Dienst and a Z39.50 server, and we discuss the potential advantages and weaknesses of our framework.  KEYWORDS: mediator, wrapper, interoperability, component design  1 INTRODUCTION  Heterogeneity is one of the main challenges faced by digital libraries. Too often documents are stored in different formats, collections are searched with disparate query languages, search services are accessed with incompatibleprotocols, intellectual property protection and access sche...",
    "neighbors": [
      766
    ],
    "mask": "Validation"
  },
  {
    "node_id": 713,
    "label": 1,
    "text": "Fusion of Perceptual Cues for Robust Tracking of. . . The paradigm of perceptual fusion provides robust solutions to computer vision problems. By combining the outputs of multiple vision modules, the assumptions and constraints of each module are factored out to result in a more robust system overall. The integration of dierent modules can be regarded as a form of data fusion. To this end, we propose a framework for fusing dierent information sources through estimation of covariance from observations. The framework is demonstrated in a face and 3D pose tracking system that fuses similarity-to-prototypes measures and skin colour to track head pose and face position. The use of data fusion through covariance introduces constraints that allow the tracker to robustly estimate head pose and track face position simultaneously.  Key words: data fusion, pose estimation, similarity representation, face  recognition  1 Introduction  The approach we have taken to computer vision, referred to as perceptual fusion, involves the integration of multip...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 714,
    "label": 0,
    "text": "Scalability Issues for Query Routing Service Discovery In this paper, we discuss the relationship between mediatorbased systems for service discovery in multi-agent systems, and the technique of query routing used for resource discovery in distributed information systems. We then construct a model of the query routing task which we use to examine the complexity and scalability characteristics of a number of commonly encountered architectures for resource or service discovery.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 715,
    "label": 4,
    "text": "Battery-aware Static Scheduling for Distributed Real-time Embedded Systems This paper addresses battery-aware static scheduling in batterypowered  distributed real-time embedded systems. As suggested by  previous work, reducing the discharge current level and shaping  its distribution are essential for extending the battery lifespan. We  propose two battery-aware static scheduling schemes. The first  one optimizes the discharge power profile in order to maximize  the utilization of the battery capacity. The second one targets  distributed systems composed of voltage-scalable processing  elements (PEs). It performs variable-voltage scheduling via  efficient slack time re-allocation, which helps reduce the average  discharge power consumption as well as flatten the discharge  power profile. Both schemes guarantee the hard real-time  constraints and precedence relationships in the real-time  distributed embedded system specification. Based on previous  work, we develop a battery lifespan evaluation metric which is  aware of the shape of the discharge power profile. Our  experimental results show that the battery lifespan can be  increased by up to 29% by optimizing the discharge power file  alone. Our variable-voltage scheme increases the battery lifespan  by up to 76% over the non-voltage-scalable scheme and by up to  56% over the variable-voltage scheme without slack-time reallocation.  1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 716,
    "label": 0,
    "text": "Winner Determination in Combinatorial Auction Generalizations Combinatorial markets where bids can be submitted on bundles of items can be economically desirable coordination mechanisms in multiagent systems where the items exhibit complementarity and substitutability. There has been a surge of recent research on winner determination in combinatorial auctions. In this paper we study a wider range of combinatorial market designs: auctions, reverse auctions, and exchanges, with one or multiple units of each item, with and without free disposal. We first theoretically characterize the complexity. The most interesting results are that reverse auctions with free disposal can be approximated, and in all of the cases without free disposal, even finding a feasible solution is ##-complete. We then ran experiments on known benchmarks as well as ones which we introduced, to study the complexity of the market variants in practice. Cases with free disposal tended to be easier than ones without. On many distributions, reverse auctions with free disposal were easier than auctions with free disposal -- as the approximability would suggest -- but interestingly, on one of the most realistic distributions they were harder. Single-unit exchanges were easy, but multi-unit exchanges were extremely hard.",
    "neighbors": [
      589,
      1100
    ],
    "mask": "Validation"
  },
  {
    "node_id": 717,
    "label": 3,
    "text": "Constraint-based Processing of Multiway Spatial Joins . A multiway spatial join combines information found in three or more spatial relations with respect to some spatial predicates. Motivated by their close correspondence with constraint satisfaction problems (CSPs), we show how multiway spatial joins can be processed by systematic search algorithms traditionally used for CSPs. This paper describes two different strategies, window reduction and synchronous traversal, that take advantage of underlying spatial indexes to effectively prune the search space. In addition, we provide cost models and optimization methods that combine the two strategies to compute more efficient execution plans. Finally, we evaluate the efficiency of the proposed techniques and the accuracy of the cost models through extensive experimentation with several query and data combinations.  Key Words. Spatial Databases, Spatial Joins, Constraint Satisfaction, R-trees  1. INTRODUCTION  Spatial DBMSs and GISs store large amounts of multi-dimensional data, such as points...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 718,
    "label": 3,
    "text": "Materialized View Selection and Maintenance Using Multi-Query Optimization Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.  In this paper, we show how to find an efficient plan for the maintenance of a set of materialized views, by exploiting common subexpressions between different view maintenance expressions. In particular, we show how to efficiently select (a) expressions and indices that can be effectively shared, by transient materialization; (b) additional expressions and indices for permanent materialization; and (c) the best maintenance plan -- incremental or recomputation -- for each view. These three decisions are highly interdependent, and the choice of...",
    "neighbors": [
      637,
      993
    ],
    "mask": "Test"
  },
  {
    "node_id": 719,
    "label": 3,
    "text": "Learning Comprehensible Descriptions of Multivariate Time Series Supervised classification is one of the most active areas of machine learning research. Most work has focused on classification in static domains, where an instantaneous snapshot of attributes is meaningful. In many domains, attributes are not static; in fact, it is the way they vary temporally that can make classification possible. Examples of such domains include speech recognition, gesture recognition and electrocardiograph classification. While it is possible to use ad-hoc, domain-specific techniques for \"attening\" the time series to a learner-friendly representation, this fails to take into account both the special problems and special heuristics applicable to temporal data and often results in unreadable concept descriptions. Though traditional time series techniques can sometimes produce accurate classi ers, few can provide comprehensible descriptions. We propose a general architecture for classification and description of multivariate time series. It employs event primitives to ana...",
    "neighbors": [
      193
    ],
    "mask": "Train"
  },
  {
    "node_id": 720,
    "label": 1,
    "text": "Dimensionality Reduction by Random Mapping: Fast Similarity Computation for Clustering When the data vectors are high-dimensional it is computationally infeasible to use data analysis or pattern recognition algorithms which repeatedly compute similarities or distances in the original data space. It is therefore necessary to reduce the dimensionality before, for example, clustering the data. If the dimensionality is very high, like in the WEBSOM method which organizes textual document collections on a Self-Organizing Map, then even the commonly used dimensionality reduction methods like the principal component analysis may be too costly. It will be demonstrated that the document classi\u00f8cation accuracy obtained after the dimensionality has been reduced using a random mapping method will be almost as good as the original accuracy if the \u00f8nal dimensionality is sufficiently large (about 100 out of 6000). In fact, it can be shown that the inner product (similarity) between the mapped vectors follows closely the inner product of the original vectors.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 721,
    "label": 4,
    "text": "Instant Messaging and Awareness of Presence in WebWho This is a study of how awareness of presence affects content of instant messages via an awareness tool, WebWho. The awareness tool is an easily accessible web based system that visualises a large university computer lab. The instant messaging system is one of the functions of the tool, which allows students to virtually locate one another and to communicate via the instant messaging system. As WebWho is there to be accessed through any web browser, it requires no programming skills or special software. It may also be used from outside the computer lab by students located elsewhere.",
    "neighbors": [
      747
    ],
    "mask": "Validation"
  },
  {
    "node_id": 722,
    "label": 2,
    "text": "Probabilistic Latent Semantic Analysis Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two--mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.",
    "neighbors": [
      427,
      505,
      818,
      1068
    ],
    "mask": "Test"
  },
  {
    "node_id": 723,
    "label": 0,
    "text": "Emile: Marshalling Passions in Training and Education Emotional reasoning can be an important contribution to auto- mated tutoring and training systems. This paper describes mile, a model of emotional reasoning that builds upon existing approaches and significantly generalizes and extends their capabilities. The main contribution is to show how an explicit planning model allows a more general treatment of several stages of the reasoning process. The model supports educational applications by allowing agents to appraise the emotional significance of events as they relate to students' (or their own) plans and goals, model and predict the emotional state of others, and alter behavior accordingly. 1 INTRODUCTION Emotional computers may seem an oxymoron but recent years have seen a flurry of computation accounts of emotion in a variety of applications. This paper describes mile, a model of emotional reasoning that extends and significantly generalizes prior work. mile illustrates how an explicit planning model supports a more general treatme...",
    "neighbors": [
      821,
      841,
      1191
    ],
    "mask": "Train"
  },
  {
    "node_id": 724,
    "label": 0,
    "text": "Controlling Cooperative Problem Solving in Industrial Multi-Agent Systems using Joint Intentions One reason why Distributed AI (DAI) technology has been deployed in relatively few real-size applications is that it lacks a clear and implementable model of cooperative problem solving which specifies how agents should operate and interact in complex, dynamic and unpredictable environments. As a consequence of the experience gained whilst building a number of DAI systems for industrial applications, a new principled model of cooperation has been developed. This model, called Joint Responsibility, has the notion of joint intentions at its core. It specifies pre-conditions which must be attained before collaboration can commence and prescribes how individuals should behave both when joint activity is progressing satisfactorily and also when it runs into difficulty. The theoretical model has been used to guide the implementation of a general-purpose cooperation framework and the qualitative and quantitative benefits of this implementation have been assessed through a series of comparativ...",
    "neighbors": [
      126,
      196,
      222,
      250,
      312,
      329,
      441,
      557,
      597,
      686,
      687,
      839,
      852,
      963,
      1166,
      1204,
      1266
    ],
    "mask": "Test"
  },
  {
    "node_id": 725,
    "label": 0,
    "text": "Towards the Standardization of Multi-Agent Systems Architectures: An Overview This article briefly describes these groups' efforts toward the standardization of multi-agent systems architectures, and sketches early works to define a multi-agent systems architecture at the University of Calgary. However, the main objective of this article is to give the reader a basic overview of the background and terminology in this exciting area of research.",
    "neighbors": [
      962
    ],
    "mask": "Validation"
  },
  {
    "node_id": 726,
    "label": 2,
    "text": "Distributional Clustering of Words for Text Classification This paper applies Distributional Clustering (Pereira  et al. 1993) to document classification. The approach clusters words into groups based on the distribution of class labels associated with each word. Thus, unlike some other unsupervised dimensionality-reduction techniques, such as Latent Semantic Indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy. Experimental results obtained on three real-world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2% accuracy---significantly better than Latent Semantic Indexing (Deerwester et al. 1990), class-based clustering (Brown et al. 1992), feature selection by mutual information (Yang and Pederson 1997), or Markovblanket -based feature selection (Koller and Sahami 1996). We also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clusteri...",
    "neighbors": [
      291,
      347,
      616,
      1126
    ],
    "mask": "Train"
  },
  {
    "node_id": 727,
    "label": 1,
    "text": "View-Based 3d Object Recognition With Support Vector Machines . Support Vector Machines have demonstrated excellent results in pattern recognition tasks and 3D object recognition. In this contribution, we confirm some of the results in 3D object recognition and compare it to other object recognition systems. We use di#erent pixel-level representations to perform the experiments, while we extend the setting to the more challenging and practical case when only a limited number of views of the object are presented during training. We report high correct classification of unseen views, especially considering that no domain knowledge is including into the proposed system. Finally, we suggest an active learning algorithm to reduce further the required number of training views. INTRODUCTION  Humans are able to recognize everyday 3D objects when shown previously only one - or at most a few - views of the object. In contrast, artificial systems must either been shown many views of an object (e.g. [8]) or either a lot of knowledge of object structure must ...",
    "neighbors": [
      973
    ],
    "mask": "Train"
  },
  {
    "node_id": 728,
    "label": 4,
    "text": "Multi-Sensor Context Aware Clothing Inspired by perception in biological systems, distribution of a massive amount of simple sensing devices is gaining more support in detection applications. A focus on fusion of sensor signals instead of strong analysis algorithms, and a scheme to distribute sensors, results in new issues. Especially in wearable computing, where sensor data continuously changes, and clothing provides an ideal supporting structure for simple sensors, this approach may prove to be favourable. Experiments with a body-distributed sensor system investigate the influence of two factors that affect classification of what has been sensed: an increase in sensors enhances recognition, while adding new classes or contexts depreciates the results. Finally, a wearable computing related scenario is discussed that exploits the presence of many sensors.",
    "neighbors": [
      307,
      910,
      1177
    ],
    "mask": "Validation"
  },
  {
    "node_id": 729,
    "label": 4,
    "text": "AdApt - a multimodal conversational dialogue system in an apartment domain A general overview of the AdApt project and the research that is performed within the project is presented. In this project various aspects of human-computer interaction in a multimodal conversational dialogue systems are investigated. The project will also include studies on the integration of user/system/dialogue dependent speech recognition and multimodal speech synthesis. A domain in which multimodal interaction is highly useful has been chosen, namely, finding available apartments in Stockholm. A Wizard-of-Oz data collection within this domain is also described. 1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 730,
    "label": 5,
    "text": "Ontologies Description and Applications The word \"ontology\" has gained a good popularity within the AI  community. Ontology is usually viewed as a high-level description consisting  of concepts that organize the upper parts of the knowledge base.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 731,
    "label": 1,
    "text": "Improving Minority Class Prediction Using Case-Specific Feature Weights This paper addresses the problem of handling skewed class distributions within the case-based learning (CBL) framework. We rst present as a baseline an informationgain-weighted CBL algorithm and apply it to three data sets from natural language processing (NLP) with skewed class distributions. Although overall performance of the baseline CBL algorithm is good, we show that the algorithm exhibits poor performance on minority class instances. We then present two CBL algorithms designed to improve the performance of minority class predictions. Each variation creates test-case-speci c feature weights by rst observing the path taken by the test case in a decision tree created for the learning task, and then using pathspeci c information gain values to create an appropriate weight vector for use during case retrieval. When applied to the NLP data sets, the algorithms are shown to signi cantly increase the accuracy of minority class predictions while maintaining or improving overall classi cation accuracy. 1",
    "neighbors": [
      611,
      1075,
      1135
    ],
    "mask": "Validation"
  },
  {
    "node_id": 732,
    "label": 1,
    "text": "Learning Complex Patterns for Document Categorization Knowledge-based approaches to document categorization make use of well elaborated and powerful pattern languages for manual writing of classification rules. Although such classification patterns have proven useful in many practical applications, algorithms for learning classifiers from examples mostly rely on much simpler representations of classification knowledge. In this paper, we describe a learning algorithm which employs a pattern language similar to languages used for manual rule editing. We focus on the learning of three specific constructs of this pattern language, namely phrases, tolerance matches of words and substring matches of words. Introduction  Manually writing document categorization rules is labor intensive and requires much expertise. This caused a growing research interest in learning systems for document categorization. Most of these systems transform pre-classified example documents into a propositional attribute-value representation. Simple attributes indicate w...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 733,
    "label": 4,
    "text": "Designing Synchronous User Interface for Collaborative Applications Synchronous User interface is a medium where all objects being shared on it can be  viewed indifferently from the geographical location and its users can interact with each other  in real-time. Designing such an interface for users working collaboratively requires to deal  with a number of issues. Herein, our concerns lies on the design of control component  of Human-Computer Interaction (HCI) and corresponding User Interface (UI) software that  implements it. We make use of our approach to interactive system development based on the  MPX - Mapping from PAN (Protagonist Action Notation) into Xchart (eXtended Statechart)  - and illustrate it by presenting the case study of a collaborative application.  Keywords: PAN, MPX, HCI design, UI software design.  1 INTRODUCTION  To survive, human beings need to organize themselves into a society. Differently from other animals that are able to live separately in reasonable manner, human beings are endowed with physical and cognitive abilities ne...",
    "neighbors": [
      326
    ],
    "mask": "Train"
  },
  {
    "node_id": 734,
    "label": 2,
    "text": "Domain-Specific Keyphrase Extraction Keyphrases are an important means of document  summarization, clustering, and topic  search. Only a small minority of documents  have author-assigned keyphrases, and manually  assigning keyphrases to existing documents is  very laborious. Therefore it is highly desirable  to automate the keyphrase extraction process.  This paper shows that a simple procedure for  keyphrase extraction based on the naiveBayes  learning scheme performs comparably to the  state of the art. It goes on to explain how  this procedure's performance can be boosted by  automatically tailoring the extraction process  to the particular document collection at hand.  Results on a large collection of technical reports  in computer science show that the quality of  the extracted keyphrases improves signi#cantly  when domain-speci#c information is exploited.  1 Introduction  Keyphrases give a high-level description of a document's contents that is intended to make it easy for prospective readers to decide whether or no...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 735,
    "label": 4,
    "text": "The Design of History Mechanisms and their Use in Collaborative Educational Simulations Reviewing past events has been useful in many domains. Videotapes and flight data recorders provide invaluable technological help to sports coaches or aviation engineers. Similarly, providing learners with a readable recording of their actions may help them monitor their behavior, reflect on their progress, and experiment with revisions of their experiences. It may also facilitate active collaboration among dispersed learning communities. Learning histories can help students and professionals make more effective use of digital library searching, word processing tasks, computer-assisted design tools, electronic performance support systems, and web navigation. This paper describes the design space and discusses the challenges of implementing learning histories. It presents guidelines for creating effective implementations, and the design tradeoffs between sparse and dense history records. The paper also presents a first implementation of learning histories for a simulation-based engineer...",
    "neighbors": [
      743
    ],
    "mask": "Train"
  },
  {
    "node_id": 736,
    "label": 4,
    "text": "Between Information and Communication: Middle Spaces in Computer Media for Learning In this paper, we identify two categories of media that are common in computer-supported collaborative learning and software in general: communication media, and information media. These two types of media map easily on to two types of social activities in which learning is grounded: dialogue and monologue. Drawing on literature in learning theory, we suggest the need for interfaces that helpstudents transition from dialogue to monologue and back again. This \"middle space\" between communication and information interfaces is illustrated with several examples from CSCL. We advocate filling in this middle space with software and activities that transcend some of the traditional design tradeoffs associated with information and communication interfaces. Keywords: Collaboration, Interaction & Design Tradeoffs Introduction: computers, communication & learning Computer mediated acts of communication are becoming more commonplace in today's classroom. Like all media, particular computer techn...",
    "neighbors": [
      883
    ],
    "mask": "Train"
  },
  {
    "node_id": 737,
    "label": 4,
    "text": "Towards Group Communication for Mobile Participants Group communication will undoubtedly be a useful paradigm for many applications of wireless networking in which reliability and timeliness are requirements. Moreover, location awareness is clearly central to mobile applications such as traffic management and smart spaces. In this paper, we introduce our definition of proximity groups in which group membership depends on location and then discuss some requirements for a group membership management service suitable for proximity groups. We describe a novel approach to efficient coverage estimation, giving applications feedback on the proportion of the area of interest covered by a proximity group, and also discuss our approach to partition anticipation.",
    "neighbors": [
      919,
      1029
    ],
    "mask": "Train"
  },
  {
    "node_id": 738,
    "label": 4,
    "text": "Finding Location Using Omnidirectional Video on a Wearable Computing Platform In this paper we present a framework for a navigation system in an indoor environment using only omnidirectional video. Within a Bayesian framework we seek the appropriate place and image from the training data to describe what we currently see and infer a location. The posterior distribution over the state space conditioned on image similarity is typically not Gaussian. The distribution is represented using sampling and the location is predicted and verified over time using the Condensation algorithm. The system does not require complicated feature detection, but uses a simple metric between two images. Even with low resolution input, the system may achieve accurate results with respect to the training data when given favorable initial conditions.  1. Introduction and Previous Work  Recognizing location is a difficult but often essential part of identifying a wearable computer user's context. Location sensing may be used to provide mobility aids for the blind [13], spatially-based not...",
    "neighbors": [
      75,
      215,
      307,
      497,
      531,
      665,
      680,
      910
    ],
    "mask": "Test"
  },
  {
    "node_id": 739,
    "label": 4,
    "text": "Social and Semiotic Analyses for Theorem Prover User Interface Design : We describe an approach to user interface design based on ideas from cognitive science, social science, especially the theory of stories, and a new area tentatively called algebraic semiotics. Social analysis helps to identify coherent classes of users and their requirements, and suggests some ways to make proofs more understandable, while algebraic semiotics, which combines semiotics with algebraic specification, provides a rigorous theory of interface functionality and quality. We apply these techniques to designing user interfaces for a distributed cooperative theorem proving system, whose main component is a website generation and proof assistance tool called Kumo. This interface integrates formal proving, proof browsing, animation, informal explanation, and online background tutorials. Experience with using the interface is reported, and some conclusions are drawn. 1 Introduction  Recent large advances in performance have made it arguable that the most pressing open problems in ...",
    "neighbors": [
      750
    ],
    "mask": "Train"
  },
  {
    "node_id": 740,
    "label": 1,
    "text": "Fuzzy Finite-state Automata Can Be Deterministically Encoded into Recurrent Neural Networks There has been an increased interest in combining fuzzy systems with neural networks because fuzzy neural systems merge the advantages of both paradigms. On the one hand, parameters in fuzzy systems have clear physical meanings and rule-based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way. On the other hand, there exist powerful algorithms for training various neural network models. However, most of the proposed combined architectures are only able to process static input-output relationships, i.e. they are not able to process temporal input sequences of arbitrary length. Fuzzy finite-state automata (FFAs) can model dynamical processes whose current state depends on the current input and previous states. Unlike in the case of deterministic finite-state automata (DFAs), FFAs are not in one particular state, rather each state is occupied to some degree defined by a membership function. Based on previous work on encoding DFAs in discrete-tim...",
    "neighbors": [
      116
    ],
    "mask": "Train"
  },
  {
    "node_id": 741,
    "label": 1,
    "text": "Embodied Evolution: A Response to Challenges in Evolutionary Robotics We introduce Embodied Evolution (EE), a new methodology for conducting evolutionary robotics (ER). Embodied evolution uses a population of physical robots that evolve by reproducing with one another in the task environment. EE addresses several issues identified by researchers in the evolutionary robotics community as problematic for the development of ER. We review results from our first experiments and discuss the advantages and limitations of the EE methodology.",
    "neighbors": [
      163
    ],
    "mask": "Test"
  },
  {
    "node_id": 742,
    "label": 3,
    "text": "Routing Through Networks with Hierarchical Topology Aggregation Abstract In the future, global networks will consist of a hierarchy of subnetworks called domains. For reasons of both scalability and security, domains will not reveal details of their internal structure to outside nodes. Instead, these domains will advertise only a summary, or aggregated view, of their internal structure, e.g., as proposed by the ATM PNNI standard. This work compares, by simulation, the performance of several different aggregation schemes in terms of network throughput (the fraction of attempted connections that are realized), and network control load (the average number of crankbacks per realized connection.) Our main results are: ffl Minimum spanning tree is a good aggregation scheme; ffl Exponential link cost functions perform better than min-hop routing; ffl Our suggested logarithmic update scheme that determine when re-aggregation should be computed can significantly reduce the computational overhead due to re-aggregation with a negligible decrease in performance. 1.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 743,
    "label": 4,
    "text": "Search History for User Support in Information-Seeking Interfaces The research overview described focuses on the design of search history displays to support information seeking (IS). It examines users' IS activities, current and potential use of histories, and building on this theoretical framework, assesses prototype interfaces that integrate these histories into search systems. Preliminary results described indicate search history use in coordinated work, mental model building, and end user IS strategies. Searchers create and use external records of their actions and the corresponding results by writing/typing notes, using copy and paste functions, and making printouts. Recording user actions and results in computerized systems automates this process, and enables the creation of search history displays that support users in their IS. Existing systems provide search history capabilities, however these often do not offer enough flexibility for users. Legal information has been selected as the domain for the research.  Keywords  History, Information-...",
    "neighbors": [
      429,
      735
    ],
    "mask": "Train"
  },
  {
    "node_id": 744,
    "label": 4,
    "text": "UMLi: The unified modeling language for interactive applications User interfaces (UIs) are essential components of most software systems, and significantly affect the effectiveness of installed applications. In addition, UIs often represent a significant proportion of the code delivered by a development activity. However, despite this, there are no modelling languages and tools that support contract elaboration between UI developers and application developers. The Unified Modeling Language (UML) has been widely accepted by application developers, but not so much by UI designers. For this reason, this paper introduces the notation of the Unified Modelling Language for Interactive Applications (UMLi), that extends UML, to provide greater support for UI design. UI elements elicited in use cases and their scenarios can be used during the design of activities and UI presentations. A diagram notation for modelling user interface presentations is introduced. Activity diagram notation is extended to describe collaboration between interaction and domain objects. Further, a case study using UMLi notation and method is presented.",
    "neighbors": [
      44
    ],
    "mask": "Train"
  },
  {
    "node_id": 745,
    "label": 3,
    "text": "An Event-Condition-Action Language for XML XML repositories are now a widespread means for storing and exchanging information on the Web. As these repositories become increasingly used in dynamic applications such as e-commerce, there is a rapidly growing need for a mechanism to incorporate reactive functionality in an XML setting. Event-condition-action (ECA) rules are a technology from active databases and are a natural method for supporting such functionality. ECA rules can be used for activities such as automatically enforcing document constraints, maintaining repository statistics, and facilitating publish/subscribe applications. An important question associated with the use of a ECA rules is how to statically predict their run-time behaviour. In this paper, we de ne a language for ECA rules on XML repositories. We theninvestigate methods for analysing the behaviour of a set of ECA rules, ataskwhich has added complexity in this XML setting compared with conventional active databases. Keywords: Event-condition-action rules, XML, XML repositories, reactive functionality, rule analysis. 1",
    "neighbors": [
      651,
      673,
      708
    ],
    "mask": "Train"
  },
  {
    "node_id": 746,
    "label": 2,
    "text": "Scalable Association-based Text Classification Nave Bayes (NB) classifier has long been considered a core methodology in text classification mainly due to its simplicity and computational efficiency. There is an increasing need however for methods that can achieve higher classification accuracy while maintaining the ability to process large document collections. In this paper we examine text categorization methods from a perspective that considers the tradeoff between accuracy and scalability to large data sets and large feature sizes. We start from the observation that Support Vector Machines, one of the best text categorization methods cannot scale up to handle the large document collections involved in many real word problems. We then consider bayesian extensions to NB that achieve higher accuracy by relaxing its strong independence assumptions. Our experimental results show that LB, an association-based lazy classifier can achieve a good tradeoff between high classification accuracy and scalability to large document collections...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 747,
    "label": 4,
    "text": "WebWho: Support for Student Awareness and Coordination this paper, WEBWHO, is a lightweight, value-adding service that relies on readily available server status information, which is refined and visualized in a way that is easily accessible to individuals from any workstation with a web browser.",
    "neighbors": [
      721,
      854
    ],
    "mask": "Validation"
  },
  {
    "node_id": 748,
    "label": 3,
    "text": "Overlapping Linear Quadtrees and Spatio-Temporal Query Processing indexing in spatio-temporal databases by using the technique of overlapping is investigated. Overlapping has been previously applied in various access methods to combine consecutive structure instances into a single structure, without storing identical sub-structures. In this way, space is saved without sacrificing time performance. A new access method, overlapping linear quadtrees is introduced. This structure is able to store consecutive historical raster images, a database of evolving images. Moreover, it can be used to support query processing in such a database. Five such spatio-temporal queries along with the respective algorithms that take advantage of the properties of the new structure are introduced. The new access method was implemented and extensive experimental studies for space efficiency and query processing performance were conducted. A number of results of these experiments are presented. As far as space is concerned, these results indicate that, in the case of similar consecutive images, considerable storage is saved in comparison to independent linear quadtrees. In the case of query processing, the results indicate that the proposed algorithmic approaches outperform the respective straightforward algorithms, in most cases. The region data sets used in experiments were real images of meteorological satellite views and synthetic random images with specified aggregation",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 749,
    "label": 4,
    "text": "Using Handhelds as Controls for Everyday Appliances: A Paper Prototype Study Everyday appliances, including telephones, copiers, and home stereos, increasingly contain embedded computers which enable greater functionality. If the interfaces to these appliances were easy to use, people might benefit from these new functions. Unfortunately, it is rare to find a well-designed appliance interface. This study shows that existing appliance interfaces could be improved by using a remote control interface on a handheld computer.  Keywords: Handheld computers, remote control, appliance, Pebbles  INTRODUCTION  The problem with many appliances is that they are too complex. Some appliances need thirty or more buttons to cover all of their functions. This complexity can even make relatively simple tasks, like setting the clock on a VCR, so difficult that people avoid them.  Most appliances do not provide unambiguous feedback to users. Indicators of appliance state can be confusing. For example, on a stereo that combines a CD and tape player, it may be difficult to decide wh...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 750,
    "label": 4,
    "text": "An Overview of the Tatami Project This paper describes the Tatami project at UCSD, which is developing a system to support distributed cooperative software development over the web, and in particular, the validation of concurrent distributed software. The main components of our current prototype are a proof assistant, a generator for documentation websites, a database, an equational proof engine, and a communication protocol to support distributed cooperative work. We believe behavioral specification and verification are important for software development, and for this purpose we use first order hidden logic with equational atoms. The paper also briefly describes some novel user interface design methods that have been developed and applied in the project",
    "neighbors": [
      739
    ],
    "mask": "Train"
  },
  {
    "node_id": 751,
    "label": 2,
    "text": "Inverted files and dynamic signature files for optimisation of Web Directories Web directories are taxonomies for the classification of Web documents. This kind of IR systems present a specific type of search where the document collection is restricted to one area of the category graph. This paper introduces a specific data architecture for Web directories which improves the performance of restricted searches. That architecture is based on a hybrid data structure composed of an inverted file with multiple embedded signature files. Two variants based on the proposed model are presented: hybrid architecture with total information and hybrid architecture with partial information. The validity of this architecture has been analysed by means of developing both variants to be compared with a basic model. The performance of the restricted queries was clearly improved, specially the hybrid model with partial information, which yielded a positive response under any load of the search system.",
    "neighbors": [
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 752,
    "label": 3,
    "text": "Querying Network Directories Hierarchically structured directories have recently proliferated with the growth of the Internet, and are being used to store not only address books and contact information for people, but also personal pro les, network resource information, and network and service policies. These systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way far superior to what conventional relational or object-oriented databases o er. Yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated \\queries &quot; involve navigational access. In this paper, we develop the core of a formal data model for network directories, and propose a sequence of e ciently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the e ciency of each algorithm in terms of its I/O complexity. Our data model and query languages share the exibility and utility of the recent proposals for semi-structured data models, while at the same time e ectively addressing the speci c needs of network directory applications, which we demonstrate by means of a representative real-life example. This work was done when the authors were at AT&T Labs{",
    "neighbors": [
      304,
      711,
      1254
    ],
    "mask": "Test"
  },
  {
    "node_id": 753,
    "label": 1,
    "text": "Learning Reactive Robot Behaviors with a Neural-Q Learning Approach The purpose of this paper is to propose a Neural-Q_learning approach designed for online learning of simple and reactive robot behaviors. In this approach, the Q_function is generalized by a multi-layer neural network allowing the use of continuous states and actions. The algorithm uses a database of the most recent learning samples to accelerate and guarantee the convergence. Each Neural-Q_learning function represents an independent, reactive and adaptive behavior which maps sensorial states to robot control actions. A group of these behaviors constitutes a reactive control scheme designed to fulfill simple missions. The paper centers on the description of the Neural-Q_learning based behaviors showing their performance with an autonomous underwater vehicle (AUV) in a target following mission. Simulated experiments demonstrate the convergence and stability of the learning system, pointing out its suitability for online robot learning. Advantages and limitations are discussed.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 754,
    "label": 2,
    "text": "Improving Response Time by Search Pruning in a Content-Based Image Retrieval System, Using Inverted File Techniques This paper describes several methods for improving query evaluation speed in a content-based image retrieval system (CBIRS). Response time is an extremely important factor in determining the usefulness of any interactive system, as has been demonstrated by human factors studies over the past thirty years. In particular, response times of less than one second are often specified as a usability requirement. It is shown that the use of inverted files facilitates the reduction of query evaluation time without significantly reducing the accuracy of the response. The performance of the system is evaluated using precision vs. recall graphs, which are an established evaluation method in information retrieval (IR), and are beginning to be used by CBIR researchers.  KEYWORDS: content-based image retrieval, search pruning, inverted file, response time 1 Introduction  Response times in the interaction between computer systems and human users are of great importance to user satisfaction. At present...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 755,
    "label": 2,
    "text": "Improving Retrieval on Imperfect Speech Transcriptions This paper presents the results from adding several forms of query expansion to our retrieval system running on transcriptions of broadcast news from the 1997 TREC-7 spoken document retrieval track. 1  Introduction  Retrieving documents which originated as speech is complicated by the presence of errors in the transcriptions. If some method of increasing retrieval performance despite these errors could be found, then even low-accuracy recognisers could be used as part of a successful spoken document retrieval (SDR) system. This paper presents results using four query expansion techniques described in [3] on 8 different sets of transcriptions generated for the 1997 TREC-7 SDR evaluation. The baseline retrieval system and the techniques used for query expansion are described in section 2, the transcriptions on which the experiments were performed in section 3 and results and further discussion are offered in section 4. 2  Retrieval Systems  2.1  Baseline System (BL)  Our baseline system ...",
    "neighbors": [
      604,
      796
    ],
    "mask": "Validation"
  },
  {
    "node_id": 756,
    "label": 4,
    "text": "User-Centered Design and Evaluation of a Real-Time Battlefield Visualization Virtual Environment The ever-increasing power of computers and hardware rendering systems has, to date, primarily motivated the creation of visually rich and perceptually realistic virtual environment (VE) applications. Comparatively very little effort has been expended on the user interaction components of VEs. As a result, VE user interfaces are often poorly designed and are rarely evaluated with users. Although usability engineering is a newly emerging facet of VE development, user-centered design and usability evaluation in VEs as a practice still lags far behind what is needed. This paper presents a structured, iterative approach for the user-centered design and evaluation of VE user interaction. This approach consists of the iterative use of expert heuristic evaluation, followed by formative usability evaluation, followed by summative evaluation. We describe our application of this approach to a real-world VE for battlefield visualization, describe the resulting series of design iterations, and present evidence that this approach provides a cost-effective strategy for assessing and iteratively improving user interaction design in VEs. This paper is among the first to report applying an iterative, structured, user-centered design and evaluation approach to VE user interaction design.",
    "neighbors": [
      677,
      895
    ],
    "mask": "Validation"
  },
  {
    "node_id": 757,
    "label": 0,
    "text": "On Maintaining Code Mobility We introduce the aspect of maintenance to code mobility with its major problem of keeping track of code migrating through computer networks. Our approach introduces the concept of mobile, lightweight knowledge repositories to support the maintenance of applications deploying mobile code in highly distributed computing environments. Our proposed system establishes a virtual global database with information about the mobile application and its surrounding environment based on distributed structured XML-based knowledge repositories. These distributed databases provide well-defined structural query and retrieval capabilities and ensure the abstraction from programming languages and proprietary hardware platforms. Finally, we point out several future research issues in the field of maintaining mobile code with respect to automatic maintenance of applications and automatic quality checking among others.",
    "neighbors": [
      119,
      269,
      424
    ],
    "mask": "Train"
  },
  {
    "node_id": 758,
    "label": 3,
    "text": "Intelligent Techniques for the Extraction and Integration of Heterogeneous Information Developing intelligent tools for the integration of information extracted from multiple heterogeneous sources is a challenging issue to effectively exploit the numerous sources available on-line in global information systems. In this paper, we propose intelligent, tool-supported techniques to information extraction and integration which take into account both structured and semistructured data sources. An object-oriented language called ODL I  3 , derived from the standard ODMG, with an underlying Description Logics, is introduced for information extraction. ODL I 3 descriptions of the information sources are exploited first to set a shared vocabulary for the sources. Information integration is performed in a semiautomatic way, by exploiting ODL I 3 descriptions of source schemas with a combination of Description Logics and clustering techniques. Techniques described in the paper have been implemented in the MOMIS system, based on a conventional mediator architecture. Keywords - Hetero...",
    "neighbors": [
      766
    ],
    "mask": "Train"
  },
  {
    "node_id": 759,
    "label": 4,
    "text": "Model for Unistroke Writing Time Unistrokes are a viable form of text input in pen-based user interfaces. However, they are a very heterogeneous group of gestures the only common feature being that all are drawn with a single stroke. Several unistroke alphabets have been proposed including the original Unistrokes, Graffiti, Allegro, T-Cube and MDITIM. Comparing these methods usually requires a lengthy study with many writers and even then the results are biased by the earlier handwriting experience that the writers have. Therefore, a simple descriptive model can make these comparisons easier. In this paper we propose a model for predicting the writing time for an expert user on any given unistroke alphabet thus enabling sounder argumentation on the properties of different writing methods.  Keywords  Modeling of motor performance, handwriting, pen input  INTRODUCTION  Unistrokes were introduced as a text input method for penbased user interfaces by Goldberg and Richardson in their 1993 paper [8]. Unistrokes are an alte...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 760,
    "label": 3,
    "text": "Optimizing OID Indexing Cost in Temporal Object-Oriented Database Systems In object-oriented database systems (OODB) with logical OIDs, an OID index (OIDX) is needed to map from OID to the physical location of the object. In a transaction time temporal OODB, the OIDX should also index the object versions. In this case, the index entries, which we call object descriptors (OD), also include the commit timestamp of the transaction that created the object version. In this report, we develop an analytical model for OIDX access costs in temporal OODBs. The model includes the index page buffer as well as an OD cache. We use this model to study access cost and optimal use of memory for index page buffer and OD cache, with different access patterns. The results show that 1) the OIDX access cost can be high, and can easy become a bottleneck in large temporal OODBs, 2) the optimal OD cache size can be relatively large, and 3) the gain from using an optimal size is considerable, and because access pattern in a database system can be very dynamic, the system should be ab...",
    "neighbors": [
      503,
      630
    ],
    "mask": "Train"
  },
  {
    "node_id": 761,
    "label": 3,
    "text": "Distributed Knowledge Networks Distributed Knowledge Networks (DKN) provide some of the key enabling technologies for translating recent advances in automated data acquisition, digital storage, computers and communications into fundamental advances in organizational decision support, data analysis, and related applications. DKN include computational tools for accessing, organizing, transforming, and analyzing the contents of heterogeneous, distributed data and knowledge sources and for distributed problem solving and decision making under tight time, resource, and performance constraints. This paper presents an overview of the DKN project in the Iowa State University Artificial Intelligence Laboratory.  I. Introduction  Advanced scientific research (e.g., the genome project), military applications (e.g., intelligence data handling, situation assessment, command and control) , law enforcement (e.g., terrorism prevention), crisis management, design and manufacturing systems, and medical information infrastructure, pow...",
    "neighbors": [
      801,
      1079
    ],
    "mask": "Validation"
  },
  {
    "node_id": 762,
    "label": 2,
    "text": "HySpirit - a Probabilistic Inference Engine for Hypermedia Retrieval in Large Databases . HySpirit is a retrieval engine for hypermedia retrieval integrating concepts from information retrieval (IR) and deductive databases. The logical view on IR models retrieval as uncertain inference, for which we use probabilistic reasoning. Since the expressiveness of classical IR models is not sufficient for hypermedia retrieval, HySpirit is based on a probabilistic version of Datalog. In hypermedia retrieval, different nodes may contain contradictory information; thus, we introduce probabilistic four-valued Datalog. In order to support fact queries as well as contentbased retrieval, HySpirit is based on an open world assumption, but allows for predicate-specific closed world assumptions. For performing efficient retrieval on large databases, our system provides access to external data. We demonstrate the application of HySpirit by giving examples for retrieval on images, structured documents and large databases. 1 Introduction  Due to the advances in hardware, processing of multimed...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 763,
    "label": 2,
    "text": "Towards Web-Scale Web Archeology Web-scale Web research is difficult. Information on the Web is vast in quantity, unorganized and uncatalogued, and available only over a network with varying reliability. Thus, Web data is difficult to collect, to store, and to manipulate efficiently. Despite these difficulties, we believe performing Web research at Web-scale is important. We have built a suite of tools that allow us to experiment on collections that are an order of magnitude or more larger than are typically cited in the literature. Two key components of our current tool suite are a fast, extensible Web crawler and a highly tuned, in-memory database of connectivity information. A Web page repository that supports easy access to and storage for billions of documents would allow us to study larger data sets and to study how the Web evolves over time.",
    "neighbors": [
      11,
      536,
      774,
      968,
      1017,
      1096
    ],
    "mask": "Train"
  },
  {
    "node_id": 764,
    "label": 0,
    "text": "G'day Mate. Let me Introduce you to Everyone: An Infrastructure for Scalable Human-System Interaction We are exposed to physical and virtual systems every day. They consist of computers, PDAs, wireless devices and increasingly, robots. Each provides services to individual or groups of users whether they are local or remote to the system. Services offered by these systems may be useful beyond these users to others, however connecting many of these systems to more users presents a challenging problem. The primary goal of the research presented in this paper is to demonstrate a scalable approach for connecting multiple users to the services provided by multiple systems. Such an approach must be simple, robust and general to contend with the heterogeneous capabilities of the services. An infrastructure is presented that addresses these scalability requirements and establishes the foundation for contending with heterogeneous services. Additionally, it allows services to be linked to form higher-level abstractions. The infrastructure is demonstrated in simulation on several similar multirobot systems with multiple users. The results propose it as a solution for large-scale human-system interaction.",
    "neighbors": [
      668,
      989
    ],
    "mask": "Validation"
  },
  {
    "node_id": 765,
    "label": 3,
    "text": "Active Information Gathering in InfoSleuth . InfoSleuth  1  is an agent-based system that can be configured to perform many different information management activities in a distributed environment. InfoSleuth agents provide a number of complex query services that require resolving ontology-based queries over dynamically changing, distributed, heterogeneous resources. These include distributed query processing, location-independent single-resource updates, event and information monitoring, statistical or inferential data analysis, and trend discovery in complex event streams. It has been used in numerous applications, including the Environmental Data Exchange Network and the Competitive Intelligence System. 1 Introduction  In the past 15-20 years, numerous products and prototypes have regularly appeared to provide uniform access to heterogeneous data sources. As a result, that access to heterogeneous sources is now taken as a \"given\" by customers. Current MCC studies indicate that, given the availability of products that achieve...",
    "neighbors": [
      132,
      663,
      1241
    ],
    "mask": "Validation"
  },
  {
    "node_id": 766,
    "label": 3,
    "text": "Semantic Integration of Semistructured and Structured Data Sources this paper is to describe the MOMIS [4, 5] (Mediator envirOnment for Multiple Information Sources) approach to the integration and query of multiple, heterogeneous information sources, containing structured and semistructured data. MOMIS has been conceived as a joint collaboration between University of Milano and Modena in the framework of the INTERDATA national research project, aiming at providing methods and tools for data management in Internet-based information systems. Like other integration projects [1, 10, 14], MOMIS follows a \"semantic approach\" to information integration based on the conceptual schema, or metadata, of the information sources, and on the following architectural elements: i) a common object-oriented data model, defined according to the ODL I 3 language, to describe source schemas for integration purposes. The data model and ODL I 3 have been defined in MOMIS as subset of the ODMG-93 ones, following the proposal for a standard mediator language developed by the I",
    "neighbors": [
      28,
      145,
      294,
      712,
      758,
      915,
      1234
    ],
    "mask": "Train"
  },
  {
    "node_id": 767,
    "label": 3,
    "text": "Community Webs (C-Webs): Technological Assessment and System Architecture this paper, our presentation mainly relies on examples taken from one of the potential C-Web applications, namely C-Web Portals for cultural communities.",
    "neighbors": [
      543,
      865,
      1085,
      1162
    ],
    "mask": "Validation"
  },
  {
    "node_id": 768,
    "label": 2,
    "text": "A Content-Based Image Meta-Search Engine using Relevance Feedback Search engines are the most powerful resources for finding information on the rapidly expanding World-Wide Web. Finding the desired search engines and learning how to use them, however, can be very time consuming. Metasearch engines, which integrate a group of such search tools, enable users to access information across the world in a transparent and more efficient manner. The recent emergence of visual information retrieval (VIR) systems on the Web is leading to the same efficiency problem. This paper describes MetaSEEk, a meta-search engine used for retrieving images based on their visual content on the Web. MetaSEEk is designed to intelligently select and interface with multiple on-line image search engines by ranking their performance for different classes of user queries. User feedback is also integrated in the ranking refinement. MetaSEEk has been developed to explore the issues involved in querying large, distributed, on-line visual information system sources. We compare MetaSEE...",
    "neighbors": [
      118,
      931
    ],
    "mask": "Train"
  },
  {
    "node_id": 769,
    "label": 4,
    "text": "Distributed and Disappearing User Interfaces in . . . f expression, will often dominate the pure functional aspect to a user. We are already witnessing this transition with mobile phones.  Novel computational artifacts based on these technologies may become `invisible', or reside in the background, in several different ways:  Truly invisible - when the actual computer and its interface is almost totally integrated with an environment that is familiar to the user. The user interaction model is implicit and perhaps even unnoticeable to the user. Such a system truly resides in the background of the user's attention at all times.  Transparent - here the `invisible' UI is not invisible in the literal sense. Rather, it is transparent in the same sense as a very familiar tool is transparent to its user, to the point where it almost acts as an extension of one's body. Every- Distributed and Disappearing User Interfaces in  Ubiquitous Computing  Keywords  context-awareness, disappearing user interfaces",
    "neighbors": [
      1006
    ],
    "mask": "Validation"
  },
  {
    "node_id": 770,
    "label": 2,
    "text": "An Evaluation of Linguistically-motivated Indexing Schemes In this article, we describe a number of indexing experiments based on indexing terms other than simple keywords. These experiments were conducted as one step in validating a linguistically-motivated indexing model. The problem is important but not new. What is new in this approach is the variety of schemes evaluated. It is important since it should not only help to overcome the well-known problems of bag-of-words representations, but also the difficulties raised by non-linguistic text simplification techniques such as stemming, stop-word deletion, and term selection. Our approach in the selection of terms is based on part-of-speech tagging and shallow parsing. The indexing schemes evaluated vary from simple keywords to nouns, verbs, adverbs, adjectives, adjacent word-pairs, and head-modifier pairs. Our findings apply to Information Retrieval and most of related areas.",
    "neighbors": [
      850,
      1213
    ],
    "mask": "Test"
  },
  {
    "node_id": 771,
    "label": 3,
    "text": "Semantic Query Optimization through Abduction and Constraint Handling . The use of integrity constraints to perform Semantic Query Optimization (SQO) in deductive databases can be formalized in a way similar to the use of integrity constraints in Abductive Logic Programming (ALP) and the use of Constraint Handling Rules in Constraint Logic Programming (CLP). Based on this observation and on the similar role played by, respectively, extensional, abducible and constraint predicates in SQO, ALP and CLP, we present a unified framework from which (variants of) SQO, ALP and CLP can be obtained as special instances. The framework relies on a proof procedure which combines backward reasoning with logic programming clauses and forward reasoning with integrity constraints. 1 Introduction  Semantic Query Optimization (SQO) in deductive databases uses implicit knowledge coded in Integrity Constraints (ICs) to transform queries into new queries that are easier to evaluate and ideally contain only atoms of extensional predicates. SQO sometimes allows for unsatisfiable...",
    "neighbors": [
      400
    ],
    "mask": "Validation"
  },
  {
    "node_id": 772,
    "label": 1,
    "text": "Continuous Conceptual Set Covering: Learning Robot Operators From Examples Continuous Conceptual Set Covering (CCSC) is  an algorithm that uses engineering knowledge to  learn operator effects from training examples.  The program produces an operator hypothesis  that, even in noisy and nondeterministic  domains, can make good quantitative  predictions. An empirical evaluation in the traytilting  domain shows that CCSC learns faster  than an alternative case-based approach. The  best results, however, come from integrating  CCSC and the case-based approach.  Figure 1. Experimental Set Up  1. INTRODUCTION  Initially, the robot knows how to physically execute the tilt_operator. It does not, however, know the effects of the operator. When the tray-tilt operator is executed, the robot tips the tray down 30 from the horizontal in the direction of tilt. The new position of the puck is hard to predict because of uncertainty in the initial conditions (the initial position of the puck and the tilting angle are continuous values subject to measurement error). In additio...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 773,
    "label": 3,
    "text": "Decomposition of Object-Oriented Database Schemas Based on F-logic, we specify an advanced data model with object-oriented and  logic-oriented features. For this model we study the decomposition of a class, the  counterpart to the well-known decomposition of a relation scheme under functional  dependencies. For this decomposition of a class, the transformation pivoting is  used. Pivoting transplants some attributes of the class to a newly generated class.  This new class is a subclass of the result class of the so-called pivot attribute. The  pivot attribute maintains the link between the original class and the new subclass.  We identify the conditions for the output of pivoting being equivalent with its  input. Additionally, we show under which conditions a schema with functional  dependencies can be recursively transformed into an equivalent one without nonkey  functional dependencies.  1 Introduction  The theory of database schema design aims at formally characterising \\good\" schemas and at inventing algorithms to measure and to im...",
    "neighbors": [
      77,
      1073
    ],
    "mask": "Train"
  },
  {
    "node_id": 774,
    "label": 2,
    "text": "Improved Algorithms for Topic Distillation in a Hyperlinked Environment Abstract This paper addresses the problem of topic distillation on the World Wide Web, namely, given a typ-ical user query to find quality documents related to the query topic. Connectivity analysis has been shown to be useful in identifying high quality pages within a topic specific graph of hyperlinked documents. The essence of our approach is to augment a previous connectivity anal-ysis based algorithm with content analysis. We identify three problems with the existing approach and devise al-gorithms to tackle them. The results of a user evaluation are reported that show an improvement of precision at 10 documents by at least 45 % over pure connectivity anal-ysis. 1",
    "neighbors": [
      1,
      53,
      112,
      149,
      219,
      235,
      247,
      255,
      281,
      382,
      427,
      457,
      496,
      536,
      572,
      578,
      649,
      763,
      845,
      933,
      1000,
      1005,
      1056,
      1098,
      1099,
      1228,
      1247
    ],
    "mask": "Train"
  },
  {
    "node_id": 775,
    "label": 0,
    "text": "Training Teams with Collaborative Agents . Training teams is an activity that is expensive, time-consuming,  hazardous in some cases, and can be limited by availability of equipment and  personnel. In team training, the focus is on optimizing interactions, such as  efficiency of communication, conflict resolution and prioritization, group  situation awareness, resource distribution and load balancing, etc. This paper  presents an agent-based approach to designing intelligent team training systems.  We envision a computer-based training system in which teams are trained by  putting them through scenarios, which allow them to practice their team skills.  There are two important roles that intelligent agents can play; these are as  virtual team members and as coach. To carry out these functions, these agents  must be equipped with an understanding of the task domain, the team structure,  the selected decision-making process and their belief about other team  members' mental states.  1 Introduction  An integral element of large c...",
    "neighbors": [
      49,
      375
    ],
    "mask": "Train"
  },
  {
    "node_id": 776,
    "label": 1,
    "text": "Case-Based Learning Algorithms Abstract. Storing and using specific instances improves the performance of several supervised learning algorithms. These include algorithms that learn decision trees, classification rules, and distributed networks. However, no investigation has analyzed algorithms that use only specific instances to solve incremental learning tasks. In this paper, we describe a framework and methodology, called instance-based learning, that generates classification predictions using only specific instances. Instance-based learning algorithms do not maintain a set of abstractions derived from specific instances. This approach extends the nearest neighbor algorithm, which has large storage requirements. We describe how storage requirements can be significantly reduced with, at most, minor sacrifices in learning rate and classification accuracy. While the storage-reducing algorithm performs well on several realworld databases, its performance degrades rapidly with the level of attribute noise in training instances. Therefore, we extended it with a significance test to distinguish noisy instances. This extended algorithm's performance degrades gracefully with increasing noise levels and compares favorably with a noise-tolerant decision tree algorithm.",
    "neighbors": [
      935,
      975,
      1154,
      1259
    ],
    "mask": "Test"
  },
  {
    "node_id": 777,
    "label": 1,
    "text": "An Incremental Learning Algorithm with Automatically Derived Discriminating Features We propose a new technique which incrementally derive discriminating features in the input space. This technique casts both classification problems (class labels as outputs) and regression problems (numerical values as outputs) into a unified regression problem. The virtual labels are formed by clustering in the output space. We use these virtual labels to extract discriminating features in the input space. This procedure is performed recursively. We organize the resulting discriminating subspace in a coarse-to-fine fashion and store the information in a decision tree. Such an incrementally hierarchical discriminating regression (IHDR) decision tree can be realized as a hierarchical probability distribution model. We also introduce a sample size dependent negativelog -likelihood (NLL) metric to deal with large-sample size cases, small-sample size cases, and unbalanced-sample size cases. This is very essential since the number of training samples per class are different at each internal node of the IHDR tree. We report experimental results for two types of data: face image data along with comparison with some major appearance-based method and decision trees, hall way images with driving directions as outputs for the automatic navigation problem -- a regression application.",
    "neighbors": [
      1218
    ],
    "mask": "Validation"
  },
  {
    "node_id": 778,
    "label": 4,
    "text": "VRML with Constraints In this paper we discuss the benefits of extending VRML by constraints and present a new way based on prototypes and scripting to implement this extension. Our approach is easy-to-use, extensible and it considerably increases the expressivity of VRML. Our implementation supports one-way equational and finite domain constraints. We demonstrate the use of these constraints by means of several examples. Finally we argue that in the long run constraints should become an integral part of VRML.  CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism---Virtual Reality; D.3.3 [Programming Languages ]: Language Constructs and Features---Constraints  Keywords: VRML, Animation, Programming  1",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 779,
    "label": 4,
    "text": "The Gesture Pendant: A Self-illuminating, Wearable, Infrared Computer Vision System for Home Automation Control and Medical Monitoring In this paper we present a wearable device for control of home automation systems via hand gestures. This solution has many advantages over traditional home automation interfaces in that it can be used by those with loss of vision, motor skills, and mobility. By combining other sources of context with the pendant we can reduce the number and complexity of gestures while maintaining functionality. As users input gestures, the system can also analyze their movements for pathological tremors. This information can then be used for medical diagnosis, therapy, and emergency services.Currently, the Gesture Pendant can recognize control gestures with an accuracy of 95% and user defined gestures with an accuracy of 97% It can detect tremors above 2HZ within /- Hz.",
    "neighbors": [
      334
    ],
    "mask": "Test"
  },
  {
    "node_id": 780,
    "label": 2,
    "text": "WebMate: A Personal Agent for Browsing and Searching The World-Wide Web is developing very fast. Currently, #nding useful information  on the Web is a time consuming process. In this paper, we presentWebMate, an  agent that helps users to e#ectively browse and search the Web. WebMate extends  the state of the art in Web-based information retrieval in manyways. First, it uses  multiple TF-IDF vectors to keep track of user interests in di#erent domains. These  domains are automatically learned byWebMate. Second, WebMate uses the Trigger  Pair Model to automatically extract keywords for re#ning document search. Third,  during search, the user can provide multiple pages as similarity#relevance guidance  for the search. The system extracts and combines relevantkeywords from these relevant  pages and uses them for keyword re#nement. Using these techniques, WebMate  provides e#ective browsing and searching help and also compiles and sends to users  personal newspaper by automatically spiding news sources. Wehave experimentally  evaluated the performance of the system.",
    "neighbors": [
      228,
      348,
      606,
      952,
      1114
    ],
    "mask": "Train"
  },
  {
    "node_id": 781,
    "label": 2,
    "text": "Image Retrieval: Past, Present, And Future This paper provides a comprehensive survey of the technical achievements in the research area of Image Retrieval, especially Content-Based Image Retrieval, an area so active and prosperous in the past few years. The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multi-dimensional indexing, and system design, three of the fundamental bases of Content-Based Image Retrieval. Furthermore, based on the state-of-the-art technology available now and the demand from real-world applications, open research issues are identified, and future promising research directions are suggested. 1. INTRODUCTION Recent years have seen a rapid increase of the size of digital image collections. Everyday, both military and civilian equipment generates giga-bytes of images. Huge amount of information is out there. However, we can not access to or make use of the information unless it is organized so as to allow efficient browsing, searching and retriev...",
    "neighbors": [
      118,
      1203
    ],
    "mask": "Test"
  },
  {
    "node_id": 782,
    "label": 1,
    "text": "Probabilistic Affine Invariants for Recognition Under a weak perspective camera model, the image plane coordinates in different views of a planar object are related by an affine transformation. Because of this property, researchers have attempted to use affine invariants for recognition. However, there are two problems with this approach: (1) objects or object classes with inherent variability cannot be adequately treated using invariants; and (2) in practice the calculated affine invariants can be quite sensitive to errors in the image plane measurements. In this paper we use probability distributions to address both of these difficulties. Under the assumption that the feature positions of a planar object can be modeled using a jointly Gaussian density, we have derived the joint density over the corresponding set of affine coordinates. Even when the assumptions of a planar object and a weak perspective camera model do not strictly hold, the results are useful because deviations from the ideal can be treated as deformability in the ...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 783,
    "label": 0,
    "text": "Parameter Control Using the Agent Based Patchwork Model The setting of parameters in Evolutionary Algorithms (EA) has crucial influence on their performance. Typically, the best choice depends on the optimization task. Some parameters yield better results when they are varied during the run. Recently, the so-called TerrainBased Genetic Algorithm (TBGA) was introduced, which is a self-tuning version of the traditional Cellular Genetic Algorithm (CGA). In a TBGA, the individuals of the population are placed in a two-dimensional grid, where only neighbored individuals can mate with each other. The position of an individual in this grid is interpreted as its offspring 's specific mutation rate and number of crossover points. This approach allows to apply GA parameters that are optimal for (i) the type of optimization task and (ii) the current state of the optimization process. However, only a few individuals can apply the optimal parameters simultaneously due to their fixed position in the grid lattice. In this paper, we substituted the fixed s...",
    "neighbors": [
      462
    ],
    "mask": "Validation"
  },
  {
    "node_id": 784,
    "label": 1,
    "text": "Attribute Grammars for Genetic Representations of Neural Networks and Syntactic Constraints of Genetic Programming context-free grammar augmented by the assignment of semantic attributes to the symbols of the grammar. A production rule specifies not only the replacement of symbols, but also the evaluation of the symbol\u2019s attributes. In our research, an attribute grammar is used to specify classes of neural network structures with explicit representation of their functional organization. These representations provide useful constraints upon a genetic optimization that guarantee the preservation of syntactically correct genetic trees with semantically meaningful sub-trees. In this paper, we give a broad overview of our research into attribute grammar representations, from the basic and known capabilities, to the current ideas being addressed, to the future directions of our research. Attribute Grammars and Neural Networks",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 785,
    "label": 4,
    "text": "Seeking a Foundation for Context-Aware Computing Context-aware computing is generally associated with elements of the Ubiquitous Computing program, and the opportunity to distribute computation and interaction through the environment rather than concentrating it at the desktop computer. However, issues of context have also been important in other areas of HCI research. I argue that the scope of context-based computing should be extended to include not only Ubiquitous Computing, but also recent trends in tangible interfaces as well as work on sociological investigations of the organization of interactive behavior. By taking a view of contextaware computing that integrates these different perspectives, we can begin to understand the foundational relationships that tie them all together, and that provide a framework for understanding the basic principles behind these various forms of embodied interaction. In particular, I point to phenomenology as a basis for the development of a new framework for design and evaluation of context-aware ...",
    "neighbors": [
      1027
    ],
    "mask": "Train"
  },
  {
    "node_id": 786,
    "label": 4,
    "text": "Co-operative Evaluation of a Desktop Virtual Reality System A summative usability evaluation of a desktop virtual reality (VR) system was developed and a preliminary study then conducted. The purpose of the study was twofold. Firstly, to test whether the traditional evaluation technique, co-operative evaluation, is effective in the evaluation of desktop VR systems. Co-operative evaluation is a variation on a `think-aloud' verbal protocol, whereby, in addition to concurrently 'thinking-aloud', users are encouraged to ask any questions about an evaluation, relating to the computer-based system, the application, or the tasks that they are required to perform during the evaluation. As well as this, the evaluator may ask questions of the user at any time during the evaluation. Results from the study indicate that this additional probing technique enables an evaluator to elicit further usability problems that may not have otherwise been exteriorized by the user. Additionally, a method is developed which attempts to turn round the qualitative 'think-aloud' type data into quantitative data. This provides a way of evaluating empirical 'think-aloud' evaluation methods and will be useful for comparing their effectiveness to evaluate 3D virtual reality systems.",
    "neighbors": [
      565,
      678,
      1077
    ],
    "mask": "Train"
  },
  {
    "node_id": 787,
    "label": 1,
    "text": "Evolution and Development of a Central Pattern Generator for the Swimming of a Lamprey This paper describes the design of neural control architectures for locomotion using an evolutionary approach. Inspired by the central pattern generators found in animals, we develop neural controllers which can produce the patterns of oscillations necessary for the swimming of a simulated lamprey. This work is inspired by Ekeberg's neuronal and mechanical model of a lamprey [11], and follows experiments in which swimming controllers were evolved using a simple encoding scheme [26, 25]. Here, controllers are developed using an evolutionary algorithm based on the SGOCE encoding [31, 32] in which a genetic programming approach is used to evolve developmental programs which encode the growing of a dynamical neural network. The developmental programs determine how neurons located on a 2D substrate produce new cells through cellular division and how they form efferent or afferent interconnections. Swimming controllers are generated when the growing networks eventually create connections to ...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 788,
    "label": 3,
    "text": "A New Heuristic for Optimizing Large Queries There is a number of OODB optimization techniques proposed recently, such as the translation of path expressions into joins and query unnesting, that may generate a large number of implicit joins even for simple queries. Unfortunately, most current commercial query optimizers are still based on the dynamic programming approach of System R, and cannot handle queries of more than ten tables. There is a number of recent proposals that advocate the use of combinatorial optimization techniques, such as iterative improvement and simulated annealing, to deal with the complexity of this problem. These techniques, though, fail to take advantage of the rich semantic information inherent in the query specification, such as the information available in query graphs, which gives a good handle to choose which relations to join each time. This paper presents a polynomial-time algorithm that generates a good quality order of relational joins. It can also be used with minor modifications to sort OODB a...",
    "neighbors": [
      10,
      1047
    ],
    "mask": "Train"
  },
  {
    "node_id": 789,
    "label": 5,
    "text": "An Experimental Comparison of Localization Methods Localization is the process of updating the pose of a robot in an environment, based on sensor readings. In this experimental study, we compare two recent methods for localization of indoor mobile robots: Markov localization, which uses a probability distribution across a grid of robot poses; and scan matching, which uses Kalman filtering techniques based on matching sensor scans. Both these techniques are dense matching methods, that is, they match dense sets of environment features to an a priori map. To arrive at results for a range of situations, we utilize several different types of environments, and add noise to both the dead-reckoning and the sensors. Analysis shows that, roughly, the scan-matching techniques are more efficient and accurate, but Markov localization is better able to cope with large amounts of noise. These results suggest hybrid methods that are efficient, accurate and robust to noise.  1. Introduction  To carry out tasks, such as delivering objects, an indoor ro...",
    "neighbors": [
      1194
    ],
    "mask": "Train"
  },
  {
    "node_id": 790,
    "label": 1,
    "text": "The Use Of Artificial Intelligence To Improve The Numerical Optimization Of Complex Engineering Designs Gradient-based numerical optimization of complex engineering designs promises to produce better designs rapidly. However, such methods generally assume that the objective function and constraint functions are continuous, smooth, and defined everywhere. Unfortunately, realistic simulators tend to violate these assumptions. We present several artificial intelligence-based techniques for improving the numerical optimization of complex engineering designs in the presence of such pathologies in the simulators. We have tested the resulting system in several realistic engineering domains, and have found that using our techniques can greatly decrease the cost of design space search, and can also increase the quality of the resulting designs.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 791,
    "label": 5,
    "text": "The Bayes Net Toolbox for MATLAB The Bayes Net Toolbox (BNT) is an open-source Matlab package for directed graphical models. BNT supports many kinds of nodes (probability distributions), exact and approximate inference, parameter and structure learning, and static and dynamic models. BNT is widely used in teaching and research: the web page has received over 28,000 hits since May 2000. In this paper, we discuss a broad spectrum of issues related to graphical models (directed and undirected), and describe, at a high-level, how BNT was designed to cope with them all. We also compare BNT to other software packages for graphical models, and to the nascent OpenBayes effort.",
    "neighbors": [
      35,
      592
    ],
    "mask": "Train"
  },
  {
    "node_id": 792,
    "label": 2,
    "text": "A Statistical Method for Estimating the Usefulness of Text Databases Searching desired data in the Internet is one of the most common ways the Internet is used. No single  search engine is capable of searching all data in the Internet. The approach that provides an interface for  invoking multiple search engines for each user query has the potential to satisfy more users. When the  number of search engines under the interface is large, invoking all search engines for each query is often  not cost effective because it creates unnecessary network traffic by sending the query to a large number  of useless search engines and searching these useless search engines wastes local resources. The problem  can be overcome if the usefulness of every search engine with respect to each query can be predicted. In  this paper, we present a statistical method to estimate the usefulness of a search engine for any given  query. For a given query, the usefulness of a search engine in this paper is defined to be a combination of  the number of documents in the search engine that are sufficiently similar to the query and the average  similarity of these documents. Experimental results indicate that our estimation method is much more  accurate than existing methods.",
    "neighbors": [
      115,
      224,
      241,
      271,
      433,
      435,
      502,
      510,
      526,
      696,
      897,
      1124,
      1165,
      1253
    ],
    "mask": "Train"
  },
  {
    "node_id": 793,
    "label": 4,
    "text": "Adapting Web Information to Disabled and Elderly Users : Substantial research and standardization efforts already exist to make it easier for  people with physical impairments to perceive and interact with web pages. This paper describes  work aimed at catering the content of web pages to the needs of different users, including  elderly people and users with vision and motor impairments. The AVANTI system and related  efforts in the AVANTI project will be discussed and experiences reported.  1. Introduction  The World Wide Web is currently the most frequently visited electronic resource and is likely to become the access ramp to the electronic information highway of the next millennium. Web access should therefore ideally be available to everyone in order not to create yet another informational, and hence economical and social, disparity in society. Special efforts must be put into making the access to the web available to those who so far have been at a disadvantage, including people with disabilities and elderly people who until recently...",
    "neighbors": [
      352
    ],
    "mask": "Train"
  },
  {
    "node_id": 794,
    "label": 3,
    "text": "Event Composition in Time-dependent Distributed Systems Many interesting application systems, ranging from workflow  management and CSCW to air traffic control, are eventdriven  and time-dependent and must interact with heterogeneous  components in the real world. Event services are used  to glue together distributed components. They assume a virtual  global time base to trigger actions and to order events.  The notion of a global time that is provided by synchronized  local clocks in distributed systems has a fundamental impact  on the semantics of event-driven systems, especially the composition  of events. The well studied 2g-precedence model,  which assumes that the granularity of global time-base g can  be derived from a priori known and bounded precision of  local clocks may not be suitable for the Internet where the  accuracy and external synchronization of local clocks is best  effort and cannot be guaranteed because of large transmission  delay variations and phases of disconnection. In this  paper we introduce a mechanism based on...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 795,
    "label": 3,
    "text": "Incremental Computation and Maintenance of Temporal Aggregates Abstract. We consider the problems of computing aggregation queries in temporal databases and of maintaining materialized temporal aggregate views efficiently. The latter problem is particularly challenging since a single data update can cause aggregate results to change over the entire time line. We introduce a new index structure called the SB-tree, which incorporates features from both segment-trees and B-trees. SB-trees support fast lookup of aggregate results based on time and can be maintained efficiently when the data change. We extend the basicSB-tree index to handle cumulative (also called moving-window) aggregates, considering separately cases when the window size is or is not fixed in advance. For materialized aggregate views in a temporal database or warehouse, we propose building and maintaining SB-tree indices instead of the views themselves.",
    "neighbors": [
      368,
      706,
      863
    ],
    "mask": "Train"
  },
  {
    "node_id": 796,
    "label": 2,
    "text": "General Query Expansion Techniques For Spoken Document Retrieval This paper presents some developments in query expansion and document representation of our Spoken Document Retrieval (SDR) system since the 1998 Text REtrieval Conference (TREC-7). We have shown that a modification of the document representation combining several techniques for query expansion can improve Average Precision by 17 % relative to a system similar to that which we presented at TREC-7 [1]. These new experiments have also confirmed that the degradation of Average Precision due to a Word Error Rate (WER) of 25 % is relatively small (around 2 % relative). We hope to repeat these experiments when larger document collections become available to evaluate the scalability of these techniques. 1.",
    "neighbors": [
      11,
      604,
      755
    ],
    "mask": "Train"
  },
  {
    "node_id": 797,
    "label": 1,
    "text": "Symbol Grounding: A New Look At An Old Idea Symbols should be grounded, as has been argued before. But we insist that they should be  grounded not only in subsymbolic activities, but also in the interaction between the agent and the  world. The point is that concepts are not formed in isolation (from the world), in abstraction, or  \\objectively\". They are formed in relation to the experience of agents, through their perceptual  /motor apparatuses, in their world and linked to their goals and actions.  In this paper, we will take a detailed look at this relatively old issue, using a new perspective,  aided by our new work of computational cognitive model development. To further our understanding,  we also go back in time to link up with earlier philosophical theories related to this issue. The  result is an account that extends from computational mechanisms to philosophical abstractions.  1  Symbol Grounding: A New Look At An Old Idea  Symbols should be grounded, as has been argued before. But we insist that they should be ground...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 798,
    "label": 4,
    "text": "What Do We Want From a Wearable User Interface? This document outlines the author's ultimate goal and suggests one technology that can be exploited by applications writers to make them fit neatly into future application integration frameworks.",
    "neighbors": [
      345
    ],
    "mask": "Train"
  },
  {
    "node_id": 799,
    "label": 2,
    "text": "Using Category-Based Collaborative Filtering in the Active Webmuseum Collaborative filtering is an important technology for creating useradapting Web sites. In general the efforts of improving filtering algorithms and using the predictions for the presentation of filtered objects are decoupled. Therefore, common measures (or metrics) for evaluating collaborative filtering (recommender) systems focus mainly on the prediction algorithm. It is hard to relate the classic measurements to actual user satisfaction because of the way the  user interacts with the recommendations, determined by their representation,  influences the benefits for the user. We propose an abstract access paradigm, which can be applied to the design of filtering systems, and at the same time formalizes the access to filtering results via multi-corridors (based on content-based categories) . This leads to new measures which better relate to the user satisfaction. We use these measures to evaluate the use of various  kinds of multi-corridors for our prototype user-adapting Web site the: Active WebMuseum.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 800,
    "label": 0,
    "text": "Secure Mobile Code: The JavaSeal experiment Mobile agents are programs that move between sites during execution to benefit from the services and information present at each site. To gain wide acceptance, strong security guarantees must be given: sites must be protected from malicious agents and agents must be protected from each other. Software based protection is widely viewed as the most efficient way of enforcing agent security. In the first part of the paper, we review programming language support for security. This review also helps to highlight weaknesses in the Java security model. In the second part of the paper, we make good on the lessons learned in the review to design a security architecture for the JavaSeal agent platform.  1 Introduction  Wide area networks such as the Internet hold the promise of a brave new wired world of global computing. The hope is to have distributed applications that scale to the size of the Internet and seamlessly provide access to massive amounts of information and value added services. Bu...",
    "neighbors": [
      258
    ],
    "mask": "Train"
  },
  {
    "node_id": 801,
    "label": 2,
    "text": "Data-Driven Generation of Decision Trees for Motif-Based Assignment of Protein Sequences to Functional Families This paper describes an approach to data-driven discovery of sequence motif-based models in the form of decision trees for assigning",
    "neighbors": [
      761
    ],
    "mask": "Test"
  },
  {
    "node_id": 802,
    "label": 1,
    "text": "Automated Facial Expression Recognition Based on FACS Action Units Automated recognition of facial expression is an important addition to computer vision research because of its relevance to the study of psychological phenomena and the development of human-computer interaction (HCI). We developed a computer vision system that automatically recognizes individual action units or action unit combinations in the upper face using Hidden Markov Models (HMMs). Our approach to facial expression recognition is based on the Facial Action Coding System (FACS), which separates expressions into upper and lower face action. In this paper, we use three approaches to extract facial expression information: (1) facial feature point tracking, (2) dense flow tracking with principal component analysis (PCA), and (3) high gradient component detection (i.e., furrow detection). The recognition results of the upper face expressions using feature point tracking, dense flow tracking, and high gradient component detection are 85%, 93%, and 85%, respectively.  1. Introduction  Fa...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 803,
    "label": 3,
    "text": "Probabilistic Logic Programming with Conditional Constraints . We introduce a new approach to probabilistic logic programming in which probabilities are defined over a set of possible worlds. More precisely, classical program clauses are extended by a subinterval of [0; 1] that describes a range for the conditional probability of the head of a clause given its body. We then analyze the complexity of selected probabilistic logic programming tasks. It turns out that probabilistic logic programming is computationally more complex than classical logic programming. More precisely, the tractability of special cases of classical logic programming generally does not carry over to the corresponding special cases of probabilistic logic programming. Moreover, we also draw a precise picture of the complexity of deciding and computing tight logical consequences in probabilistic reasoning with conditional constraints in general. We then present linear optimization techniques for deciding satisfiability and computing tight logical consequences of probabilistic...",
    "neighbors": [
      237
    ],
    "mask": "Test"
  },
  {
    "node_id": 804,
    "label": 1,
    "text": "Learning Feed-Forward and Recurrent Fuzzy Systems: A Genetic Approach In this paper we present a new learning method for rule-based feed-forward and recurrent fuzzy systems. Recurrent fuzzy systems have hidden fuzzy variables and can approximate the temporal relation embedded in dynamic processes of unknown order. The learning method is universal i.e. it selects optimal width and position of Gaussian like membership functions and it selects a minimal set of fuzzy rules as well as the structure of the rules. A Genetic Algorithm is used to estimate the Fuzzy Systems which capture low complexity and minimal rule base. Optimization of the \"entropy\" of a fuzzy rule base leads to a minimal number of rules, of membership functions and of sub-premises together with an optimal input/output behavior. Most of the resulting Fuzzy Systems are comparable to systems designed by an expert but offers a better performance. The approach is compared to others by a standard benchmark (a system identification process). Different results for feed-forward and first order recurrent Fuzzy Systems with symmetric and non-symmetric membership functions are presented. Key words: Fuzzy logic controller, recurrent fuzzy systems, genetic algorithm, entropy of fuzzy rule, machine learning, dynamic processes. 1",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 805,
    "label": 1,
    "text": "The Evolutionary Unfolding of Complexity   We analyze the population dynamics of a broad class of tness functions that exhibit epochal evolution -- a dynamical behavior, commonly observed in both natural and artificial evolutionary processes, in which long periods of stasis in an evolving population are punctuated by sudden bursts of change. Our approach -- statistical dynamics -- combines methods from both statistical mechanics and dynamical systems theory in a way that offers an alternative to current \"landscape\" models of evolutionary optimization. We describe the population dynamics on the macroscopic level of tness classes or phenotype subbasins, while averaging out the genotypic variation that is consistent with a macroscopic state. Metastability in epochal evolution occurs solely at the macroscopic level of the fitness distribution. While a balance between selection and mutation maintains a quasistationary distribution of fitness, individuals diffuse randomly through selectively neutral subbasins in genotype space. Sudden innovations occur when, through this diffusion, a genotypic portal is discovered that connects to a new subbasin of higher fitness genotypes. In this way, we identify innovations with the unfolding and stabilization of a new dimension in the macroscopic state space. The architectural view of subbasins and portals in genotype space clarifies how frozen accidents and the resulting phenotypic constraints guide the evolution to higher complexity.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 806,
    "label": 4,
    "text": "Consumer Eye Movement Patterns on Yellow Pages Advertising Process tracing data help understand how yellow pages advertisement characteristics influence consumer information processing behavior. A laboratory experiment collected eye movement data while consumers chose businesses from phone directories. Consumers scan listings in alphabetic order. Their scan is not exhaustive. As a result, some ads are never seen. Consumers noticed over 93% of the quarter page display ads but only 26% of the plain listings. Consumers perceived color ads before ads without color, noticed more color ads than non-color ads and viewed color ads 21% longer than equivalent ads without color. Users viewed 42% more bold listings than plain listings. Consumers spent 54% more time viewing ads they end up choosing which demonstrates the importance of attention on subsequent choice behavior.  1 INTRODUCTION  In 1992, yellow pages directories were a $9.4 billion dollar information services business that reached 98% of American households (Mangel 1992). It is the fourth larg...",
    "neighbors": [
      694
    ],
    "mask": "Test"
  },
  {
    "node_id": 807,
    "label": 1,
    "text": "A Unifying Information-theoretic Framework for Independent Component Analysis We show that different theories recently proposed for Independent Component Analysis (ICA) lead to the same iterative learning algorithm for blind separation of mixed independent sources. We review those theories and suggest that information theory can be used to unify several lines of research. Pearlmutter and Parra (1996) and Cardoso (1997) showed that the infomax approach of Bell and Sejnowski (1995) and the maximum likelihood estimation approach are equivalent. We show that negentropy maximization also has equivalent properties and therefore all three approaches yield the same learning rule for a fixed nonlinearity. Girolami and Fyfe (1997a) have shown that the nonlinear Principal Component Analysis (PCA) algorithm of Karhunen and Joutsensalo (1994) and Oja (1997) can also be viewed from information-theoretic principles since it minimizes the sum of squares of the fourth-order marginal cumulants and therefore approximately minimizes the mutual information (Comon, 1994). Lambert (19...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 808,
    "label": 0,
    "text": "Extending Multi-Agent Cooperation by Overhearing Much cooperation among humans happens following a common pattern:  by chance or deliberately, a person overhears a conversation between  two or more parties and steps in to help, for instance by suggesting answers  to questions, by volunteering to perform actions, by making observations  or adding information. We describe an abstract architecture to support a  similar pattern in societies of articial agents. Our architecture involves  pairs of so-called service agents (or services) engaged in some tasks, and  unlimited number of suggestive agents (or suggesters). The latter have  an understanding of the work behaviors of the former through a publicly  available model, and are able to observe the messages they exchange. Depending  on their own objectives, the understanding they have available,  and the observed communication, the suggesters try to cooperate with  the services, by initiating assisting actions, and by sending suggestions to  the services. These in eect may induce a change in services behavior.  To test our architecture, we developed an experimental, multi-agent Web  site. The system has been implemented by using a BDI toolkit, JACK  Intelligent Agents.  Keywords: autonomous agents, multiagent systems, AI architectures, distributed AI.  1",
    "neighbors": [
      285,
      499,
      687,
      816
    ],
    "mask": "Test"
  },
  {
    "node_id": 809,
    "label": 3,
    "text": "Partial Models of Extended Generalized Logic Programs . In recent years there has been an increasing interest in extensions  of the logic programming paradigm beyond the class of normal  logic programs motivated by the need for a satisfactory respresentation  and processing of knowledge. An important problem in this area is to find  an adequate declarative semantics for logic programs. In the present paper  a general preference criterion is proposed that selects the `intended'  partial models of extended generalized logic programs which is a conservative  extension of the stationary semantics for normal logic programs  of [13], [15] and generalizes the WFSX-semantics of [12]. The presented  preference criterion defines a partial model of an extended generalized  logic program as intended if it is generated by a stationary chain. The  GWFSX-semantics is defined by the set-theoretical intersection of all  stationary generated models, and thus generalizes the results from [9]  and [1].  1 Introduction  Declarative semantics provides a mathem...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 810,
    "label": 3,
    "text": "Processing of Spatiotemporal Queries in Image Databases Overlapping Linear Quadtrees is a structure suitable for storing consecutive raster images according to transaction time (a database of evolving images). This structure saves considerable space without sacrificing time performance in accessing every single image. Moreover, it can be used for answering efficiently window queries for a number of consecutive images (spatio-temporal queries). In this paper, we present three such temporal window queries: strict containment, border intersect and cover. Besides, based on a method of producing synthetic pairs of evolving images (random images with specified aggregation) we present empirical results on the I/O performance of these queries.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 811,
    "label": 4,
    "text": "From PETS to Storykit: Creating New Technology with an Intergenerational Design Team Working with children as our design partners, our  intergenerational design team at the University of Maryland  has been developing both new design methodologies and  new storytelling technology for children. In this paper, we  focus on two recent results of our efforts: PETS, a robotic  storyteller, and Storykit, a construction kit of low-tech and  high-tech components to build immersive StoryRooms. We  then describe some lessons we learned.  Introduction  Over the past two years, our intergenerational design team  at the University of Maryland has been developing new design methodologies to create new storytelling technology for children. This team is made of six adult researchers from computer science, education, art, and engineering, and seven children, ages 7 to 11, from local elementary schools. These children stay with us for a long term, at least for one year. The adults are undergraduate students, graduate students, and faculty from art, education, engineering, and computer sc...",
    "neighbors": [
      1074
    ],
    "mask": "Test"
  },
  {
    "node_id": 812,
    "label": 3,
    "text": "dlv { An Overview the Intelligent Grounding, the Model Generator and the Model Checker. All of these modules perform a modular evaluation of their input according to various dependency graphs as de  ned in [5, 2] and try to detect and eciently handle special (syntactic) subclasses, which in general yields a tremendous speedup.    Supported by FWF (Austrian Science Funds) under the project P11580-MAT \\A Query System for Disjunctive Deductive Databases\"    Please address correspondence to this author.  The Intelligent Grounding takes an input program, whose facts can be stored also in the tables of external relational databases, and eciently generates a subset of the program instantiation that has exactly the same stable models as the full program, but is much smaller in general. (For strati  ed programs, for example, the Grounding already computes the single stable model.)  Then the Model Generator is run on the (ground) output of the Intelligent Grounding. It generates one candidate for a stable model a",
    "neighbors": [
      413
    ],
    "mask": "Train"
  },
  {
    "node_id": 813,
    "label": 3,
    "text": "Electronic Books in Digital Libraries 1  Electronic book is an application with a multimedia database of instructional resources, which include hyperlinked text, instructor's audio/video clips, slides, animation, still images, etc. as well as content-based information about these data, and metadata such as annotations, tags, and cross-referencing information. Electronic books in the Internet or on CDs today are not easy to learn from. We propose the use of a multimedia database of instructional resources in constructing and delivering multimedia lessons about topics in an electronic book. We introduce an electronic book data model containing (a) topic objects and (b) instructional resources, called instruction module objects, which are multimedia presentations possibly capturing real-life lectures of instructors. We use the notion of topic prerequisites for topics at different detail levels, to allow electronic book users to request/compose multimedia lessons about topics in the electronic book. We present automated construction of the \"best\" user-tailored lesson (as a multimedia presentation. 1.",
    "neighbors": [
      590
    ],
    "mask": "Validation"
  },
  {
    "node_id": 814,
    "label": 3,
    "text": "InterBase-KB: Integrating a Knowledge Base System with a Multidatabase System for Data Warehousing This paper describes the integration of a multidatabase system and a knowledge-base system to support",
    "neighbors": [
      57
    ],
    "mask": "Train"
  },
  {
    "node_id": 815,
    "label": 3,
    "text": "Dynamic CPU Scheduling with Imprecise Knowledge of Computation-Time : The majority of the studies conducted in scheduling real-time transactions mostly concentrate  on concurrency control protocols, while overlooking the CPU as being the primary resource.  Consequently, there are various techniques for scheduling the CPU in conventional time-critical systems;  meanwhile, there does not seem to be any technique that is adequately designed for scheduling  such a resource in Real-Time Database (RTDB) systems. In this paper, we construct an efficient CPU  scheduling scheme that minimizes the preemption rate in order to reduce the frequency by which synchronization  protocols must be invoked, along with their inherited performance degradation. In addition,  we also introduce a new timing model upon which the newly introduced scheduler is incorpo-  rated in order to utilize the system's imprecise knowledge of computation time estimates.  Keywords: CPU Scheduling, lowering-preemption, timeliness-functions, and imprecise computation  estimates.  1. INTRODUCTIO...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 816,
    "label": 0,
    "text": "Means-End Plan Recognition - Towards a Theory of Reactive Recognition This paper draws its inspiration from current work in reactive planning to guide plan recognition using \"plans as recipes\". The plan recognition process guided by such a library of plans is called means-end plan recognition. An extension of dynamic logic, called dynamic agent logic, is introduced to provide a formal semantics for means-end plan recognition and its counterpart, means-end plan execution. The operational semantics, given by algorithms for means-end plan recognition, are then related to the provability of formulas in the dynamic agent logic. This establishes the relative soundness and completeness of the algorithms with respect to a given library of plans. Some of the restrictive assumptions underlying means-end plan recognition are then relaxed to provide a theory of reactive recognition that allows for changes in the external world during the recognition process. Reactive recognition, when embedded with the mental attitudes of belief, desire, and intention, leads to a po...",
    "neighbors": [
      223,
      303,
      499,
      808,
      953
    ],
    "mask": "Train"
  },
  {
    "node_id": 817,
    "label": 2,
    "text": "Assessing Software Libraries by Browsing Similar Classes, Functions and Relationships Comparing and contrasting a set of software libraries is useful for reuse related activities such as selecting a library from among several candidates or porting an application from one library to another. The current state of the art in assessing libraries relies on qualitative methods. To reduce costs and/or assess a large collection of libraries, automation is necessary. Although there are tools that help a developer examine an individual library in terms of architecture, style, etc., we know of no tools that help the developer directly compare several libraries. With existing tools, the user must manually integrate the knowledge learned about each library. Automation to help developers directly compare and contrast libraries requires matching of similar components (such as classes and functions) across libraries. This is different than the traditional component retrieval problem in which components are returned that best match a user's query. Rather, we need to find those component...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 818,
    "label": 2,
    "text": "Generative Models for Cold-Start Recommendations Systems for automatically recommending items (e.g., movies, products, or information) to users are becoming increasingly important in e-commerce applications, digital libraries, and other domains where personalization is highly valued. Such recommender systems typically base their suggestions on (1) collaborative data encoding which users like which items, and/or (2) content data describing item features and user demographics. Systems that rely solely on collaborative data fail when operating from a cold start---that is, when recommending items (e.g., first-run movies) that no member of the community has yet seen. We develop several generative probabilistic models that circumvent the cold-start problem by mixing content data with collaborative data in a sound statistical manner. We evaluate the algorithms using MovieLens movie ratings data, augmented with actor and director information from the Internet Movie Database. We find that maximum likelihood learning with the expectation maximization (EM) algorithm and variants tends to overfit complex models that are initialized randomly. However, by seeding parameters of the complex models with parameters learned in simpler models, we obtain greatly improved performance. We explore both methods that exploit a single type of content data (e.g., actors only) and methods that leverage multiple types of content data (e.g., both actors and directors) simultaneously.",
    "neighbors": [
      722,
      864,
      937,
      1141
    ],
    "mask": "Validation"
  },
  {
    "node_id": 819,
    "label": 2,
    "text": "What can you do with a Web in your Pocket? The amount of information available online has grown enormously over the past decade. Fortunately, computing power, disk capacity, and network bandwidth have also increased dramatically. It is currently possible for a university research project to store and process the entire World Wide Web. Since there is a limit on how much text humans can generate, it is plausible that within a few decades one will be able to store and process all the human-generated text on the Web in a shirt pocket. The Web is a very rich and interesting data source. In this paper, we describe the Stanford WebBase, a local repository of a significant portion of the Web. Furthermore, we describe a number of recent experiments that leverage the size and the diversity of the WebBase. First, we have largely automated the process of extracting a sizable relation of books (title, author pairs) from hundreds of data sources spread across the World Wide Web using a technique we call Dual Iterative Pattern Relation Extraction. Second, we have developed a global ranking of Web pages called PageRank based on the link structure of the Web that has properties that are useful for search and navigation. Third, we have used PageRank to develop a novel search engine called Google, which also makes heavy use of anchor text. All of these experiments rely significantly on the size and diversity of the WebBase. 1",
    "neighbors": [
      572,
      1017,
      1189
    ],
    "mask": "Train"
  },
  {
    "node_id": 820,
    "label": 0,
    "text": "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy ofsmaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics|as a subroutine hierarchy|and a declarative semantics|as a representation of the value function of a hierarchical policy. MAXQ uni es and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and de ne subtasks that achieve these subgoals. By de ning such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the",
    "neighbors": [
      65,
      1238
    ],
    "mask": "Train"
  },
  {
    "node_id": 821,
    "label": 0,
    "text": "Agents That Talk Back (Sometimes): Filter Programs for Affective Communication This paper introduces a model of interaction between  users and animated agents as well as inter-agent interaction  that supports basic features of affective conversation. As essential requirements for animated agents' capability to engage in and exhibit affective communication  we motivate reasoning about emotion and emotion expression, personality, and social role awareness. The main contribution of our paper is the discussion of  so-called `filter programs' that may qualify an agent's expression of its emotional state by its personality and the social context. All of the mental concepts that determine  emotion expression, such as emotional state, personality, standards, and attitudes, have associated  intensities for fine-tuning the agent's reactions in user-adapted  environments.",
    "neighbors": [
      723,
      1102
    ],
    "mask": "Train"
  },
  {
    "node_id": 822,
    "label": 5,
    "text": "Accurate and Fast Proximity Queries Between Polyhedra Using Convex Surface Decomposition The need to perform fast and accurate proximity queries arises frequently in physically-based modeling, simulation,  animation, real-time interaction within a virtual environment, and game dynamics. The set of proximity  queries include intersection detection, tolerance verification, exact and approximate minimum distance computation,  and (disjoint) contact determination. Specialized data structures and algorithms have often been designed  to perform each type of query separately. We present a unified approach to perform any of these queries seamlessly  for general, rigid polyhedral objects with boundary representations which are orientable 2-manifolds. The  proposed method involves a hierarchical data structure built upon a surface decomposition of the models. Furthermore,  the incremental query algorithm takes advantage of coherence between successive frames. It has been  applied to complex benchmarks and compares very favorably with earlier algorithms and systems.  1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 823,
    "label": 1,
    "text": "Hierarchical Discriminant Regression The main motivation of this paper is to propose a new classification and regression method for challenging high dimensional data. The proposed new technique casts classification problems (class labels as output) and regression problems (numeric values as output) into a unified regression problem. This unified view enables classification problems to use numeric information in the output space that is available for regression problems but are traditionally not readily available for classification problems -- distance metric among clustered class labels for coarse and fine classifications. A doubly clustered subspace-based hierarchical discriminating regression (HDR) method is proposed in this work. The major characteristics include: (1) Clustering is performed in both output space and input space at each internal node and thus the term \"doubly clustered.\" Clustering in the output space provides virtual labels for computing clusters in the input space. (2) Discriminants in the input spa...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 824,
    "label": 2,
    "text": "Similarity Measures With complex multimedia data, we see the emergence of database systems in which the fundamental operation is similarity assessment. Before database issues can be addressed, it is necessary to give a definition of similarity as an operation. In this paper we develop a similarity measure, based on fuzzy logic, that exhibit several features that match experimental findings in humans. The model is dubbed Fuzzy Feature Contrast (FFC) and is an extension to a more general domain of the Feature Contrast model due to Tversky. We show how the FFC model can be used to model similarity assessment from fuzzy judgment of properties, and we address the use of fuzzy measures to deal with dependencies among the properties. 1 Introduction  Comparing two images, or an image and a model, is the fundamental operation for many Visual Information Retrieval systems. In most systems of interest, a simple pixel-by-pixel comparison won't do: the difference that we determine must bear some correlation with the p...",
    "neighbors": [
      523
    ],
    "mask": "Test"
  },
  {
    "node_id": 825,
    "label": 1,
    "text": "A Novel Sensor for Dynamic Tactile Information We present a novel tactile sensor, which is useful for dextrous grasping with a simple robot gripper. The novel part consists of an array of capacitive sensors, which couple to the object by means of little brushes of fibers. These sensor elements are very sensitive (with a threshold of about 5 mN) but robust enough not to be damaged during grasping. They yield two types of dynamical tactile information corresponding roughly to two types of tactile sensors in the human skin. The complete sensor consists of a foil-based static force sensor, which yields the total force and the center of the two-dimensional force distribution and is surrounded by an array of the dynamical sensor elements. One such sensor has been mounted on each of the two gripper jaws of our humanoid robot and equipped with the necessary read-out electronics and a CAN bus interface. As first applications we describe experiments to evaluate the quality of a grip using the sensor measurements and a utility that allows to ...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 826,
    "label": 1,
    "text": "A Process-Oriented Heuristic for Model Selection Current methods to avoid overfitting are either data-oriented (using separate data for validation) or representation-oriented (penalizing complexity in the model). This paper proposes process-oriented evaluation, where a model's expected generalization error is computed as a function of the search process that led to it. The paper develops the necessary theoretical framework, and applies it to one type of learning: rule induction. A process-oriented version of the CN2 rule learner is empirically compared with the default CN2. The process-oriented version is more accurate in a large majority of the datasets, with high significance, and also produces simpler models. Experiments in artificial domains suggest that processoriented evaluation is particularly useful in high-dimensional domains. 1 INTRODUCTION  Overfitting avoidance is often considered the central problem of machine learning (e.g., (Cheeseman & Oldford, 1994)). If a learner is sufficiently powerful, it must guard against selec...",
    "neighbors": [
      890
    ],
    "mask": "Train"
  },
  {
    "node_id": 827,
    "label": 0,
    "text": "Modelling Rational Inquiry in Non-Ideal Agents The construction of rational agents is one of the goals that has been pursued in Artificial Intelligence (AI). In most of the architectures that have been proposed for this kind of agents, its behaviour is guided by its set of beliefs. In our work, rational agents are those systems that are permanently engaged in the process of rational inquiry; thus, their beliefs keep evolving in time, as a consequence of their internal inference procedures and their interaction with the environment. Both AI researchers and philosophers are interested in having a formal model of this process, and this is the main topic in our work. Beliefs have been formally modelled in the last decades using doxastic logics. The possible worlds model and its associated Kripke semantics provide an intuitive semantics for these logics, but they seem to commit us to model agents that are logically omniscient and perfect reasoners. We avoid these problems by replacing possible worlds by conceivable situations,  which ar...",
    "neighbors": [
      425
    ],
    "mask": "Train"
  },
  {
    "node_id": 828,
    "label": 1,
    "text": "Compact Fuzzy Models Through Complexity Reduction and Evolutionary Optimization Genetic Algorithms (GAs) and other evolutionary optimization  methods to design fuzzy rules from data for systems modeling and  classification have received much attention in recent literature. We show  that different tools for modeling and complexity reduction can be favorably  combined in a scheme with GA-based parameter optimization. Fuzzy clustering,  rule reduction, rule base simplification and constrained genetic optimization  are integrated in a data-driven modeling scheme with low human  intervention. Attractive models with respect to compactness, transparency  and accuracy, are the result of this symbiosis.  I. INTRODUCTION  We focus on learning fuzzy rules from data with low human intervention. Many tools to initialize, tune and manipulate fuzzy models have been developed. We show that different tools can be favorably combined to obtain compact fuzzy rule-based models of low complexity with still good approximation accuracy. A modeling scheme is presented that combine four pr...",
    "neighbors": [
      157,
      560,
      1202
    ],
    "mask": "Train"
  },
  {
    "node_id": 829,
    "label": 0,
    "text": "Autonomous Cyber Agents: Rules For Collaboration A cyber agent is any program, machine or person engaged in computer-enabled work. Thus, cyber agents can vary considerably in complexity and intelligence. Can they, despite their variety, be organized to collaborate effectively ? Both empirical evidence and theory suggest that they can. Moreover, there seem to be simple rules for designing problem-solving organizations in which collaboration among cyber agents is automatic and scale-effective (adding agents tends to improve solution-quality; adding computers tends to improve solution-speed). This paper develops some of these rules.  1. INTRODUCTION  Computer networks make it possible to interconnect and therefore, organize, large numbers of distributed cyber agents, varying in type from simple programs to skilled humans. Our goal is to develop a class of organizations in which such agents can collaborate easily and effectively. More specifically, our goal is to develop methods for routinely solving arbitrary instances of the following ...",
    "neighbors": [
      672
    ],
    "mask": "Test"
  },
  {
    "node_id": 830,
    "label": 1,
    "text": "Integration Of Speech And Vision Using Mutual Information We are developing a system which learns words from co-occurring spoken and visual input. The goal is to automatically segment continuous speechatword boundaries without a lexicon, and to form visual categories which correspond to spoken words. Mutual information is used to integrate acoustic and visual distance metrics in order to extract an audio-visual lexicon from raw input. Wereport  results of experiments with a corpus of infant-directed speech and images.  1. INTRODUCTION  We are developing systems which learn words from co-occurring  audio and visual input [5, 4]. Input consists of naturally spoken mutliword utterances paired with visual representations of object shapes (Figure 1). Output of the system is an audio-visual lexicon of sound-shape associations which encode acoustic forms of words (or phrases) and their visually grounded referents. We assume that, in general, the audio and visual signals are uncorrelated in time. However, when a wordisspoken, its visual representatio...",
    "neighbors": [
      848
    ],
    "mask": "Train"
  },
  {
    "node_id": 831,
    "label": 1,
    "text": "Enhancing the Sense of Other Learners in Student-Centred Web-Based Education Student-centred learning can be used in Web-courses to increase student activity, motivation and commitment. EDUCO is a system for student-centred learning, both for the learners and the teachers. Students can use EDUCO within a standard web-browser to navigate towards useful information and Web-resources gathered into the system. The key issue is that every participant can see everyone else in the system and their navigational steps, so that the feeling of student companions taking part in the same tasks is increased. The implications of this type of social navigation are discussed along with the description of the system itself. 1.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 832,
    "label": 1,
    "text": "A Hybrid Decision Tree/genetic Algorithm for Coping With the Problem of Small Disjuncts in Data Mining The problem of small disjuncts is a serious  challenge for data mining algorithms. In essence,  small disjuncts are rules covering a small  number of examples. Due to their nature, small  disjuncts tend to be error prone and contribute to  a decrease in predictive accuracy. This paper  proposes a hybrid decision tree/genetic algorithm  method to cope with the problem of small  disjuncts. The basic idea is that examples  belonging to large disjuncts are classified by  rules produced by a decision-tree algorithm,  while examples belonging to small disjuncts  (whose classification is considerably more  difficult) are classified by rules produced by a  genetic algorithm specifically designed for this  task.  1 INTRODUCTION  In the context of the well-known classification task of data mining, the discovered knowledge is often expressed as a set of IF-THEN rules, since this kind of knowledge representation is intuitive for the user. From a logical viewpoint, typically the discovered rules ar...",
    "neighbors": [
      231
    ],
    "mask": "Train"
  },
  {
    "node_id": 833,
    "label": 4,
    "text": "On-line 3D gesture recognition utilising dissimilarity measures In the field of Human-Computer Interaction (HCI), gesture recognition is becoming increasingly important as a mode of communication, in addition to the more common visual, aural and oral modes, and is of particular interest to designers of Augmentative and Alternative Communication (AAC) systems for people with disabilities. A complete microcomputer system is described, GesRec3D, which facilitates the data acquisition, segmentation, learning, and recognition of 3-Dimensional arm gestures. The gesture data is acquired from a Polhemus electro-magnetic tracker system, where sensors are placed on the finger, wrist and elbow of one arm. Coded gestures are linked to user-defined text, to be typed or spoken by a text-to-speech engine, which is integrated into the system. A segmentation method and an algorithm for classification are both presented, which includes acceptance/rejection thresholds based on intra-class and inter-class dissimilarity measures. Results of recognition hits, confusion ...",
    "neighbors": [
      635
    ],
    "mask": "Validation"
  },
  {
    "node_id": 834,
    "label": 1,
    "text": "Different approaches to induce cooperation in fuzzy linguistic models under the COR methodology Nowadays, Linguistic Modeling is considered to be one of the most important areas of application for Fuzzy Logic. It is accomplished by linguistic Fuzzy Rule-Based Systems, whose most interesting feature is the interpolative reasoning developed. This characteristic plays a key role in their high performance and is a consequence of the cooperation among the involved fuzzy rules. A new approach that makes good use of this aspect inducing cooperation among rules is introduced in this contribution: the Cooperative Rules methodology. One of its interesting advantages is its flexibility allowing it to be used with dierent combinatorial search techniques. Thus, four specic metaheuristics are considered: simulated annealing, tabu search, genetic algorithms and ant colony optimization. Their good performance is shown when solving a real-world problem.",
    "neighbors": [
      152
    ],
    "mask": "Train"
  },
  {
    "node_id": 835,
    "label": 2,
    "text": "S-CREAM -- Semi-automatic CREAtion of Metadata Abstract. Richly interlinked, machine-understandable data constitute the basis for the Semantic Web. We provide a framework, S-CREAM, that allows for creation of metadata and is trainable for a specific domain. Annotating web documents is one of the major techniques for creating metadata on the web. The implementation of S-CREAM, OntoMat supports now the semi-automatic annotation of web pages. This semi-automatic annotation is based on the information extraction component Amilcare. OntoMat extract with the help of Amilcare knowledge structure from web pages through the use of knowledge extraction rules. These rules are the result of a learningcycle based on already annotated pages. 1",
    "neighbors": [
      188,
      239,
      934,
      1106
    ],
    "mask": "Train"
  },
  {
    "node_id": 836,
    "label": 4,
    "text": "Heuristic Evaluation of Groupware Based on the Mechanics of Collaboration . Despite the increasing availability of groupware, most systems are  awkward and not widely used. While there are many reasons for this, a  significant problem is that groupware is difficult to evaluate. In particular, there  are no discount usability evaluation methodologies that can discover problems  specific to teamwork. In this paper, we describe how we adapted Nielsen's  heuristic evaluation methodology, designed originally for single user  applications, to help inspectors rapidly, cheaply effectively identify usability  problems within groupware systems. Specifically, we take the `mechanics of  collaboration' framework and restate it as heuristics for the purposes of  discovering problems in shared visual work surfaces for distance-separated  groups.  1.",
    "neighbors": [
      45,
      860
    ],
    "mask": "Train"
  },
  {
    "node_id": 837,
    "label": 1,
    "text": "An Evolutionary Approach to Case Adaptation . We present a case adaptation method that employs ideas from the field of genetic algorithms. Two types of adaptations, case combination and case mutation, are used to evolve variations on the contents of retrieved cases until a satisfactory solution is found for a new specified problem. A solution is satisfactory if it matches the specified requirements and does not violate any constraints imposed by the domain of applicability. We have implemented our ideas in a computational system called GENCAD, applied to the layout design of residences such that they conform to the principles of feng shui, the Chinese art of placement. This implementation allows us to evaluate the use of GA's for case adaptation in CBR. Experimental results show the role of representation and constraints. 1 Introduction  Many different methods have been proposed for performing the task of case adaptation in CBR. They have been surveyed in several publications, including [1], [2], and [3]. Different approaches ma...",
    "neighbors": [
      24
    ],
    "mask": "Train"
  },
  {
    "node_id": 838,
    "label": 4,
    "text": "A Survey and Taxonomy of Location Systems for Ubiquitous Computing Emerging mobile computing applications often need to know where  things are physically located. To meet this need, many di#erent location  systems and technologies have been developed. In this paper we present  a the basic techniques used for location-sensing, describe a taxonomy of  location system properties, present a survey of research and commercial  location systems that define the field, show how the taxonomy can be  used to evaluate location-sensing systems, and o#er suggestions for future  research. It is our hope that this paper is a useful reference for researchers  and location-aware application builders alike for understanding and evaluating  the many options in this domain.  1",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 839,
    "label": 0,
    "text": "Cooperation Mechanisms in a Multi-Agent Distributed Environment In the paper we present our work on design and analysis of agent cooperation in distributed systems. The work is not completed yet, so that some parts of it, especially the formal framework, should be viewed as a preliminary version. Multi-agent systems are represented by BDI-automata, i.e., asynchronous automata composed of non-deterministic agents equipped with mental attitudes like belief, desire, and intentions. These attitudes are acquired by the agents by executing so called mental actions. Behaviours of multi-agent systems are represented by prime event structures. The prime event structure when augmented with utility functions defined on the terminal nodes of the structure may be viewed as games in extensive form defined on local states rather than on global states as in the classical definition. The definition of knowledge, in our framework, captures the change of state due to action executions. A notion of local knowledge-based protocols is defined. A game theory method of backwards induction is applied in order to obtain a very natural definition of rationality in agent's behaviours. All the notions are exemplified using the running example. For this example we construct several cooperation mechanisms for the agents. A team formation mechanism is one of them.",
    "neighbors": [
      303,
      724,
      1156,
      1266
    ],
    "mask": "Validation"
  },
  {
    "node_id": 840,
    "label": 2,
    "text": "Localizing and Segmenting Text in Images and Videos Many images---especially those used for page design on web pages---as well as videos contain visible text. If these text occurrences could be detected, segmented, and recognized automatically, they would be a valuable source of high-level semantics for indexing and retrieval. In this paper, we propose a novel method for localizing and segmenting text in complex images and videos. Text lines are identified by using a complex-valued multilayer feed-forward network trained to detect text at a fixed scale and position. The network's output at all scales and positions is integrated into a single text-saliency map, serving as a starting point for candidate text lines. In the case of video, these candidate text lines are refined by exploiting the temporal redundancy of text in video. Localized text lines are then scaled to a fixed height of 100 pixels and segmented into a binary image with black characters on white background. For videos, temporal redundancy is exploited to improve segmentation performance. Input images and videos can be of any size due to a true multiresolution approach. Moreover, the system is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video, so that one text bitmap is created for all instances of that text line. Therefore, our text segmentation results can also be used for object -based video encoding such as that enabled by MPEG-4.",
    "neighbors": [
      550,
      605
    ],
    "mask": "Test"
  },
  {
    "node_id": 841,
    "label": 0,
    "text": "Social Role Awareness in Animated Agents This paper promotes social role awareness as a desirable capability of animated agents, that are by now strong affective reasoners, but otherwise often lack the social competence observed with humans. In particular, humans may easily adjust their behavior depending on their respective role in a socio-organizational setting, whereas their synthetic pendants tend to be driven mostly by attitudes, emotions, and personality. Our main contribution is the incorporation of `social filter programs' to mental models of animated agents. Those programs may qualify an agent's expression of its emotional state by the social context, thereby enhancing the agent's believability as a conversational partner or virtual teammate. Our implemented system is entirely webbased and demonstrates socially aware animated agents in an environment similar to Hayes-Roth's Cybercaf'e.  Keywords  believability, social agents, human-like qualities of synthetic agents, social dimension in communication, affective reaso...",
    "neighbors": [
      723,
      1102
    ],
    "mask": "Train"
  },
  {
    "node_id": 842,
    "label": 3,
    "text": "Eddies: Continuously Adaptive Query Processing In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.  In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry  during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates prom...",
    "neighbors": [
      658,
      876,
      992
    ],
    "mask": "Validation"
  },
  {
    "node_id": 843,
    "label": 3,
    "text": "Negation in Logic and Deductive Databases This thesis studies negation in logic and deductive databases. Among other things, two kinds of negation are discussed in detail: strong negation and nonmonotonic negation.  In the logic part, we have constructed a first-order logic CF  0  of strong negation with bounded quantifiers. The logic is based on constructive logics, in particular, Thomason's logic CF. However, unlike constructive logic, quantifiers in our system as in Thomason's are static rather than dynamic. For the logic CF  0  , the usual Kripke formal semantics is defined but based on situations instead of conventional possible worlds. A sound and complete axiomatic system of CF  0  is established based on the axiomatic systems of constructive logics with strong negation and Thomason's completeness proof techniques. CF  0  is proposed as the underlying logic for situation theory. Thus the connection between CF  0  and infon logic is briefly discussed.  In the database part, based on the study of some main existing semant...",
    "neighbors": [
      1109
    ],
    "mask": "Train"
  },
  {
    "node_id": 844,
    "label": 3,
    "text": "Automatic Generation of Warehouse Mediators Using an Ontology Engine Data warehouses created for dynamic scientific environments, such as genetics, face significant challenges to their long-term feasibility. One of the most significant of these is the high frequency of schema evolution resulting from both technological advances and scientific insight. Failure to quickly incorporate these modifications will quickly render the warehouse obsolete, yet each evolution requires significant effort to ensure the changes are correctly propagated. DataFoundry utilizes a mediated warehouse architecture with an ontology infrastructure to reduce the maintenance requirements of a warehouse. Among other things, the ontology is used as an information source for automatically generating mediators, the programs that transfer data between the data sources and the warehouse. The identification, definition, and representation of the metadata required to perform this task are the primary contributions of this work.  1 Introduction  The DataFoundry research project at LLNL's ...",
    "neighbors": [
      31
    ],
    "mask": "Train"
  },
  {
    "node_id": 845,
    "label": 2,
    "text": "DiscoWeb: Applying Link Analysis to Web Search How often does the search engine of your choice produce results that are less than satisfying, generating endless links to irrelevant pages even though those pages may contain the query keywords? How often are you given pages that tell you things you already know? While the search engines and related tools continue to make improvements in their information retrieval algorithms, for the most part they continue to ignore an essential part of the web \u2013 the links. We have found that link analysis can have significant contributions to web page retrieval from search engines, to web community discovery, and to the measurement of web page influence. It can help to rank results and find high-quality index/hub/link pages that contain links to the best sites on the topic of interest. Our work is based on research from IBM\u2019s CLEVER project [7, 4, 6], Stanford\u2019s Google [3], and the Web Archaeology research [2, 1] at Compaq\u2019s Systems Research Center. These research teams have demonstrated some of the contributions that link analysis can make in the web. In our work, we have attempted to generalize and improve upon these approaches. Just as in citation analysis of published works, the most influential documents on the web will have many other documents recommending (pointing to) them. This idea underlies all link analysis efforts, from the straightforward technique of counting the number of incoming edges to a page, to the deeper eigenvector analysis used in our work and in those projects mentioned above. It turns out that the identification of \u201chigh-quality \u201d web pages reduces to a sparse eigenvalue of the adjacency matrix",
    "neighbors": [
      216,
      496,
      774,
      1000,
      1017,
      1099
    ],
    "mask": "Train"
  },
  {
    "node_id": 846,
    "label": 1,
    "text": "Automatic Detection of Human Faces in Natural Scene Images by Use of a Skin Color Model and of Invariant Moments We use a skin color model based on the Mahalanobis metric and a shape analysis based on invariant moments to automatically detect and locate human faces in two-dimensional natural scene images. First, color segmentation of an input image is performed by thresholding in a perceptually plausible hue-saturation color space where the effects of the variability of human skin color and the dependency of chrominance on changes in illumination are reduced. We then group regions of the resulting binary image which have been classified as face candidates into clusters of connected pixels. Performing median filtering on the image and discarding the smallest remaining clusters ensures that only a small number of clusters will be used for further analysis. Fully translation-, scale- and in-plane rotation-invariant moments are calculated for each remaining cluster. Finally, in order to distinguish faces from distractors, a multilayer perceptron neural network is used with the invariant moments as the input vector. Supervised learning of the network is implemented with the backpropagation algorithm, at first for frontal views of faces. Preliminary results show the efficiency of the combination of color segmentation and of invariant moments in detecting faces with a large variety of poses and against relatively complex backgrounds. 1.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 847,
    "label": 0,
    "text": "Coordination Assistance for Mixed Human and Computational Agent Systems In many application areas (such as concurrent engineering, software development, hospital scheduling, manufacturing scheduling, and military planning), individuals are responsible for an agenda of tasks and face choices about the best way to locally handle each task, in what order to do tasks, and when to do them. Such decisions are often hard to make because of coordination problems: individual tasks are related to the tasks of others in complex ways, and there are many sources of uncertainty (no one has a complete view of the task structure at arbitrary levels of detail, the situation may be changing dynamically, and no one is entirely sure of the outcomes of all of their actions). The focus of this paper is the development of support tools for distributed, cooperative work by groups (collaborative teams) of human and computational agents. We will discuss the design of a set of distributed autonomous computer programs (\"agents\") that assist people in coordinating their activities by ...",
    "neighbors": [
      441,
      495,
      1119
    ],
    "mask": "Train"
  },
  {
    "node_id": 848,
    "label": 1,
    "text": "Grounded Speech Communication Language is grounded in sensory-motor experience. Grounding connects concepts to the physical world enabling humans to acquire and use words and sentences in context. Currently, machines which process text and spoken language are not grounded in human-like ways. Instead, semantic representations in machines are highly abstract and have meaning only when interpreted by humans. We are interested in developing computational systems which represent words, utterances, and underlying concepts in terms of sensory-motor experiences, leading to richer levels of understanding by machines. Inspired by theories of infant cognition, we present a computational model which learns from untranscribed multisensory input. Acquired words are represented in terms associations between acoustic and visual sensory experience. The system has been tested in a robotic embodiment which supports interactive language learning and understanding. Successful learning has also been demonstrated using infant -directed s...",
    "neighbors": [
      830
    ],
    "mask": "Train"
  },
  {
    "node_id": 849,
    "label": 4,
    "text": "Running the Web Backwards: Appliance Data Services Appliance\" digital devices such as handheld cameras, scanners, and microphones generate data that people want to put on Web pages. Unfortunately, numerous complex steps are required. Contrast this with Web output: handheld web browsers enjoy increasing infrastructural support such as user-transparent transformation proxies, allowing unmodified Web pages to be conveniently viewed on devices not originally designed for the task. We hypothesize that the utility of input appliances will be greatly increased if they too were \"infrastructure enabled.\" Appliance Data Services attempts to systematically describe the task domain of providing seamless and graceful interoperability between input appliances and the Web. We offer an application architecture and a validating prototype that we hope will \"open up the playing field\" and motivate further work. Our initial efforts have identified two main design challenges: dealing with device heterogeneity, and providing a \"no-futz\" out-of-the-box user experience for novices without sacrificing expressive power for advanced users. We address heterogeneity by isolating device and protocol heterogeneity considerations into a single extensible architectural component, allowing most of the application logic to deal exclusively with Web-friendly protocols and formats. We address the user interface issue in two ways: first, by specifying how to tag input with commands that specify how data is to be manipulated once injected into the infrastructure; second, by describing a late-binding mechanism for these command tags, which allows \"natural\" extensions of the device's UI for application selection and minimizes the amount of configuration required before end-users benefit from Appliance Data Services. Finally, we describe how to leverage existi...",
    "neighbors": [
      900,
      944
    ],
    "mask": "Test"
  },
  {
    "node_id": 850,
    "label": 2,
    "text": "Indexing and Retrieving Natural Language Using Ternary Expressions Traditional information retrieval systems based on the \"bag-of-words\" paradigm cannot completely capture the semantic content of documents. Yet it is impossible with current technology to build a practical information access system that fully analyzes and understands unrestricted natural language. However, if we avoid the most complex and processing-intensive natural language understanding techniques, we can construct a large-scale information access system which is capable of processing unrestricted text, largely understanding it, and answering natural language queries with high precision. We believe that ternary expressions are the most suitable representational structure for such a system; they are expressive enough for information retrieval purposes, yet amenable to rapid large-scale indexing.",
    "neighbors": [
      770
    ],
    "mask": "Test"
  },
  {
    "node_id": 851,
    "label": 2,
    "text": "An Integrated Ontology for the WWW . Knowledge-intensive processing of WWW information should  be founded on clear and uniform conceptualisation. An integrated ontology  covering different aspects of the WWW (documents, sites, network  addressing, HTML code) has been laid down, upon which a knowledge  base of the WWW domain is being built. This knowledge base should  support \"intelligent\" metasearch of the Web, in particular, postprocessing  of hit-lists returned by external search engines.  1 Introduction  During the last few years, the World-Wide Web has become one of the most widespread technologies of information presentation. It is thus not surprising that many Knowledge Engineering (KE) projects focus on it: some use HTML as a cheap, ready-made user-interface, other thrive to mine valuable information hidden inside existing WWW pages. A necessary prerequisite of mutual comprehensibility and knowledge reuse among different KE communities and projects dealing with the Web is a clear and unified conceptualisation, wh...",
    "neighbors": [
      347,
      410,
      1085
    ],
    "mask": "Train"
  },
  {
    "node_id": 852,
    "label": 0,
    "text": "Using Multi-Context Systems to Engineer Executable Agents In the area of agent-based computing there are many proposals  for specific system architectures, and a number of proposals for general  approaches to building agents. As yet, however, there are few  attempts to relate these together, and even fewer attempts to provide  methodologies which relate designs to architectures and then to executable  agents. This paper provides a first attempt to address this  shortcoming; we propose a general method of defining architectures  for logic-based agents which can be directly executed. Our approach is  based upon the use of multi-context systems and we illustrate its use  with an example architecture capable of argumentation-based negotiation.  1 Introduction  Agent-based computing is fast emerging as a new paradigm for engineering complex, distributed systems [13, 27]. An important aspect of this trend is the use of agent architectures as a means of delivering agent-based functionality (cf. work on agent programming languages [14, 23, 25]). In t...",
    "neighbors": [
      263,
      557,
      724
    ],
    "mask": "Train"
  },
  {
    "node_id": 853,
    "label": 3,
    "text": "DataBlitz Storage Manager: Main-Memory Database Performance for Critical Applications Introduction  General-purpose commercial disk-based database systems, though widely employed in practice, have failed to meet the performance requirements of applications requiring short, predictable response times, and extremely high throughput rates. Main memory is the only technology capable of these characteristics.  DataBlitz  1  is a main-memory storage manager product that supports the development of high-performance and fault-resilient applications requiring concurrent access to shared data. In DataBlitz, core algorithms for concurrency, recovery, index management and space management are optimized for the case that data is memory resident.  2 DataBlitz Architecture and Features  In this section, we give a high-level overview of the architecture and features of the DataBlitz Storage Manager product implemented at Bell Laboratories (for more details, see [1]).  Direct Access to Data. DataBlitz is designed to a",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 854,
    "label": 4,
    "text": "Http://www.playresearch.com/ this report. The following link takes you directly to a page with links to the core publications and other evaluation material in PDF format:",
    "neighbors": [
      204,
      206,
      747
    ],
    "mask": "Test"
  },
  {
    "node_id": 855,
    "label": 2,
    "text": "Learning Information Extraction Rules for Semi-structured and Free Text . A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically. WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences. Such semistructured text has largely been beyond the scope of previous systems. When used in conjunction with a syntactic analyzer and semantic tagging, WHISK can also handle extraction from free text such as news stories.  Keywords: natural language processing, information extraction, rule learning 1. Information extraction  As more and more text becomes available on-line, there is a growing need for systems that extract information automatically from text data. An information extraction (IE) sys...",
    "neighbors": [
      112,
      243,
      379,
      401,
      412,
      496,
      570,
      1090,
      1178,
      1232,
      1233
    ],
    "mask": "Train"
  },
  {
    "node_id": 856,
    "label": 1,
    "text": "Online Learning with Random Representations We consider the requirements of online learning---learning which must be done incrementally and in realtime, with the results of learning available soon after each new example is acquired. Despite the abundance of methods for learning from examples, there are few that can be used effectively for online learning, e.g., as components of reinforcement learning systems. Most of these few, including radial basis functions, CMACs, Kohonen 's self-organizing maps, and those developed in this paper, share the same structure. All expand the original input representation into a higher dimensional representation in an unsupervised way, and then map that representation to the final answer using a relatively simple supervised learner, such as a perceptron or LMS rule. Such structures learn very rapidly and reliably, but have been thought either to scale poorly or to require extensive domain knowledge. To the contrary, some researchers (Rosenblatt, 1962; Gallant & Smith, 1987; Kanerva, 1988; Prager ...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 857,
    "label": 1,
    "text": "Fast Reinforcement Learning through Eugenic Neuro-Evolution In this paper we introduce EuSANE, a novel reinforcement learning algorithm based on the SANE neuroevolution method. It uses a global search algorithm, the Eugenic Algorithm, to optimize the selection of neurons to the hidden layer of SANE networks. The performance of EuSANE is evaluated in the two-pole balancing benchmark task, showing that EuSANE is significantly stronger than other reinforcement learning methods to date in this task.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 858,
    "label": 4,
    "text": "Interacting with Spatially Augmented Reality Traditional user interfaces for off-the-desktop applications are designed to display the output on flat 2D surfaces while the input is with 2D or 3D devices. In this paper, we focus on projectorbased augmented reality applications. We describe a framework to easily incorporate the interaction on a continuum of display surfaces and input devices. We first create a 3D understanding of the relationship between the user, the projectors and the display surfaces. Then we use some new calibration and rendering techniques to create a simple procedure to effectively illuminate the surfaces. We describe various underlying techniques and discuss the results in the context of three different applications.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 859,
    "label": 2,
    "text": "Validating Access to External Information Sources in a Mediator Environment A mediator integrates existing information sources into a new application. In order to answer complex queries, the mediator splits them up into sub-queries which it sends to the information sources. Afterwards, it combines the replies to answer the original query. Since the information sources are usually external, autonomous systems, the access to them can sometimes be erroneous, most notably when the information source is changed. This results in an incorrect behaviour of the whole system. The question that this paper addresses, is: how to check whether or not the access was correct? The paper introduces a notational framework for the general information access validation problem, describes the typical errors that can occur in a mediator environment, and proposes several validation mechanisms. It is also investigated how the validation functionality can be integrated into the mediator architecture, and what the most important quality measures of a validation method are. Moreover, the practical usability of the presented approaches is demonstrated on a real-world application using Web-based information sources. Several measurements are performed to compare the presented methods with previous work in the field.",
    "neighbors": [
      169,
      185,
      243,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 860,
    "label": 4,
    "text": "A Comparison of Usage Evaluation and Inspection Methods for Assessing Groupware Usability Many researchers believe that groupware can only be evaluated by studying real collaborators in their real contexts, a process that tends to be expensive and timeconsuming. Others believe that it is more practical to evaluate groupware through usability inspection methods. Deciding between these two approaches is difficult, because it is unclear how they compare in a real evaluation situation. To address this problem, we carried out a dual evaluation of a groupware system, with one evaluation applying userbased techniques, and the other using inspection methods. We compared the results from the two evaluations and concluded that, while the two methods have their own strengths, weaknesses, and trade-offs, they are complementary. Because the two methods found overlapping problems, we expect that they can be used in tandem to good effect, e.g., applying the discount method prior to a field study, with the expectation that the system deployed in the more expensive field study has a better chance of doing well because some pertinent usability problems will have already been addressed.  Keywords  Evaluation, groupware usability, inspection evaluation techniques, usage evaluation techniques.",
    "neighbors": [
      45,
      836
    ],
    "mask": "Train"
  },
  {
    "node_id": 861,
    "label": 3,
    "text": "Relationlog: A Typed Extension to Datalog with Sets and Tuples This paper presents a novel logic programming based language for nested relational and complex value models called Relationlog. It stands in the same relationship to the nested relational and complex value models as Datalog stands to the relational model. The main novelty of the language is the introduction of powerful mechanisms, namely, partial and complete set terms, for representing and manipulating both partial and complete information on nested sets, tuples and relations. They generalize the set grouping and set enumeration mechanisms of LDL and allow the user to directly encode the open and closed world assumptions on nested sets, tuples, and relations. They allow direct inference and access to deeply embedded values in a complex value relation as if the relation is normalized, which greatly increases the ease of use of the language. As a result, the extended relational algebra operations can be represented in Relationlog directly, and more importantly, recursively in a way similar to Datalog. Like Datalog, Relationlog has a well-defined Herbrand model-theoretic semantics, which captures the intended semantics of nested sets, tuples and relations, and also a well-defined proof-theoretic semantics which coincides with its model-theoretic semantics.",
    "neighbors": [
      192,
      349,
      501,
      1053,
      1061
    ],
    "mask": "Test"
  },
  {
    "node_id": 862,
    "label": 1,
    "text": "The Evolution of the Soar Cognitive Architecture The origins of the Soar architecture can be traced back to the seminal research of Allen Newell and Herbert Simon on symbol systems, heuristic search, goals, problem spaces, and production systems. Since its official inception in 1982, Soar has evolved through six major releases, as both an AI architecture and as the basis for a unified theory of cognition. This paper traces this evolutionary path, starting with Soar's intellectual roots, and then proceeding through the stages defined by the six major system releases. Each stage is characterized with respect to a hierarchy of four levels of analysis: the knowledge level, the problem space level, the symbolic architecture level, and the implementation level.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 863,
    "label": 3,
    "text": "Improving Min/Max Aggregation over Spatial Objects We examine the problem of computing MIN/MAX aggregate queries over a collection of spatial objects. Each spatial object is associated with a weight (value), for example, the average temperature or rainfall over the area covered by the object. Given a query rectangle, the MIN/MAX problem computes the minimum/maximum weight among all objects intersecting the query rectangle. Traditionally such queries have been performed as range search queries. Assuming that the objects are indexed by a spatial access method, the MIN/MAX is computed as objects are retrieved. This requires effort proportional to the number of objects intersecting the query interval, which may be large. A better approach is to maintain aggregate information among the index nodes of the spatial access method; then various index paths can be eliminated during the range search. In this paper we propose four optimizations that further improve the performance of MIN/MAX queries. Our experiments show that the proposed optimizations offer drastic performance improvement over previous approaches. Moreover, as a by-product of this work we present an optimized version of the MSB-tree, an index that has been proposed for the MIN/MAX computation over 1-dimensional interval objects.",
    "neighbors": [
      795,
      992,
      1132
    ],
    "mask": "Train"
  },
  {
    "node_id": 864,
    "label": 1,
    "text": "Content-Based Book Recommending Using Learning for Text Categorization Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes and dislikes. Most existing recommender systems use collaborative filtering methods that base recommendations on other users' preferences. By contrast, content-based methods use information about an item itself to make suggestions. This approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations. We describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization. Initial experimental results demonstrate that this approach can produce accurate recommendations.  KEYWORDS: Recommender systems, information filtering,  machine learning, text categorization  INTRODUCTION  There is a growing interest in recommender systems that suggest music, films, books, and othe...",
    "neighbors": [
      643,
      818,
      865,
      937,
      1068
    ],
    "mask": "Test"
  },
  {
    "node_id": 865,
    "label": 1,
    "text": "Learning to Classify Text from Labeled and Unlabeled Documents In many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap. This paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents. We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function. We then introduce an algorithm for learning from labeled and unlabeled text based on the combination of Expectation-Maximization with a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents; it then trains a new classifier using the labels for all the documents, and iterates to convergence. Experimental results, obtained using text from three different realworld tasks, show that the use of unlabeled data reduces classification error by up to 33%.",
    "neighbors": [
      347,
      407,
      643,
      682,
      767,
      864,
      1001,
      1090,
      1133
    ],
    "mask": "Validation"
  },
  {
    "node_id": 866,
    "label": 1,
    "text": "Non-Standard Crossover for a Standard Representation -- Commonality-Based Feature Subset Selection The Commonality-Based Crossover Framework has been presented as a general model for designing problem specific operators. Following this model, the Common Features/Random Sample Climbing operator has been developed for feature subset selection--a binary string optimization problem. Although this problem should be an ideal application for genetic algorithms with standard crossover operators, experiments show that the new operator can find better feature subsets for classifier training. 1 INTRODUCTION  A classification system is used to predict the decision class of an object based on its features. When training a classifier, it is beneficial to use only the features relevant to prediction accuracy, and to ignore the irrelevant features [Koh95]. The benefit arises from an increase in the \"signalto -noise ratio\" of the data, and a reduction in the time required to train the classifier. Thus, the objective of feature subset selection is to identify the (most) relevant features. Feature sub...",
    "neighbors": [
      62
    ],
    "mask": "Train"
  },
  {
    "node_id": 867,
    "label": 2,
    "text": "Web Interaction and the Navigation Problem in Hypertext written for Encyclopedia of Microcomputers The web has become a ubiquitous tool, used in day-to-day work, to find information  and conduct business, and it is revolutionising the role and availability of information. One  of the problems encountered in web interaction, which is still unsolved, is the navigation  problem, whereby users can \"get lost in hyperspace\", meaning that when following a  sequence of links, i.e. a trail of information, users tend to become disoriented in terms of  the goal of their original query and the relevance to the query of the information they are  currently browsing.  Herein we build statistical foundations for tackling the navigation problem based on a  formal model of the web in terms of a probabilistic automaton, which can also be viewed  as a finite ergodic Markov chain. In our model of the web the probabilities attached  to state transitions have two interpretations, namely, they can denote the proportion of  times a user followed a link, and alternatively they can denote the expected utility of  following a link. Using this approach we have developed two techniques for constructing a  web view based on the two interpretations of the probabilities of links, where a web view  is a collection of relevant trails. The first method we describe is concerned with finding  frequent user behaviour patterns. A collection of trails is taken as input and an ergodic  Markov chain is produced as output with the probabilities of transitions corresponding  to the frequency the user traversed the associated links. The second method we describe  is a reinforcement learning algorithm that attaches higher probabilities to links whose  expected trail relevance is higher. The user's home page and a query are taken as input  and an ergodic Markov chain is produced as output with the probabilities of...",
    "neighbors": [
      216,
      536,
      1017,
      1189
    ],
    "mask": "Train"
  },
  {
    "node_id": 868,
    "label": 3,
    "text": "Guaranteeing No Interaction between Functional Dependencies and Tree-Like Inclusion Dependencies Functional dependencies (FDs) and inclusion dependencies (INDs) are the most fundamental integrity constraints that arise in practice in relational databases. A given set of FDs does not interact with a given set of INDs if logical implication of any FD can be determined solely by the given set of FDs, and logical implication of any IND can be determined solely by the given set of INDs. The set of tree-like INDs constitutes a useful subclass of INDs whose implication problem is polynomial time decidable. We exhibit a necessary and sufficient condition for a set of FDs and tree-like INDs not to interact; this condition can be tested in polynomial time. 1 Introduction  The implication problem for FDs and INDs is the problem of deciding for a given set \\Sigma of FDs and INDs whether \\Sigma logically implies oe, where oe is an FD or an IND. The implication problem is central in data dependency theory and is also utilised in the process of database design, since it can be used to test wheth...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 869,
    "label": 4,
    "text": "Supporting Group Collaboration with Inter-Personal Awareness Devices . An Inter-Personal Awareness Device, or IPAD, is a hand-held or  wearable device designed to support awareness and collaboration between  people who are in the physical vicinity of each other. An IPAD is designed to  supply constant awareness information to users in any location without relying  on an underlying infrastructure. We have constructed one such device, the  Hummingbird, which gives members of a group continuous aural and visual  indication when other group members are close. We have used the  Hummingbirds in several different situations to explore how they affect group  awareness. These experiences indicated that the Hummingbird increased  awareness between group members, and that it could complement other forms of  communication such as phone and e-mail. In particular, we found the  Hummingbird to be useful when a group of people were in an unfamiliar  location, for instance during a trip, where no other communication support was  available. We argue that IPADs such as th...",
    "neighbors": [
      206,
      520
    ],
    "mask": "Test"
  },
  {
    "node_id": 870,
    "label": 3,
    "text": "Temporal Patterns (TRAPs) in ASR of Noisy Speech In this paper we study a new approach to processing temporal information for automatic speech recognition (ASR). Specifically, we study the use of rather longtime TempoRAl Patterns (TRAPs) of spectral energies in place of the conventional spectral patterns for ASR. The proposed Neural TRAPs are found to yield significant amount of complementary information to that of the conventional spectral feature based ASR system. A combination of these two ASR systems is shown to result in improved robustness to several types of additive and convolutive environmental degradations.  1. INTRODUCTION 1.1. Spectral features  Spectrum-based techniques form the basis of most feature extraction methods in current ASR. A drawback of the spectral features is that they are quite sensitive to changes in the communication environment e.g. characteristics of different communication channels or environmental noise. Subsequently, recognizers based on spectral features exhibit rapid degradation in performance in ...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 871,
    "label": 5,
    "text": "Remote Agent: To Boldly Go Where No AI System Has Gone Before Renewed motives for space exploration have inspired NASA to work toward the goal of establishing a virtual presence in space, through heterogeneous effets of robotic explorers. Information technology, and Artificial Intelligence in particular, will play a central role in this endeavor by endowing these explorers with a form of computational intelligence that we call remote agents. In this paper we describe the Remote Agent, a specific autonomous agent architecture based on the principles of model-based programming, on-board deduction and search, and goal-directed closed-loop commanding, that takes a significant step toward enabling this future. This architecture addresses the unique characteristics of the spacecraft domain that require highly reliable autonomous operations over long periods of time with tight deadlines, resource constraints, and concurrent activity among tightly coupled subsystems. The Remote Agent integrates constraint-based temporal planning and scheduling, robust multi-threaded execution, and model-based mode identification and reconfiguration. The demonstration of the integrated system as an on-board controller for Deep Space One, NASA's rst New Millennium mission, is scheduled for a period of a week in late 1998. The development of the Remote Agent also provided the opportunity to reassess some of AI's conventional wisdom about the challenges of implementing embedded systems, tractable reasoning, and knowledge representation. We discuss these issues, and our often contrary experiences, throughout the paper.",
    "neighbors": [
      1258
    ],
    "mask": "Train"
  },
  {
    "node_id": 872,
    "label": 1,
    "text": "Constructive Neural Network Learning Algorithms for Multi-Category Pattern Classification Constructive learning algorithms offer an attractive approach for the incremental construction of near-minimal neural-network architectures for pattern classification. They help overcome the need for ad hoc and often inappropriate choices of network topology in algorithms that search for suitable weights in a priori fixed network architectures. Several such algorithms are proposed in the literature and shown to converge to zero classification errors (under certain assumptions) on tasks that involve learning a binary to binary mapping (i.e., classification problems involving binary-valued input attributes and two output categories). We present two constructive learning algorithms MPyramid-real and MTiling-real that extend the pyramid and tiling algorithms, respectively, for learning real to M-ary mappings (i.e., classification problems involving real-valued input attributes and multiple output classes). We prove the convergence of these algorithms and empirically demonstrate their applicability to practical pattern classification problems. Additionally, we show how the incorporation of a local pruning step can eliminate several redundant neurons from MTiling-real networks.",
    "neighbors": [
      197
    ],
    "mask": "Test"
  },
  {
    "node_id": 873,
    "label": 0,
    "text": "Providing Applications with Mobile Agent Technology Over the last couple of years we have been working on the development of mobile agents systems and its application to the areas of telecommunications and network management. This work path produced positive results: a competitive mobile agent platform was built, the run-time benefits of mobile agents were proved, and our industrial partners have developed practical applications that are being integrated into commercial products. However,",
    "neighbors": [
      273
    ],
    "mask": "Validation"
  },
  {
    "node_id": 874,
    "label": 0,
    "text": "A Language for Exchanging Agent UML Protocol Diagrams For several years, interaction protocol designers have a new formalism which takes into account multiagent system features: autonomy, cooperation, etc. This formalism is called Agent UML [6]. For the moment, designers can describe protocols with the Agent UML protocol diagrams but they do not have a textual language in order to exchange protocols or to check properties on them. The aim of this paper is to provide such a language. This language is called AXF (Agent UML eXchange Format) and is structured as an XML file. This paper presents the syntax of this language and applies AXF to the example of the English Auction Protocol. This paper is published as the technical report ULCS-02-009 from the department of computer science, University of Liverpool.",
    "neighbors": [
      264,
      619
    ],
    "mask": "Train"
  },
  {
    "node_id": 875,
    "label": 2,
    "text": "Clipping and Analyzing News Using Machine Learning Techniques Generating press clippings for companies manually requires a considerable amount of resources. We describe a system that monitors online newspapers and discussion boards automatically. The system extracts, classifies and analyzes messages and generates press clippings automatically, taking the specific needs of client companies into account. Key components of the system are a spider, an information extraction engine, a text classifier based on the Support Vector Machine that categorizes messages by subject, and a second classifier that analyzes which emotional state the author of a newsgroup posting was likely to be in. By analyzing large amount of messages, the system can summarize the main issues that are being reported on for given business sectors, and can summarize the emotional attitude of customers and shareholders towards companies.",
    "neighbors": [
      201,
      279,
      379,
      956,
      1024,
      1122
    ],
    "mask": "Validation"
  },
  {
    "node_id": 876,
    "label": 3,
    "text": "Query Optimization in the Presence of Limited Access Patterns 1 Introduction The goal of a query optimizer of a database system is to translate a declarative query expressed on a logical schema into an imperative query execution plan that accesses the physical storage of the data, and applies a sequence of relational operators. In building query execution plans, traditional relational query optimizers try to find the most efficient method for accessing the necessary data. When possible, a query optimizer will use auxiliary data structures such as an index on a file in order to efficiently retrieve a certain set of tuples in a relation. However, when such structures do not exist or are not useful for the given query, the alternative of scanning the entire relation always exists. The existence of the fall back option to perform a complete scan is an important assumption in traditional query optimization. Several recent query processing applications have the common characteristic that it is not always possible to perform complete scans on the data. Instead, the query optimization problem is complicated by the fact that there are only limited access patterns to the data. One such",
    "neighbors": [
      419,
      420,
      842,
      879,
      1070,
      1219
    ],
    "mask": "Test"
  },
  {
    "node_id": 877,
    "label": 5,
    "text": "Strongly Typed Inductive Concept Learning . In this paper we argue that the use of a language with a type system, together with higher-order facilities and functions, provides a suitable basis for knowledge representation in inductive concept learning and, in particular, illuminates the relationship between attribute-value learning and inductive logic programming (ILP). Individuals are represented by closed terms: tuples of constants in the case of attribute-value learning; arbitrarily complex terms in the case of ILP. To illustrate the point, we take some learning tasks from the machine learning and ILP literature and represent them in Escher, a typed, higher-order, functional logic programming language being developed at the University of Bristol. We argue that the use of a type system provides better ways to discard meaningless hypotheses on syntactic grounds and encompasses many ad hoc approaches to declarative bias. 1. Motivation and scope  Inductive concept learning consists of finding mappings of individuals (or objects...",
    "neighbors": [
      1004
    ],
    "mask": "Validation"
  },
  {
    "node_id": 878,
    "label": 4,
    "text": "Reinventing the Familiar: Exploring an Augmented Reality Design Space for Air Traffic Control This paper describes our exploration of a design space for an augmented reality prototype. We began by observing air traffic controllers and their interactions with paper flight strips. We then worked with a multi-disciplinary team of researchers and controllers over a period of a year to brainstorm and prototype ideas for enhancing paper flight strips. We argue that augmented reality is more promising (and simpler to implement) than the current strategies that seek to replace flight strips with keyboard/monitor interfaces. We also argue that an exploration of the design space, with active participation from the controllers, is essential not only for designing particular artifacts, but also for understanding the strengths and limitations of augmented reality in general.  Keywords: Augmented Reality, Design Space, Interactive Paper, Participatory Design, Video Prototyping  INTRODUCTION  Air traffic control is a complex, collaborative activity, with well-established and successful work p...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 879,
    "label": 3,
    "text": "Efficient Data and Program Integration Using Binding Patterns : In this work, we investigate data and program integration in a fully distributed peer-to-peer mediation architecture. The challenge in making such a system succeed at a large scale is twofold. First, sharing a resource should be easy; therefore, we need a simple concept for modeling resources. Second, we need an ecient architecture for distributed query execution, capable of handling well costly computations and large data transfers. To model heterogeneous resources, we propose using the unied abstraction of table with binding patterns, simple yet powerful enough to capture data and programs. To exploit a resource with restricted binding patterns, we propose an ecient BindJoin operator, following the classical iterator model, in which we build optimization techniques for minimizing large data transfers and costly computations, and maximizing parallelism. Furthermore, our BindJoin operator can be tuned to deliver most of its output in the early stages of the execution, which is an important asset in a system meant for human interaction. Our preliminary experimental evaluation validates the proposed BindJoin algorithms, and shows they can provide good performance in queries involving distributed data and expensive programs.  Key-words: data and program integration, distributed query processing, binding patterns   INRIA, Caravel project. Contact: Ioana.Manolescu@inria.fr  y  PRISM laboratory, University of Versailles and INRIA, Caravel project. Contact: Luc.Bouganim@prism.uvsq.fr  z  INRIA, Caravel project. Contact: Francoise.Fabret@inria.fr  x  INRIA, Caravel project. Contact: Eric.Simon@inria.fr  Intgration ecace de donnes et de programmes  utilisants des patterns d'accs  Rsum : Dans ce rapport, nous tudions l'intgration de donnes et de programmes dans une archite...",
    "neighbors": [
      218,
      419,
      876
    ],
    "mask": "Train"
  },
  {
    "node_id": 880,
    "label": 0,
    "text": "A Framework For Designing, Modeling and Analyzing Agent Based Software Systems The agent paradigm is gaining popularity because it brings intelligence, reasoning and autonomy to software systems. Agents are being used in an increasingly wide variety of applications from simple email filter programs to complex mission control and safety systems. However there appears to be very little work in defining practical software architecture, modeling and analysis tools that can be used by software engineers. This should be contrasted with object-oriented paradigm that is supported by models such as UML and CASE tools that aid during the analysis, design and implementation phases of object-oriented software systems. In our research we are developing a framework and extensions to UML to address this need. Our approach is rooted in the BDI formalism, but stresses the practical software design methods instead of reasoning about agents. In this paper we describe our preliminary ideas  Index Terms: Agent-Oriented programming, ObjectOriented programming, BDI, UML  1.",
    "neighbors": [
      269,
      964
    ],
    "mask": "Train"
  },
  {
    "node_id": 881,
    "label": 3,
    "text": "Querying the Uncertain Position of Moving Objects In this paper we propose a data model for representing moving objects with uncertain positions in database systems. It is called the Moving Objects Spatio-Temporal (MOST) data model. We also propose Future Temporal Logic (FTL) as the query language for the MOST model, and devise an algorithm for processing FTL queries in MOST. 1 Introduction  Existing database management systems (DBMS's) are not well equipped to handle continuously changing data, such as the position of moving objects. The reason for this is that in databases, data is assumed to be constant unless it is explicitly modified. For example, if the salary field is 30K, then this salary is assumed to hold (i.e. 30K is returned in response to queries) until explicitly updated. Thus, in order to represent moving objects (e.g. cars) in a database, and answer queries about their position (e.g., How far is the car with license plate RWW860 from the nearest hospital?) the car's position has to be continuously updated. This is unsa...",
    "neighbors": [
      458,
      1158
    ],
    "mask": "Train"
  },
  {
    "node_id": 882,
    "label": 1,
    "text": "Compressive Computation in Analog VLSI Motion Sensors . We introduce several different focal plane analog VLSI motion sensors developed in the past. We show how their pixel-parallel architecture can be used to extract low-dimensional information from a higher dimensional data set. As an example we present an algorithm and corresponding experiments to compute the focus of expansion, focus of contraction and the axis of rotation from natural visual input. A fully integrated system for real-time computation of these quantities is proposed as well. In computer simulations it is shown that the direction of motion vector field is best suited to perform the algorithm even at high noise levels. 1 Analog VLSI Motion Sensors  In the past the computer vision communityhas invested much effort into developing motion detection algorithms; for a critical review see [BFB94]. Implementing these algorithms in real-time systems proved challenging for computational reasons. Additionally it has been realized that a motion vector field is useful mainly as star...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 883,
    "label": 4,
    "text": "Collaborative Representations: Supporting Face to Face and Online Knowledge-building Discourse The present widespread interest in the use of electronic media for presents an unprecedented opportunity for leveraging the computational medium's strengths for learning. However, existing software tools provide only primitive support for online knowledge-building discourse. Further work is needed in supporting coordinated use of disciplinary representations, discourse representations, and knowledge representations. This paper introduces the concept of representational guidance for discourse along with results of an initial study of this phenomenon in face to face situations. The paper then considers the requirements for supporting asynchronous online knowledge-building discourse, finding existing computer mediated communication tools to be particularly deficient in supporting artifact-centered discourse. A solution is proposed that coordinates discourse representations with disciplinary and knowledge representations.  1. Introduction  There is a great deal of interest in the use of el...",
    "neighbors": [
      736
    ],
    "mask": "Train"
  },
  {
    "node_id": 884,
    "label": 0,
    "text": "HEMASL: A Flexible Language to Specify Heterogeneous Agents In the realization of agent-based applications the developer generally needs to use heterogeneous agent architectures, so that each application component can optimally perform its task. Languages that easily model the heterogeneity of agents' architectures are very useful in the early stages of the application development. This paper presents HEMASL, a simple meta-language used to specify heterogeneous agent architectures, and sketches how HEMASL should be implemented in an object-oriented commercial programming language as Java. Moreover, the paper briefly discusses the benefits of adding HEMASL to CaseLP, a LP-based specification and prototyping environment for multi-agent systems, in order to enhance its flexibility and usability.  1. Introduction  Intelligent agents and multi-agent systems (MASs) are more and more recognized as the \"new\" modeling techniques to be used to engineer complex and distributed software applications [12]. Agent-based software engineering is concerned with ...",
    "neighbors": [
      106,
      521,
      964,
      1222
    ],
    "mask": "Train"
  },
  {
    "node_id": 885,
    "label": 0,
    "text": "Prometheus: A Methodology for Developing Intelligent Agents Abstract. As agents gain acceptance as a technology there is a growing need for practical methods for developing agent applications. This paper presents the Prometheus methodology, which has been developed over several years in collaboration with Agent Oriented Software. The methodology has been taught at industry workshops and university courses. It has proven effective in assisting developers to design, document, and build agent systems. Prometheus differs from existing methodologies in that it is a detailed and complete (start to end) methodology for developing intelligent agents which has evolved out of industrial and pedagogical experience. This paper describes the process and the products of the methodology illustrated by a running example. 1",
    "neighbors": [
      472,
      573,
      957
    ],
    "mask": "Train"
  },
  {
    "node_id": 886,
    "label": 4,
    "text": "Situated Documentaries: Embedding Multimedia Presentations in the Real World We describe an experimental wearable augmented reality system that enables users to experience hypermedia presentations that are integrated with the actual outdoor locations to which they are are relevant. Our mobile prototype uses a tracked see-through head-worn display to overlay 3D graphics, imagery, and sound on top of the real world, and presents additional, coordinated material on a hand-held pen computer. We have used these facilities to create several  situated documentaries that tell the stories of events that took place on our campus. We describe the software and hardware that underly our prototype system and explain the user interface that we have developed for it.",
    "neighbors": [
      75,
      215,
      1036,
      1043
    ],
    "mask": "Train"
  },
  {
    "node_id": 887,
    "label": 1,
    "text": "An Open Platform for Reconfigurable Control bility.  . Openness: Reconfigurability and component interchangeability require software architectures that are flexible and that support tools and algorithms from a variety of sources and domains. This requires a shift away from traditional control system implementation, which tends to be practiced with a particular apJune 2001 IEEE Control Systems Magazine 49  By Linda Wills, Suresh Kannan, Sam Sander, Murat Guler,  Bonnie Heck, J.V.R. Prasad, Daniel Schrage, and George Vachtsevanos Wills (linda.wills@ece.gatech.edu), Sander, Guler, Heck, and Vachtsevanos are with the School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332-0250, U.S.A. Kannan, Prasad, and Schrage are with the School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA 30332-0250, U.S.A.   1998 Corbis Corp. 0272-1708/01/$10.002001IEEE  plication in mind and which makes rigid,",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 888,
    "label": 4,
    "text": "Workload of a Media-Enhanced Classroom Server We charaterize a workload of media-enhanced classrooms. Such classrooms include equipment for presenting multimedia streams and for capturing streams of information (audio, video and notes) during a lecture. We present detailed quantitative performance measurements of one media-enhanced classroom system, Classroom 2000. We characterize the workload from the point of view of a server that supports multiple classrooms. The workload includes server bandwidth, network bandwidth and server storage requirements. We identify patterns in user behavior, and demonstrate how the number of simultaneous study sessions varies with time of day and with the proximity of a specific date to exams. 1 Workload of a Media-Enhanced Classroom Server 1 Introduction The ways we teach and learn will be dramatically affected by current, unprecedented rates of improvement in computational power and network bandwidth, as well as the development of innovative user interfaces and virtual environments. Alre...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 889,
    "label": 3,
    "text": "Version Management of XML Documents The problem of ensuring efficient storage and fast retrieval  for multi-version structured documents is important because of the recent  popularity of XML documents and semistructured information on  the web. Traditional document version control systems, e.g. RCS, which  model documents as a sequence of lines of text and use the shortest edit  script to represent version differences, can be inefficient and they do not  preserve the logical structure of the original document. Therefore, we propose  a new approach where the structure of the documents is preserved  intact, and their sub-objects are timestamped hierarchically for efficient  reconstruction of current and past versions. Our technique, called the  Usefulness Based Copy Control (UBCC), is geared towards efficient version  reconstruction while using small storage overhead. Our analysis and  experiments illustrate the effectiveness of the overall approach to version  control for structured documents. Moreover UBCC can easily support  multiple concurrent versions as well as partial document retrieval.  1",
    "neighbors": [
      912,
      1025
    ],
    "mask": "Train"
  },
  {
    "node_id": 890,
    "label": 1,
    "text": "Process-Oriented Estimation of Generalization Error Methods to avoid overfitting fall into two broad  categories: data-oriented (using separate data  for validation) and representation-oriented (penalizing  complexity in the model). Both have  limitations that are hard to overcome. We  argue that fully adequate model evaluation is  only possible if the search process by which  models are obtained is also taken into account.  To this end, we recently proposed a method  for process-oriented evaluation (POE), and successfully  applied it to rule induction [ Domingos,  1998b ] . However, for the sake of simplicity this  treatment made a number of rather artificial assumptions.  In this paper the assumptions are  removed, and a simple formula for error estimation  is obtained. Empirical trials show the  new, better-founded form of POE to be as accurate  as the previous one, while further reducing  theory sizes.  1 Introduction  Overfitting avoidance is a central problem in machine learning. If a learner is su#ciently powerful, whatever repre...",
    "neighbors": [
      826,
      1220
    ],
    "mask": "Train"
  },
  {
    "node_id": 891,
    "label": 2,
    "text": "Applying Co-Training methods to Statistical Parsing We propose a novel Co-Training method for statistical parsing. The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text. The algorithm iteratively labels the entire data set with parse trees. Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data. 1",
    "neighbors": [
      391,
      601,
      609,
      1153,
      1240
    ],
    "mask": "Train"
  },
  {
    "node_id": 892,
    "label": 2,
    "text": "Using Web Structure for Classifying and Describing Web Pages The structure of the web is increasingly being used to improve organization, search, and analysis  of information on the web. For example, Google uses the text in citing documents (documents that  link to the target document) for search. We analyze the relative utility of document text, and the text  in citing documents near the citation, for classification and description. Results show that the text in  citing documents, when available, often has greater discriminative and descriptive power than the text  in the target document itself. The combination of evidence from a document and citing documents  can improve on either information source alone. Moreover, by ranking words and phrases in the citing  documents according to expected entropy loss, we are able to accurately name clusters of web pages,  even with very few positive examples. Our results confirm, quantify, and extend previous research using  web structure in these areas, introducing new methods for classification and description of pages.",
    "neighbors": [
      133,
      595,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 893,
    "label": 2,
    "text": "Co-clustering documents and words using Bipartite Spectral Graph Partitioning Both document clustering and word clustering are important and well-studied problems. By using the vector space model, a document collection may be represented as a word-document matrix. In this paper, we present the novel idea of modeling the document collection as a bipartite graph between documents and words. Using this model, we pose the clustering probliem as a graph partitioning problem and give a new spectral algorithm that simultaneously yields a clustering of documents and words. This co-clustrering algorithm uses the second left and right singular vectors of an appropriately scaled word-document matrix to yield good bipartitionings. In fact, it can be shown that these singular vectors give a real relaxation to the optimal solution of the graph bipartitioning problem. We present several experimental results to verify that the resulting co-clustering algoirhm works well in practice and is robust in the presence of noise.",
    "neighbors": [
      216,
      410,
      545,
      947,
      1049
    ],
    "mask": "Validation"
  },
  {
    "node_id": 894,
    "label": 3,
    "text": "Exploiting Planned Disconnections in Mobile Environments We present the notion of a distributed database made up entirely of mobile components. Since disconnections will be frequent in such an environment, we develop a disconnection and reconnection procedure to allow normal processing on the connected components. We briefly discuss a protocol based on epidemic communication to support such a system while ensuring one-copy serializability.  1 Introduction  Mobile computers and wireless networks are now being integrated into a variety of enterprises for different applications. The prevailing mode of operation with occasional disconnection by a single user will rapidly evolve into a situation where many if not all users are disconnecting and reconnecting in networks that are created in an ad hoc manner, e.g., a wireless network in a meeting room. This will result in mobile computers being integrated as first class entities in distributed information systems. Such mobile computers will inevitably contain data and information that will need to b...",
    "neighbors": [
      575
    ],
    "mask": "Validation"
  },
  {
    "node_id": 895,
    "label": 4,
    "text": "Interaction Techniques For Common Tasks In Immersive Virtual Environments - Design, Evaluation, And Application 13.44> . Drew Kessler for help with the SVE toolkit . The Virtual Environments group at Georgia Tech . The numerous experimental subjects who volunteered their time . Dawn Bowman iv TABLE OF CONTENTS Introduction ..................................................................... ................. 1 1.1 Motivation ..................................................................... ...............1 1.2 Definitions.......................................................... ..........................4 1.3 Problem Statement............................................................ ...............6 1.4 Scope of the Research............................................................. ..........7 1.5 Hypotheses........................................................... ........................8 1.6 Contributions........................................................ .....",
    "neighbors": [
      756
    ],
    "mask": "Test"
  },
  {
    "node_id": 896,
    "label": 2,
    "text": "Twenty-One at TREC-7: Ad-hoc and Cross-language track This paper describes the official runs of the Twenty-One group for TREC-7. The Twenty-One group participated in the ad-hoc and the cross-language track and made the following accomplishments: We developed a new weighting algorithm, which outperforms the popular Cornell version of BM25 on the ad-hoc collection. For the CLIR task we developed a fuzzy matching algorithm to recover from missing translations and spelling variants of proper names. Also for CLIR we investigated translation strategies that make extensive use of information from our dictionaries by identifying preferred translations, main translations and synonym translations, by defining weights of possible translations and by experimenting with probabilistic boolean matching strategies. 1 Introduction  Twenty-One is a 2 MECU project with 12 partners funded by the EU Telematics programme, sector Information Engineering. The project subtitle is \"Development of a Multimedia Information Transaction and Dissemination Tool\". Twenty...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 897,
    "label": 2,
    "text": "SavvySearch: A Meta-Search Engine that Learns which Search Engines to Query Search engines are among the most successful applications on the Web today. So many search engines have been created that it is difficult for users to know where they are, how to use them and what topics they best address. Meta-search engines reduce the user burden by dispatching queries to multiple search engines in parallel. The SavvySearch meta-search engine is designed to efficiently query other search engines by carefully selecting those search engines likely to return useful results and by responding to fluctuating load demands on the Web. SavvySearch learns to identify which search engines are most appropriate for particular queries, reasons about resource demands and represents an iterative parallel search strategy as a simple plan. 1 The Application: Meta-Search on the Web  Companies, institutions and individuals must have a presence on the Web; each are vying for the attention of millions of people. Not too surprisingly then, the most successful applications on the Web to dat...",
    "neighbors": [
      271,
      447,
      477,
      595,
      612,
      696,
      792,
      931,
      1031,
      1066,
      1099,
      1124,
      1165,
      1207
    ],
    "mask": "Train"
  },
  {
    "node_id": 898,
    "label": 4,
    "text": "Translingual Visual Speech Synthesis Audio-driven facial animation is an interesting and evolving technique for human-computer interaction. Based on an incoming audio stream, a face image is animated with full lip synchronization. This requires a speech recognition system in the language in which audio is provided to get the time alignment for the phonetic sequence of the audio signal. However, building a speech recognition system is data intensive and is a very tedious and time consuming task. We present a novel scheme to implement a language independent system for audio-driven facial animation given a speech recognition system for just one language, in our case, English. The method presented here can also be used for text to audio-visual speech synthesis.  1.",
    "neighbors": [
      276
    ],
    "mask": "Train"
  },
  {
    "node_id": 899,
    "label": 5,
    "text": "Monte Carlo Localization: Efficient Position Estimation for Mobile Robots This paper presents a new algorithm for mobile robot localization, called Monte Carlo Localization (MCL). MCL is a version of Markov localization, a family of probabilistic approaches that have recently been applied with great practical success. However, previous approaches were either computationally cumbersome (such as grid-based approaches that represent the state space by high-resolution 3D grids), or had to resort to extremely coarse-grained resolutions. Our approach is computationally efficient while retaining the ability to represent (almost) arbitrary distributions. MCL applies sampling-based methods for approximating probability distributions, in a way that places computation \" where needed.\" The number of samples is adapted on-line, thereby invoking large sample sets only when necessary. Empirical results illustrate that MCL yields improved accuracy while requiring an order of magnitude less computation when compared to previous approaches. It is also much easier to implement...",
    "neighbors": [
      369
    ],
    "mask": "Train"
  },
  {
    "node_id": 900,
    "label": 4,
    "text": "Using Dynamic Mediation to Integrate COTS Entities in a Ubiquitous Computing Environment . The original vision of ubiquitous computing [14] is about enabling  people to more easily accomplish tasks through the seamless interworking  of the physical environment and a computing infrastructure.  A major challenge to the practical realization of this vision involves the  integration of commercial-o-the-shelf (COTS) hardware and software  components: consider the awkwardness of such a mundane task as exporting  a textual memo written on a Palm Pilot to a Microsoft Word  document. It is not enough to overcome the protocol and data format  mismatches that currently impede the interoperation of these entities: for  the user experience to be truly seamless, we must provide a framework  for the dynamic connection of such endpoints on demand, to support the  ad-hoc interactions that are an integral part of ubiquitous computing. To  this end, we oer a dynamic mediation framework called Paths. A Path  consists of dynamically instantiated, automatically composable operators  that brid...",
    "neighbors": [
      849
    ],
    "mask": "Test"
  },
  {
    "node_id": 901,
    "label": 4,
    "text": "A Secure Infrastructure for Service Discovery Access in Pervasive Computing Security is paramount to the success of pervasive computing environments. The system presented in this paper provides a communications and security infrastructure that goes far in advancing the goal of anywhere - anytime computing. Our work securely enables clients to access and utilize services in heterogeneous networks. We provide a service registration and discovery mechanism implemented through a hierarchy of service management. The system is built upon a simplified Public Key Infrastructure that provides for authentication, non-repudiation, anti-playback, and access control. Smartcards are used as secure containers for digital certificates. The system is implemented in Java and we use Extensible Markup Language as the sole medium for communications and data exchange. Currently, we are solely dependent on a base set of access rights for our distributed trust model however, we are expanding the model to include the delegation of rights based upon a predefined policy. In our proposed expansion, instead of exclusively relying on predefined access rights, we have developed a flexible representation of trust information, in Prolog, that can model permissions, obligations, entitlements, and prohibitions. In this paper, we present the implementation of our system and describe the modifications to the design that are required to further enhance distributed trust. Our implementation is applicable to any distributed service infrastructure, whether the infrastructure is wired, mobile, or ad-hoc.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 902,
    "label": 1,
    "text": "Integration of Machine Learning and Knowledge Acquisition Introduction  \"Integration of Machine Learning and Knowledge Acquisition\" may be a surprising title for an ECAI-94 workshop since most Machine Learning (ML) systems are dedicated to Knowledge Acquisition (KA). What could thus mean integrating ML and KA ? The answer lies in the difference between the approaches developed by what is referred to as ML and KA research. Apart from some major exceptions, such as learning apprentice tools [ Mitchell et al., 1989 ] , or libraries like Machine Learning Toolbox [ MLT, 1993 ] , most ML algorithms were described without any characterization in terms of real application needs, in term of what they could be effectively useful for. However, ML methods were applied to \"real world\" problems, but few general and reusable conclusions were drawn from these knowledge acquisition experiments. As ML techniques become more and more sophisticated and able to produce various forms of knowledge, the number of possible applications grows.",
    "neighbors": [
      230
    ],
    "mask": "Train"
  },
  {
    "node_id": 903,
    "label": 2,
    "text": "Genre Classification and Domain Transfer for Information Filtering The World Wide Web is a vast repository of information, but the sheer volume makes it difficult to identify useful documents. We identify document genre is an important factor in retrieving useful documents and focus on the novel document genre dimension of subjectivity.",
    "neighbors": [
      156,
      298,
      1010
    ],
    "mask": "Train"
  },
  {
    "node_id": 904,
    "label": 4,
    "text": "Adaptation and Plasticity of User Interfaces . This paper introduces the notion of plasticity, a new property of interactive systems that denotes a particular type of user interface adaptation. It also presents a generic framework inspired from the model-based approach, for supporting the development of plastic user interfaces. This framework is illustrated with simple case studies.  KEYSWORDS. User interface adaptation, plasticity.  1. Introduction: A Design Space for Adaptation  In HCI, adaptation is modeled as two complementary system properties: adaptability and adaptivity. Adaptability is the capacity of the system to allow users to customize their system from a predefined set of parameters. Adaptivity is the capacity of the system to perform adaptation automatically without deliberate action from the user's part. Whether adaptation is performed on human requests or automatically, the design space for adaptation includes three additional orthogonal axes (see Figure 1):  . The target for adaptation. This axis denotes the enti...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 905,
    "label": 3,
    "text": "On the Equivalence of the Static and Disjunctive Well-Founded Semantics and its Computation In recent years, much work was devoted to the study of theoretical foundations of Disjunctive Logic Programming and Disjunctive Deductive Databases. While the semantics of non-disjunctive programs is fairly well understood, the declarative and computational foundations of disjunctive logic programming proved to be much more elusive and difficult. Recently, two new and promising semantics have been proposed for the class of disjunctive logic programs. The first one is the static semantics STATIC, proposed by Przymusinski, and, the other is the disjunctive well-founded semantics D-WFS, proposed by Brass and Dix.  Although the two semantics are based on very different ideas, both of them have been shown to share a number of natural and intuitive properties. In particular, both Preprint submitted to Elsevier Preprint 4 October 1999 of them extend the well-founded semantics of normal logic programs. Nevertheless, since the static semantics employs a much richer underlying language than the ...",
    "neighbors": [
      1236
    ],
    "mask": "Validation"
  },
  {
    "node_id": 906,
    "label": 5,
    "text": "Landscapes on Spaces of Trees Combinatorial optimization problems defined on sets of phylogenetic trees are an important issue in computational biology, for instance the problem of reconstruction a phylogeny using maximum likelihood or parsimony approaches. The collection of possible phylogenetic trees is arranged as a so-called Robinson graph by means of the nearest neighborhood interchange move. The coherent algebra and spectra of Robinson graphs are discussed in some detail as their knowledge is important for an understanding of the landscape structure. We consider simple model landscapes as well as landscapes arising from the maximum parsimony problem, focusing on two complementary measures of ruggedness: the amplitude spectrum arising from projecting the cost functions onto the eigenspaces of the underlying graph and the topology of local minima and their connecting saddle points.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 907,
    "label": 3,
    "text": "Deductive Queries in ODMG Databases: the DOQL Approach The Deductive Object Query Language (DOQL) is a rule-based query  language designed to provide recursion, aggregates, grouping and virtual  collections in the context of an ODMG compliant object database system.  This paper provides a description of the constructs supported by DOQL  and the algebraic operational semantics induced by DOQL's query translation  approach to implementation. The translation consists of a logical  rewriting step used to normalise DOQL expressions into molecular forms,  and a mapping step that transforms the canonical molecular form into  algebraic expressions. The paper thus not only describes a deductive  language for use with ODMG databases, but indicates how this language  can be implemented using conventional query processing techniques.  1 Introduction  The ODMG standard is an important step forward due to the provision of a reference architecture for object databases. This architecture encompasses an object model and type system, a set of imperative lan...",
    "neighbors": [
      971,
      1037
    ],
    "mask": "Train"
  },
  {
    "node_id": 908,
    "label": 1,
    "text": "How to Interpret Neural Networks In Terms of Fuzzy Logic? Neural networks are a very efficient learning tool, e.g., for transforming an experience of an expert human controller into the design of an automatic controller. It is desirable to reformulate the neural network expression for the input-output function in terms most understandable to an expert controller, i.e., by using words from natural language. There are several methodologies for transforming such natural-language knowledge into a precise form; since these methodologies have to take into consideration the uncertainty (fuzziness) of natural language, they are usually called fuzzy logics.  1",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 909,
    "label": 2,
    "text": "Going beyond Mobile Agent Platforms: Component-Based Development of Mobile Agent Systems Although mobile agents are a promising programming paradigm, the actual deployment of this technology in real applications has been far away from what the researchers were expecting. One important reason for this is the fact that in the current mobile agent frameworks it is quite difficult to develop applications without having to center them on the mobile agents and on the agent platforms. In this paper, we present a component-based framework that enables ordinary applications to use mobile agents in an easy and flexible way. By using this approach, applications can be developed using current objectoriented approaches and become able of sending and receiving agents by the simple drag-and-drop of mobility components. The framework was implemented using the JavaBeans component model and provides integration with ActiveX, which allows applications to be written in a wide variety of programming languages. By using this framework, the development of applications that can make use of mobile agents is greatly simplified, which can contribute to a wider spreading of the mobile agent technology.",
    "neighbors": [
      273
    ],
    "mask": "Validation"
  },
  {
    "node_id": 910,
    "label": 4,
    "text": "Real-time Analysis of Data from Many Sensors with Neural Networks Much research has been conducted that uses sensorbased modules with dedicated software to automatically distinguish the user's situation or context. The best results were obtained when powerful sensors (such as cameras or GPS systems) and/or sensor-specific algorithms (like sound analysis) were applied. A  somewhat new approach is to replace the one smart sensor by many simple sensors. We argue that neural networks are ideal algorithms to analyze the data coming from these sensors and describe how we came to one specific algorithm that gives good results, by giving an overview of several requirements. Finally, wearable implementations are given to show the feasibility and benefits of this approach and its implications.  1.",
    "neighbors": [
      307,
      728,
      738,
      1006
    ],
    "mask": "Train"
  },
  {
    "node_id": 911,
    "label": 0,
    "text": "From Active Objects to Autonomous Agents This paper studies how to extend the concept of active objects  into a structure of agents. It first discusses the requirements for autonomous  agents that are not covered by simple active objects. We  propose then the extension of the single behavior of an active object  into a set of behaviors with a meta-behavior scheduling their activities.  To make a concrete proposal based on these ideas we describe how we  extended a framework of active objects, named Actalk, into a generic  multi-agent platform, named DIMA. We discuss how this extension  has been implemented. We finally report on one application of DIMA  to simulate economic models.  Keywords: active object, agent, implementation, meta-behavior, modularity, re-usability, simulation.  1 Introduction  Object-oriented concurrent programming (OOCP) is the most appropriate and promising technology to implement agents. The concept of active object may be considered as the basic structure for building agents. Furthermore, the combinat...",
    "neighbors": [
      955
    ],
    "mask": "Train"
  },
  {
    "node_id": 912,
    "label": 3,
    "text": "Efficient Management of Multiversion Documents by Object Referencing Traditional approaches to versioning semistructured information are edit-based, i.e., subsequent document versions are represented by using edit scripts. This paper proposes a reference-based version management scheme that preserves the logical structure of the evolving document through the use of object references. By preserving the document structure among versions the new scheme facilitates more efficient query support. In particular, we examine queries involving projections and selections on the document versions, as well as queries on the document evolution history. Moreover, we show that the proposed scheme provides an effective representation of multiversioned XML documents, both at the transport and exchange levels. In fact, with the reference-based scheme, a document's history can also be viewed and processed as yet another XML document. Furthermore, we demonstrate the effectiveness of the new scheme at the storage level. In particular, the scheme is enhanced with a usefulness-based page management policy that extends and adapts techniques used in transaction-time databases to ensure efficient clustering of information among versions. An extensive comparison of the reference-based versioning against representations used in temporal databases and persistent object managers depicts the performance advantages of the new approach. Finally it should be noted that reference-based versioning is applicable to other kinds of semistructured information (besides XML documents), and can be used to replace traditional version control schemes, such as the edit-based RCS and the timestamp-based SCCS.",
    "neighbors": [
      336,
      402,
      889,
      1069
    ],
    "mask": "Train"
  },
  {
    "node_id": 913,
    "label": 1,
    "text": "ANSWER: Network Monitoring Using Object-Oriented Rules This paper describes ANSWER, the expert system  responsible for monitoring AT&T's 4ESS switches. These switches are extremely important, since they handle virtually all of AT&T's long distance traffic. ANSWER is implemented in R++, a rule-based extension to the C++ object-oriented programming language, and is innovative  because it employs both rule-based and object-oriented programming paradigms. The use of object technology in ANSWER has provided a principled way of modeling the 4ESS and of reasoning about failures within the 4ESS. This has resulted in an expert system that is more clearly organized, easily understood and maintainable than its predecessor, which was implemented using the rule-based paradigm alone. ANSWER has been deployed for more than a year and handles all 140 of AT&T's 4ESS switches and processes over 100,000 4ESS alarms per week. Introduction  Network reliability is of critical concern to AT&T, since its reputation for network reliability has taken many years to ...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 914,
    "label": 3,
    "text": "Knowledge Management in Heterogeneous Data Warehouse Environments This paper addresses issues related to Knowledge Management in the  context of heterogeneous data warehouse environments. The traditional notion  of data warehouse is evolving into a federated warehouse augmented by a  knowledge repository, together with a set of processes and services to support  enterprise knowledge creation, refinement, indexing, dissemination and  evolution.",
    "neighbors": [
      1086
    ],
    "mask": "Test"
  },
  {
    "node_id": 915,
    "label": 3,
    "text": "Panel: Is Generic Metadata Management Feasible? dels,   such as invert and compose?   ## What is the role of an expression language that captures the semantics of models and mappings, not only   for design but also for run-time execution?   ## Does a generic approach offer any advantages for   model manipulation areas of current interest, such as   data integration and XML?   If the skeptics are right that a generic approach to model   management is unachievable pie-in-the-sky, are writers of   metadata-driven applications doomed forever to writing   special-purpose object-at-a-time code for navigating their   information structures? If so, what is the leverage that the   database field can offer for these problems?   2. Panelists   ## Dr. Laura Haas, IBM Research is working on a tool   that can (semi-)automatically produce mappings   between two data representations. She has been   working on various aspects of data integration since   starting the Garlic project in 1994.   ##<",
    "neighbors": [
      766
    ],
    "mask": "Train"
  },
  {
    "node_id": 916,
    "label": 1,
    "text": "Toward Scalable Learning with Non-uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection Many factors influence the performance of a learned classifier. In this paper we study different methods of measuring performance based on a unified set of cost models and the effects of training class distribution with respect to these models. Observations from these effects help us devise a distributed multi-classifier meta-learning approach to learn in domains with skewed class distributions, non-uniform cost per error, and large amounts of data. One such domain is credit card fraud detection and our empirical results indicate that, up to a certain degree of skewed distribution, our approach can significantly reduce loss due to illegitimate transactions. Introduction  Inductive learning research has been focusing on devising algorithms that generate highly accurate classifiers. Many factors contribute to the quality of the learned classifier. One factor is the class distribution in the training set. Using the same algorithm, different training class distributions can generate classi...",
    "neighbors": [
      440,
      443
    ],
    "mask": "Train"
  },
  {
    "node_id": 917,
    "label": 4,
    "text": "Constraint Diagram Reasoning Diagrammatic human-computer interfaces are now becoming standard. In the near future, diagrammatic front-ends, such as those of UML-based CASE tools, will be required to offer a much more intelligent behavior than just editing. Yet there is very little formal support and there are almost no tools available for the construction of such environments. The present paper introduces a constraint-based formalism for the specification and implementation of complex diagrammatic environments. We start from grammar-based definitions of diagrammatic languages and show how a constraint solver for diagram recognition and interpretation can automatically be constructed from such grammars. In a second step, the capabilities of these solvers are extended by allowing to axiomatise formal diagrammatic systems, such as Venn Diagrams, so that they can be regarded as a new constraint domain. The ultimate aim of this schema is to establish a language of type CLP(Diagram) for diagrammatic reasoni...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 918,
    "label": 0,
    "text": "Personality in Synthetic Agents ID: A043 Personality in Synthetic Agents Rousseau, Daniel KSL, Stanford University Hayes-Roth, Barbara KSL, Stanford University Abstract Personality characterizes an individual through a set of psychological traits that influence his or her behavior. Combining visions from psychology, artificial intelligence and theater, we are studying the use of personality by intelligent, automated actors able to improvise their behavior in order to portray characters, and to interact with users in a multimedia environment We show how psychological personality traits can be exploited to produce a performance that is theatrically interesting and believable without being completely predictable. We explain how personality can influence moods and interpersonal relationships. We describe the model of a synthetic actor that takes into account those concepts to choose its behavior in a given context. In order to test our approach, we observe the performance of autonomous actors portraying waiters with di...",
    "neighbors": [
      1102
    ],
    "mask": "Train"
  },
  {
    "node_id": 919,
    "label": 4,
    "text": "Requirements for a Group Communication Service for FLARE This document explores what the requirements for a group communication service for the Framework for Location-aware Augmented Reality Environments (FLARE) are. This chapter provides an introduction to FLARE. The next chapter will explain the game rules for the rst application called Quazoom that we will build using FLARE. Since network partitions are important, we rst describe the game rules in the case when there are no partitions, and treat the partitioned case in a seperate section",
    "neighbors": [
      21,
      737
    ],
    "mask": "Train"
  },
  {
    "node_id": 920,
    "label": 0,
    "text": "The RETSINA MAS Infrastructure RETSINA is an implemented Multi-Agent System infrastructure that has been developed for several years and applied in many domains ranging from financial portfolio management to logistic planning. In this paper, we distill from our experience in developing MASs to clearly define a generic MAS infrastructure as the domain independent and reusable substratum that supports the agents' social interactions. In addition, we show that the MAS infrastructure imposes requirements on an individual agent if the agent is to be a member of a MAS and take advantage of various components of the MAS infrastructure. Although agents are expected to enter a MAS and seamlessly and e ortlessly interact with the agents in the MAS infrastructure, the current state of the art demands agents to be programmed with the knowledge of what infrastructure they will utilize, and what are various fallback and recovery mechanisms that the infrastructure provides. By providing an abstract MAS infrastructure model and a concrete implemented instance of the model, RETSINA, we contribute towards the development of principles and practice to make the MAS infrastructure \"invisible\" and ubiquitous to the interacting agents.",
    "neighbors": [
      688
    ],
    "mask": "Train"
  },
  {
    "node_id": 921,
    "label": 0,
    "text": "A Knowledge-based Approach for Lifelike Gesture Animation . The inclusion of additional modalities into the communicative behavior of virtual agents besides speech has moved into focus of human-computer interface researchers, as humans are more likely to consider computer-generated figures lifelike when appropriate nonverbal behaviors are displayed in addition to speech. In this paper, we propose a knowledge-based approach for the automatic generation of gesture animations for an articulated figure. It combines a formalism for the representation of spatiotemporal gesture features, methods for planning individual gestural animations w.r.t. to form and timing, and formation of arm trajectories. Finally, enhanced methods for rendering animations from motor programs are incorporated in the execution of planned gestures. The approach is targetted to achieve a great variety of gestures as well as a higher degree of lifelikeness in synthetic agents.  1 Introduction  The communicative behaviors of virtual anthropomorphic agents, widely used in human-...",
    "neighbors": [
      175,
      297
    ],
    "mask": "Train"
  },
  {
    "node_id": 922,
    "label": 5,
    "text": "Probabilistic Self-Localization for Mobile Robots Localization is a critical issue in mobile robotics. If the robot does not know where it is, it, cannot effectively plan movements, locate objects, or reach goals. In this paper, we describe probabilistic self-localization techniques for mobile robots that are based on the principal of maximum-likelihood estimation. The basic method is to compare a map generated at the current robot position to a previously generated map of the environment to prohabilistically maximize the agreement between the maps. This method is able to operate in both indoor and outdoor environments using either discrete features or an occupancy grid to represent the world map. The map may be generated using any method to detect features in the robot's surroundings, including vision, sonar, a d laser range-finder. A global search of the pose space is performed that guarantees that the best position in a discretized pose space is found according to the probabilistic: map agreement measure. In addition, fitting the likelihood function with a parameterized smface allows both subpixel localization and uncertainty estimation to be performed. The application of these techniques in several experiments is described, including experimental localization results for the Sojourner Mars rover. 1",
    "neighbors": [
      1089
    ],
    "mask": "Test"
  },
  {
    "node_id": 923,
    "label": 2,
    "text": "Creating a Semantic web Interface with Virtual Reality Novel initiatives amongst the Internet community such as Internet2 [1] and Qbone [2] are based on the use of high bandwidth and powerful computers. However the experience amongst the majority of Internet users is light-years from these emerging technologies. We describe the construction of a distributed high performance search engine, utilizing advanced threading techniques on a diskless Linux cluster. The resulting Virtual Reality scene is passed to a standard client machine for viewing. This search engine bridges the gap between the Internet of today, and the Internet of the future.  Keywords: Internet Searching, High Performance VRML, Visualization.  1.",
    "neighbors": [
      233,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 924,
    "label": 5,
    "text": "Using Guidelines to Constrain Interactive Case-Based HTN Planning This paper describes HICAP, a general purpose and interactive case-based planning architecture. HICAP is a decision support tool for planning a hierarchical course of action. It integrates a hierarchical task editor, HTE, with a conversational case-based planner, NaCoDAE/HTN. HTE maintains a task hierarchy representing guidelines that constrain the final plan. HTE also encodes the hierarchical organization responsible for these tasks. This supports bookkeeping, which is crucial for real-world large-scale planning tasks. HTE can be used to activate NaCoDAE/HTN to interactively refine user-selected guideline tasks into a concrete plan. Our application of HICAP to the task of noncombatant evacuation operations inspired its architecture. In this application, our empirical evaluation with ModSAF simulations confirms that the plans output by HICAP outperform those generated using alternative approaches on three dimensions.",
    "neighbors": [
      378
    ],
    "mask": "Train"
  },
  {
    "node_id": 925,
    "label": 0,
    "text": "Cryptographic Traces for Mobile Agents . Mobile code systems are technologies that allow applications  to move their code, and possibly the corresponding state, among the  nodes of a wide-area network. Code mobility is a flexible and powerful  mechanism that can be exploited to build distributed applications  in an Internet scale. At the same time, the ability to move code to and  from remote hosts introduces serious security issues. These issues include  authentication of the parties involved and protection of the hosts from  malicious code. However, the most difficult task is to protect mobile  code against attacks coming from hosts. This paper presents a mechanism  based on execution tracing and cryptography that allows one to  detect attacks against code, state, and execution flow of mobile software  components.  1 Introduction  Mobile code technologies are languages and systems that exploit some form of code mobility in an Internet-scale setting. In this framework, the network is populated by several loosely coupled co...",
    "neighbors": [
      73,
      1182
    ],
    "mask": "Test"
  },
  {
    "node_id": 926,
    "label": 4,
    "text": "Mixed-Initiative Interaction = Mixed Computation We show that partial evaluation can be usefully viewed as a programming model for realizing mixed-initiative  functionality in interactive applications. Mixed-initiative interaction between two participants is one where the  parties can take turns at any time to change and steer the flow of interaction. We concentrate on the facet of  mixed-initiative referred to as `unsolicited reporting' and demonstrate how out-of-turn interactions by users can  be modeled by `jumping ahead' to nested dialogs (via partial evaluation). Our approach permits the view of  dialog management systems in terms of their native support for staging and simplifying interactions; we characterize  three different voice-based interaction technologies using this viewpoint. In particular, we show that the  built-in form interpretation algorithm (FIA) in the VoiceXML dialog management architecture is actually a (well  disguised) combination of an interpreter and a partial evaluator.    This work is supported in part by US National Science Foundation grants DGE-9553458 and IIS-9876167.  1  1",
    "neighbors": [
      1080
    ],
    "mask": "Train"
  },
  {
    "node_id": 927,
    "label": 5,
    "text": "The RoboCup Physical Agent Challenge: Phase I Traditional AI research has not given due attention to the important role that physical bodies play for agents as their interactions produce complex emergent behaviors to achieve goals in the dynamic real world. The RoboCup Physical Agent Challenge provides a good testbed for studying how physical bodies play a signi cant role in realizing intelligent behaviors using the RoboCup framework [Kitano, et al., 95]. In order for the robots to play a soccer game reasonably well, a wide range of technologies needs to be integrated and a number of technical breakthroughs must be made. In this paper, we present three challenging tasks as the RoboCup Physical Agent Challenge Phase I: (1) moving the ball to the speci ed area (shooting, passing, and dribbling) with no, stationary, or moving obstacles, (2) catching the ball from an opponent or a teammate (receiving, goal-keeping, and intercepting), and (3) passing the ball between two players. The rst two are concerned with single agent skills while the third one is related to a simple cooperative behavior. Motivation for these challenges and evaluation methodology are given. 1.",
    "neighbors": [
      127,
      155,
      394,
      1266
    ],
    "mask": "Train"
  },
  {
    "node_id": 928,
    "label": 1,
    "text": "Face Recognition Identifying a human individual from his or her face is one of the most nonintrusive modalities in biometrics. However, it is also one of the most challenging ones. This chapter discusses why it is challenging and the factors that a practitioner can take advantage of in developing a practical face recognition system. Some major existing approaches are discussed along with some algorithmic considerations. A face recognition algorithm is presented as an example along with some experimental data. Some possible future research directions are outlined at the end of the chapter.  1.1 INTRODUCTION  Face recognition from images is a sub-area of the general object recognition problem. It is of particular interest in a wide variety of applications. Applications in law enforcement for mugshot identification, verification for personal identification such as driver's licenses and credit cards, gateways to limited access areas, surveillance of crowd behavior are all potential applications of a succes...",
    "neighbors": [
      1218
    ],
    "mask": "Validation"
  },
  {
    "node_id": 929,
    "label": 3,
    "text": "Animating Spatiotemporal Constraint Databases . Constraint databases provide a very expressive framework  for spatiotemporal database applications. However, animating such databases  is difficult because of the cost of constructing a graphical representation  of a single snapshot of a constraint database. We present a novel approach  that makes the efficient animation of constraint databases possible. The  approach is based on a new construct: parametric polygon. We present  an algorithm to construct the set of parametric polygons that represent  a given linear constraint database. We also show how to animate objects  defined by parametric polygons, analyze the computational complexity  of animation, and present empirical data to demonstrate the efficiency  of our approach.  1 Introduction  Spatiotemporal databases have recently begun to attract broader interest [10, 12, 27]. While the temporal [4, 23, 24] and spatial [14, 28] database technologies are relatively mature, their combination is far from straightforward. In this conte...",
    "neighbors": [
      27,
      539
    ],
    "mask": "Train"
  },
  {
    "node_id": 930,
    "label": 3,
    "text": "An Approach to Classify Semi-Structured Objects . Several advanced applications, such as those dealing with the Web, need to handle data whose structure is not known a-priori. Such requirement severely limits the applicability of traditional database techniques, that are based on the fact that the structure of data (e.g. the database schema) is known before data are entered into the database. Moreover, in traditional database systems, whenever a data item (e.g. a tuple, an object, and so on) is entered, the application species the collection (e.g. relation, class, and so on) the data item belongs to. Collections are the basis for handling queries and indexing and therefore a proper classication of data items in collections is crucial. In this paper, we address this issue in the context of an extended object-oriented data model. We propose an approach to classify objects, created without specifying the class they belong to, in the most appropriate class of the schema, that is, the class closest to the object state. In particular, w...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 931,
    "label": 2,
    "text": "Experiences with Selecting Search Engines Using Metasearch Search engines are among the most useful and high profile resources on the Internet. The problem of finding information on the Internet has been replaced with the problem of knowing where search engines are, what they are designed to retrieve and how to use them. This paper describes and evaluates SavvySearch, a meta-search engine designed to intelligently select and interface with multiple remote search engines. The primary meta-search issue examined is the importance of carefully selecting and ranking remote search engines for user queries. We studied the efficacy of SavvySearch's incrementally acquired meta-index approach to selecting search engines by analyzing the effect of time and experience on performance. We also compared the meta-index approach to the simpler categorical approach and showed how much experience is required to surpass the simple scheme.  1 Introduction  Search engines are powerful tools for assisting the otherwise unmanageable task of navigating the rapidly ex...",
    "neighbors": [
      115,
      118,
      271,
      404,
      433,
      435,
      496,
      510,
      526,
      627,
      768,
      897,
      1124,
      1203,
      1253
    ],
    "mask": "Validation"
  },
  {
    "node_id": 932,
    "label": 3,
    "text": "Implementing Clinical Practice Guidelines While Taking Account of Changing Evidence: ATHENA DSS, an Easily Modifiable Decision-Support System for Managing Hypertension in Primary Care This paper describes the ATHENA Decision Support System (DSS), which operationalizes guidelines for hypertension using the EON architecture. ATHENA DSS encourages blood pressure control and recommends guideline-concordant choice of drug therapy in relation to comorbid diseases. ATHENA DSS has an easily modifiable knowledge base that specifies eligibility criteria, risk stratification, blood pressure targets, relevant comorbid diseases, guideline-recommended drug classes for patients with comorbid disease, preferred drugs within each drug class, and clinical messages. Because evidence for best management of hypertension evolves continually, ATHENA DSS is designed to allow clinical experts to customize the knowledge base to incorporate new evidence or to reflect local interpretations of guideline ambiguities. Together with its database mediator Athenaeum, ATHENA DSS has physical and logical data independence from the legacy Computerized Patient Record System(CPRS) supplying the patient data, so it can be integrated into a variety of electronic medical record systems.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 933,
    "label": 2,
    "text": "Mining the Link Structure of the World Wide Web The World Wide Web contains an enormous amount of information, but it can be exceedingly difficult for users to locate resources that are both high in quality and relevant to their information needs. We develop algorithms that exploit the hyperlink structure of the WWW for information discovery and categorization, the construction of high-quality resource lists, and the analysis of on-line hyperlinked communities. 1 Introduction  The World Wide Web contains an enormous amount of information, but it can be exceedingly difficult for users to locate resources that are both high in quality and relevant to their information needs. There are a number of fundamental reasons for this. The Web is a hypertext corpus of enormous size --- approximately three hundred million Web pages as of this writing --- and it continues to grow at a phenomenal rate. But the variation in pages is even worse than the raw scale of the data: the set of Web pages taken as a whole has almost no unifying structure, wi...",
    "neighbors": [
      112,
      143,
      382,
      774,
      1000,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 934,
    "label": 5,
    "text": "Embedding Knowledge in Web Documents The paper argues for the use of general and intuitive knowledge representation languages (and simpler notational variants, e.g. subsets of natural languages) for indexing the content of Web documents and representing knowledge within them. We believe that these languages have advantages over metadata languages based on the Extensible Mark-up Language (XML). Indeed, the retrieval of precise information is better supported by languages designed to represent semantic content and support logical inference, and the readability of such a language eases its exploitation, presentation and direct insertion within a document (thus also avoiding information duplication). We advocate the use of Conceptual Graphs and simpler notational variants that enhance knowledge readability. To further ease the representation process, we propose techniques allowing users to leave some knowledge terms undeclared. We also show how lexical, structural and knowledge-based techniques may be combined to retrieve or ...",
    "neighbors": [
      188,
      456,
      835
    ],
    "mask": "Train"
  },
  {
    "node_id": 935,
    "label": 1,
    "text": "Solving Stabilization Problems in Case-Based Knowledge Acquisition Case-based reasoning is widely deemed an important methodology towards alleviating the bottleneck of knowledge acquisition. The key idea is to collect cases representing a human's or a system's experience directly rather than trying to construct generalizations. Episodic knowledge accumulated this way may be used flexibly for different purposes by determining similarities between formerly solved problems and current situations under investigation. But the flexibility of case-based reasoning brings with it a number of disadvantages. One crucial difficulty is that every new experience might seem worth to be memorized. As a result, a case-based reasoning system may substantially suffer from collecting a huge amount of garbage without being able to separate the chaff from the wheat. This paper presents a case study in case-based learning. Some target concept has to be learned by collecting cases and tuning similarity concepts. It is extremely difficult to avoid collecting a huge amount of ...",
    "neighbors": [
      473,
      776,
      1259,
      1268
    ],
    "mask": "Train"
  },
  {
    "node_id": 936,
    "label": 4,
    "text": "Affectively tunable environments for the virtual stage (Abstract) Affective computing [8, 9] is an emerging area of human-computer interaction which studies the ways in which computer systems can recognize and work with human emotions. The word affective is an adjective used in psychology to mean \"relating to emotions\". Examples of affective computing are the creation of systems which respond in different ways according to the system's perception of the user's emotional state [1, 2, 3], and systems which facilitate the communication of emotion through a virtual environment [7].  One aspect of aective computing which is particularly apposite to the theatre is the ability to create environments in which the capacity for communication of emotion (what we might call the affective bandwidth) of the environment is variable [6]. Consider the follo...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 937,
    "label": 2,
    "text": "Methods and Metrics for Cold-Start Recommendations Ve have developed a method for recommending items that combines content and collaborative data under a single probabifistic framework. We benchmark our algorithm against a naYve Bayes classifier on the cold-start problem, where we wish to recommend items that no one in the commu- nity has yet rated. Ve systematically explore three testing methodologies using a publicly available data set, and explain how these methods apply to specific real-world appli- cations. Ve advocate heuristic recommeuders when bench- marking to give competent baseline performance. Ve introduce a nev perfbrmance metric, the CROC curve, and demonstrate empirically that the various components of our testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems. Though the emphasis of onr testing is on cold-start recommending, our methods fbr recommending and evaluation are general.",
    "neighbors": [
      524,
      818,
      864,
      1068,
      1141
    ],
    "mask": "Test"
  },
  {
    "node_id": 938,
    "label": 4,
    "text": "Visualisation Effectiveness Providing evaluations of visualisations is one way to demonstrate that they support a purpose and are adequate for the role claimed for them. The problem in doing so is that there is no central source of evaluation issues that one can use a subset of for this purpose. There is also very little in the way of agreement over what constitutes a good visualisation hence the evaluation criteria differ. There are the human-computer interaction ideals, the slightly differing ones from usability engineering, those from the visualisation community, and also the need to be able to support the variable abilities of the users. Graphics, as the medium behind visualisation, may support greater bandwidth, but is also prone to more likes and dislikes than other forms of interface. The concept of visualisation effectiveness and therefore ways of evaluating visualisations provide the focus for this paper.  Keywords: Visualisation, Evaluation, Usability,  Understanding  1",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 939,
    "label": 1,
    "text": "How Statistics and Prosody can guide a Chunky Parser Introduction  Following the most common architecture of spoken dialog systems as shown in Figure 1, the main task of linguistic processing is to yield a semantic representation of what the user said. utterance User System answer Word recognizer Generator Linguistic processor Database  base Knowl. control Dialog Figure 1. Typical dialog system architecture.  These semantic representations are interpreted by the dialog module according to the dialog context and the system answer will be generated accordingly. The system utterance depends on whether the system still needs certain information or if all necessary information has been given to accomplish its task. In order to know, when all required information has been provided, the dialog  This work was partly funded by the European Community in the framework of the SQEL--Project (Spoken Queries in European Languages), Copernicus Project No. 1634. The responsibility for the contents lies with",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 940,
    "label": 3,
    "text": "A systematic classification of replicated database protocols based on atomic broadcast Database replication protocols based on group communication primitives have recently emerged as a promising technology to improve database faulttolerance and performance. Roughly speaking, this approach consists in exploiting the order and atomicity properties provided by group communication primitives or, more specifically Atomic Broadcast, to guarantee transaction properties. This paper proposes a systematic classification of non voting database replication algorithms based on Atomic Broadcast. 1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 941,
    "label": 0,
    "text": "A Methodology for Agent-Oriented Analysis and Design . This article presents Gaia: a methodology for agent-oriented analysis and design. The Gaia methodology is both general, in that it is applicable to a wide range of multi-agent systems, and comprehensive, in that it deals with both the macro-level (societal) and the micro-level (agent) aspects of systems. Gaia is founded on the view of a multi-agent system as a computational organisation consisting of various interacting roles. We illustrate Gaia through a case study (an agent-based business process management system).  1. Introduction  Progress in software engineering over the past two decades has been made through the development of increasingly powerful and natural high-level abstractions with which to model and develop complex systems. Procedural abstraction, abstract data types, and, most recently, objects and components are all examples of such abstractions. It is our belief that agents represent a similar advance in abstraction: they may be used by software developers to more n...",
    "neighbors": [
      451,
      486,
      516,
      563,
      580,
      622,
      957,
      1248
    ],
    "mask": "Validation"
  },
  {
    "node_id": 942,
    "label": 0,
    "text": "Supporting Internet-Scale Multi-Agent Systems ts a model of AgentScape from the agent perspective, that is, the location comprising the middleware and the resources are represented by a location manager agent and resource objects. Calls from an agent to the middleware are modeled by requests to the location manager agent to, for example, create an agent or move an agent. Information about resources residing at the location can be retrieved by binding to the resource objects, which are local distributed objects. These objects can be accessed only within the location they reside, not from outside the location.  For development of agent applications, an application programming interface (API) and a runtime system (RTS) are provided, see Fig. 1. The default API and RTS can be extended to provide a higher-level application programming interface with, for example, a model that offers more structure and semantics to the agent application developer.  Within AgentScape, management of large-scale agent systems is an important issue, includi",
    "neighbors": [
      74,
      257,
      688,
      964,
      1054,
      1227
    ],
    "mask": "Test"
  },
  {
    "node_id": 943,
    "label": 0,
    "text": "Exception Handling in Agent Systems A critical challenge to creating effective agent-based systems is allowing them to operate effectively when the operating environment is complex, dynamic, and error-prone. In this paper we will review the limitations of current \"agent-local\" approaches to exception handling in agent systems, and propose an alternative approach based on a shared exception handling service that is \"plugged\", with little or no customization, into existing agent systems. This service can be viewed as a kind of \"coordination doctor\"; it knows about the different ways multi-agent systems can get \"sick\", actively looks system-wide for symptoms of such \"illnesses\", and prescribes specific interventions instantiated for this particular context from a body of general treatment procedures. Agents need only implement their normative behavior plus a minimal set of interfaces. We claim that this approach offers simplified agent development as well as more effective and easier to modify exception handling behavior. T...",
    "neighbors": [
      126,
      196,
      963
    ],
    "mask": "Train"
  },
  {
    "node_id": 944,
    "label": 4,
    "text": "Appliance Data Services: Making Steps Towards an Appliance Computing World Although digital appliances are designed to be easy to use, their users often cannot even perform simple tasks because the devices lack infrastructural support. The Appliance Data Services project seeks to explore the attributes of an appliance computing world and develop the infrastructure required to support users with digital appliances.  1",
    "neighbors": [
      849
    ],
    "mask": "Test"
  },
  {
    "node_id": 945,
    "label": 0,
    "text": "Diagnosis as an Integral Part of Multi-Agent Adaptability Agents working under real world conditions may face an environment capable of changing rapidly from one moment to the next, either through perceived faults, unexpected interactions or adversarial intrusions. To gracefully and efficiently handle such situations, the members of a multi-agent system must be able to adapt, either by evolving internal structures and behavior or repairing or isolating those external influenced believed to be malfunctioning. The first step in achieving adaptability is diagnosis - being able to accurately detect and determine the cause of a fault based on its symptoms. In this paper we examine how domain independent diagnosis plays a role in multi-agent systems, including the information required to support and produce diagnoses. Particular attention is paid to coordination based diagnosis directed by a causal model. Several examples are described in the context of an Intelligent Home environment, and the issue of diagnostic sensitivity versus efficiency is ad...",
    "neighbors": [
      200,
      441,
      513
    ],
    "mask": "Train"
  },
  {
    "node_id": 946,
    "label": 4,
    "text": "The Role of Children in the Design of New Technology This paper suggests a framework for understanding the roles that children can play in the technology design process, particularly in regards to designing technologies that support learning. Each role, user, tester, informant, and design partner has been defined based upon a review of the literature and my lab\u2019s own research experiences. This discussion does not suggest that any one role is appropriate for all research or development needs. Instead, by understanding this framework the reader may be able to make more informed decisions about the design processes they choose to use with children in creating new technologies. This paper will present for each role a historical overview, research and development methods, as well as the strengths, challenges, and unique contributions associated with children in the design process.",
    "neighbors": [
      628,
      648,
      1074
    ],
    "mask": "Train"
  },
  {
    "node_id": 947,
    "label": 2,
    "text": "Concept Decompositions for Large Sparse Text Data using Clustering Abstract. Unlabeled document collections are becoming increasingly common and available; mining such data sets represents a major contemporary challenge. Using words as features, text documents are often represented as high-dimensional and sparse vectors\u2013a few thousand dimensions and a sparsity of 95 to 99 % is typical. In this paper, we study a certain spherical k-means algorithm for clustering such document vectors. The algorithm outputs k disjoint clusters each with a concept vector that is the centroid of the cluster normalized to have unit Euclidean norm. As our first contribution, we empirically demonstrate that, owing to the high-dimensionality and sparsity of the text data, the clusters produced by the algorithm have a certain \u201cfractal-like \u201d and \u201cself-similar \u201d behavior. As our second contribution, we introduce concept decompositions to approximate the matrix of document vectors; these decompositions are obtained by taking the least-squares approximation onto the linear subspace spanned by all the concept vectors. We empirically establish that the approximation errors of the concept decompositions are close to the best possible, namely, to truncated singular value decompositions. As our third contribution, we show that the concept vectors are localized in the word space, are sparse, and tend towards orthonormality. In contrast, the singular vectors are global in the word space and are dense. Nonetheless, we observe the surprising fact that the linear subspaces spanned by the concept vectors and the leading singular vectors are quite close in the sense of small principal angles between them. In conclusion, the concept vectors produced by the spherical k-means",
    "neighbors": [
      410,
      545,
      893,
      1049
    ],
    "mask": "Test"
  },
  {
    "node_id": 948,
    "label": 3,
    "text": "Belief Reasoning in MLS Deductive Databases It is envisaged that the application of the multilevel security (MLS) scheme will enhance exibility and e ectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view de nitions on a per user basis. However, as advances in this area are being made and ideas crystallized, the concomitantweaknesses of the MLS databases are also surfacing. We insist that the critical problem with the current model is that the belief at a higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported. Critics also argue that it is imperative for MLS database users to theorize about the belief of others, perhaps at di erent security levels, an apparatus that is currently missing and the absence of which is seriously felt. The impetus for our current research is this need to provide an adequate framework for belief reasoning in MLS databases. We demonstrate that a prudent application of the concept of inheritance in a deductive database setting will help capture the notion of declarative belief and belief reasoning in MLS databases in an elegantway. To this end, we develop a function to compute belief in multiple modes which can be used to reason about the beliefs of other users. We strive to develop a poised and practical logical characterization of MLS databases for the rst time based on the inherently di cult concept of non-monotonic inheritance. We present an extension of the acclaimed Datalog language, called the MultiLog, and show that Datalog is a special case of our language. We also suggest an implementation scheme for MultiLog as a front-end for CORAL. Key Words: MLS databases, belief assertion, reasoning,",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 949,
    "label": 1,
    "text": "Face Recognition Using Evolutionary Pursuit . This paper describes a novel and adaptive dictionary method for face recognition using genetic algorithms (GAs) in determining the optimal basis for encoding human faces. In analogy to pursuit methods, our novel method is called Evolutionary Pursuit (EP), and it allows for different types of (non-orthogonal) bases. EP processes face images in a lower dimensional whitened PCA subspace. Directed but random rotations of the basis vectors in this subspace are searched by GAs where evolution is driven by a fitness function defined in terms of performance accuracy and class separation (scatter index). Accuracy indicates the extent to which learning has been successful so far, while the scatter index gives an indication of the expected fitness on future trials. As a result, our approach improves the face recognition performance compared to PCA, and shows better generalization abilities than the Fisher Linear Discriminant (FLD) based methods. 1 Introduction  A successful face recognition met...",
    "neighbors": [
      330,
      1118
    ],
    "mask": "Train"
  },
  {
    "node_id": 950,
    "label": 3,
    "text": "A Case for Parameterized Views and Relational Unification In this paper, we address the issue of remedying the scepter of impedance mismatch in object-relational SQL. Our approach makes it possible to remain within the current connes of relational models, yet oers the capability of dening methods by SQL's declarative means, thereby preserving all opportunities of query optimization to the fullest extent. We propose the idea of parameterized views and an extension of SQL's create view construct with an optional with parameter clause. Parameterizing enables traditional SQL views to accept input values and delay the computation of the view until invoked with a call statement. This extension empowers users with the capability of modifying the behavior of predened procedures (views) by sending arguments and evaluating the procedure on demand.  Keywords: parameterized views, declarative methods, objectrelational databases, inheritance and overriding, reasoning, unication.  1 Introduction  An outstanding issue in object-relational databases dem...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 951,
    "label": 2,
    "text": "Rank Aggregation Revisited The rank aggregation problem is to combine many different rank orderings on the same set of candidates,  or alternatives, in order to obtain a \"better\" ordering. Rank aggregation has been studied extensively  in the context of social choice theory, where several \"voting paradoxes\" have been discovered. The problem",
    "neighbors": [
      1007,
      1017,
      1098
    ],
    "mask": "Test"
  },
  {
    "node_id": 952,
    "label": 2,
    "text": "Evolving User Profiles to Reduce Internet Information Overload . This paper discusses the use of Evolving Personal Agent Environments as a potential solution to the problem of information overload as experienced in habitual Web surfing. Some first experimental results on evolving user profiles using speciating hybrid GAs, the reasoning behind them and support for their potential application in mobile, wireless and location aware information devices are also presented.  1 Information Overload  In everyday life, the Internet user is faced with the ever increasing problem of information overload, whether this occurs at home, at the workplace, or as will soon be happening, everywhere [1] [2]. The overwhelming information feed that computer users face leads to anxiety, strain, inefficiency and finally results in uninformed (or misinformed) and frustrated users [3] [4]. Continuously and increasingly Internet users are confronted with laborious and difficult tasks of information filtering and/or gathering, which are inherently computer-oriented processes...",
    "neighbors": [
      314,
      606,
      780
    ],
    "mask": "Validation"
  },
  {
    "node_id": 953,
    "label": 0,
    "text": "Formal Models and Decision Procedures for Multi-Agent Systems The study of computational agents capable of rational behaviour has received a great deal of attention in recent years. A number of theoretical formalizations for such multiagent systems have been proposed. However, most of these formalizations do not have a strong semantic basis nor a sound and complete axiomatization. Hence, it has not been clear as to how these formalizations could be used in building agents in practice. This paper explores a particular type of multi-agent system, in which each agent is viewed as having the three mental attitudes of belief (B), desire (D), and intention (I). It provides a family of multi-modal branching-time BDI logics with a semantics that is grounded in traditional decision theory and a possible-worlds framework, categorizes them, provides sound and complete axiomatizations, and gives constructive tableaubased decision procedures for testing the satisfiability and validity of formulas. The computational complexity of these decision procedures is n...",
    "neighbors": [
      159,
      263,
      327,
      557,
      647,
      687,
      816
    ],
    "mask": "Train"
  },
  {
    "node_id": 954,
    "label": 1,
    "text": "Closing the Loop: Heuristics for Autonomous Discovery Autonomous discovery systems will be able to peruse very large databases more thoroughly than people can. In a companion paper [1], we describe a general framework for autonomous systems. We present and evaluate heuristics for use in this framework. Although these heuristics were designed for a prototype system, we believe they provide good initial solutions to problems encountered when implementing fully autonomous discovery systems. As such, these heuristics may be used as the starting point for future research into fully autonomous discovery systems. 1.",
    "neighbors": [
      994
    ],
    "mask": "Train"
  },
  {
    "node_id": 955,
    "label": 5,
    "text": "Steps towards C+C: a Language for Interactions . We present in this paper our reflections about the requirements of",
    "neighbors": [
      445,
      911,
      1067
    ],
    "mask": "Train"
  },
  {
    "node_id": 956,
    "label": 2,
    "text": "Active Hidden Markov Models for Information Extraction Information extraction from HTML documents requires a classifier capable of assigning semantic labels to the words or word sequences to be extracted. If completely labeled documents are available for training, well-known Markov model techniques can be used to learn such classifiers. In this paper, we consider the more challenging task of learning hidden Markov models (HMMs) when only partially (sparsely) labeled documents are available for training. We first give detailed account of the task and its appropriate loss function, and show how it can be minimized given an HMM. We describe an EM style algorithm for learning HMMs from partially labeled data. We then present an active learning algorithm that selects \"difficult\" unlabeled tokens and asks the user to label them. We study empirically by how much active learning reduces the required data labeling effort, or increases the quality of the learned model achievable with a given amount of user effort.",
    "neighbors": [
      279,
      379,
      875,
      1024
    ],
    "mask": "Train"
  },
  {
    "node_id": 957,
    "label": 0,
    "text": "A Methodology and Modelling Technique for Systems of BDI Agents The construction of large-scale embedded software systems demands the use of design methodologies and modelling techniques that support abstraction, inheritance, modularity, and other mechanisms for reducing complexity and preventing error. If multi-agent systems are to become widely accepted as a basis for large-scale applications, adequate agentoriented methodologies and modelling techniques will be essential. This is not just to ensure that systems are reliable, maintainable, and conformant, but to allow their design, implementation, and maintenance to be carried out by software analysts and engineers rather than researchers. In this paper we describe an agent-oriented methodology and modelling technique for systems of agents based upon the Belief-Desire-Intention (BDI) paradigm. Our models extend existing Object-Oriented (OO) models. By building upon and adapting existing, well-understood techniques, we take advantage of their maturity to produce an approach that can be easily lear...",
    "neighbors": [
      140,
      286,
      356,
      451,
      516,
      521,
      622,
      885,
      941
    ],
    "mask": "Train"
  },
  {
    "node_id": 958,
    "label": 2,
    "text": "Mixed Initiative Interfaces for Learning Tasks: SMARTedit Talks Back Applications of machine learning can be viewed as teacherstudent interactions in which the teacher provides training examples and the student learns a generalization of the training examples. One such application of great interest to the IUI community is adaptive user interfaces. In the traditional learning interface, the scope of teacher-student interactions consists solely of the teacher/user providing some number of training examples to the student/learner and testing the learned model on new examples. Active learning approaches go one step beyond the traditional interaction model and allow the student to propose new training examples that are then solved by the teacher. In this paper, we propose that interfaces for machine learning should even more closely resemble human teacher-student relationships. A teacher's time and attention are precious resources. An intelligent student must proactively contribute to the learning process, by reasoning about the quality of its knowledge, collaborating with the teacher, and suggesting new examples for her to solve. The paper describes a variety of rich interaction modes that enhance the learning process and presents a decision-theoretic framework, called DIAManD, for choosing the best interaction. We apply the framework to the SMARTedit programming by demonstration system and describe experimental validation and preliminary user feedback.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 959,
    "label": 2,
    "text": "Supporting Situated Actions in High Volume Conversational Data Situations The global conferencing system Usenet news offers an amount of articles per day that exceeds human cognitive capabilities by far although the articles are already organized in hierarchically structured discussion groups covering distinct topics. We report here on a situated information filtering system that significantly reduces the burden by supporting the user in acting situated. Interpreting the user's actions as situated actions, the approach complements current filtering and recommender approaches by completely avoiding the modeling of user interests; the user is the only instance for assigning (un-)interestingness to Usenet discussions.  Keywords  Situated cognition, situated actions, Usenet news, information filtering  INTRODUCTION  The huge and increasing amount of information available in the information age suggests to investigate new ways to support humans in gathering information that might be interesting, helpful, or necessary for them. Since the overall amount of informat...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 960,
    "label": 5,
    "text": "DOGMA: A GA-Based Relational Learner We describe a GA-based concept learning/theory revision system DOGMA and discuss how it can be applied to relational learning. The search for better theories in DOGMA is guided by a novel fitness function that combines the minimal description length and information gain measures. To show the efficacy of the system we compare it to other learners in three relational domains. Keywords: Relational Learning, Genetic Algorithms, Minimal Description Length  1 Introduction  Genetic Algorithms (GAs) are stochastic general purpose search algorithms, that have been applied to a wide range of Machine Learning problems. They work by evolving a population of chromosomes, each of which encodes a potential solution to the problem at hand. The task of a GA is to find a highly fit chromosome through the application of different selection and perturbation operators. In this paper we consider the use of GAs in relational concept learning, i.e. in the process of learning and extracting relational classif...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 961,
    "label": 1,
    "text": "Probabilistic Retrieval: New Insights and Experimental Results We present new insights on the relations between a recently introduced probabilistic formulation of the content-based retrieval problem and standard solutions. New experimental results are presented, providing evidence that probabilistic retrieval has superior performance. Finally, a unified representation for texture and color is introduced. 1 Introduction  The problem of retrieving images or video from a database is naturally formulated as a problem of pattern recognition. Given a representation (or feature) space F for the entries in the database, the design of a retrieval system consists of finding a map  g : F ! M = f1; : : : ; Kg  x ! y  from F to the set M of classes identified as useful for the retrieval operation. K, the cardinality of M , can be as large as the number of items in the database (in which case each item is a class by itself), or smaller. If the goal of the retrieval system is to minimize the probability of error, i.e. P (g(x) 6= y), it is well known that the opt...",
    "neighbors": [
      105
    ],
    "mask": "Train"
  },
  {
    "node_id": 962,
    "label": 4,
    "text": "An Introduction to Software Agents ion and delegation: Agents can be made extensible and composable in ways that common iconic interface objects cannot. Because we can \"communicate\" with them, they can share our goals, rather than simply process our commands. They can show us how to do things and tell us what went wrong (Miller and Neches 1987). . Flexibility and opportunism: Because they can be instructed at the level of 16 BRADSHAW goals and strategies, agents can find ways to \"work around\" unforeseen problems and exploit new opportunities as they help solve problems. . Task orientation: Agents can be designed to take the context of the person's tasks and situation into account as they present information and take action. . Adaptivity: Agents can use learning algorithms to continually improve their behavior by noticing recurrent patterns of actions and events. Toward Agent-Enabled System Architectures In the future, assistant agents at the user interface and resource-managing agents behind the scenes will increas...",
    "neighbors": [
      620,
      663,
      725
    ],
    "mask": "Validation"
  },
  {
    "node_id": 963,
    "label": 0,
    "text": "The Adaptive Agent Architecture: Achieving Fault-Tolerance Using Persistent Broker Teams Brokers are used in many multi-agent systems for locating agents, for routing and sharing  information, for managing the system, and for legal purposes, as independent third parties.  However, these multi-agent systems can be incapacitated and rendered non-functional when the  brokers become inaccessible due to failures such as machine crashes, network breakdowns, and  process failures that can occur in any distributed software system.  We propose that the theory of teamwork can be used to create robust brokered architectures that  can recover from broker failures, and we present the Adaptive Agent Architecture (AAA) to show  the feasibility of this approach. The AAA brokers form a team with a joint commitment to serve  any agent that registers with the broker team as long as the agent remains registered with the  team. This commitment enables the brokers to substitute for each other when needed. A multiagent  system based on the AAA can continue to work despite broker failures as long...",
    "neighbors": [
      200,
      312,
      724,
      943
    ],
    "mask": "Validation"
  },
  {
    "node_id": 964,
    "label": 0,
    "text": "BDI Agents: from Theory to Practice The study of computational agents capable of rational behaviour has received a great deal of attention in recent years. Theoretical formalizations of such agents and their implementations have proceeded in parallel with little or no connection between them. This paper explores a particular type of rational agent, a BeliefDesire -Intention (BDI) agent. The primary aim of this paper is to integrate (a) the theoretical foundations of BDI agents from both a quantitative decision-theoretic perspective and a symbolic reasoning perspective; (b) the implementations of BDI agents from an ideal theoretical perspective and a more practical perspective; and (c) the building of large-scale applications based on BDI agents. In particular, an air-traffic management application will be described from both a theoretical and an implementation perspective.  Introduction  The design of systems that are required to perform high-level management and control tasks in complex dynamic environments is becoming ...",
    "neighbors": [
      25,
      42,
      106,
      141,
      263,
      277,
      285,
      375,
      396,
      405,
      425,
      522,
      557,
      636,
      647,
      880,
      884,
      942,
      1051,
      1208,
      1269
    ],
    "mask": "Test"
  },
  {
    "node_id": 965,
    "label": 4,
    "text": "Bridging Multiple User Interface Dimensions with Augmented Reality Studierstube is an experimental user interface system, which uses collaborative augmented reality to incorporate true 3D interaction into a productivity environment. This concept is extended to bridge multiple user interface dimensions by including multiple users, multiple host platforms, multiple display types, multiple concurrent applications, and a multi-context (i. e., 3D document) interface into a heterogeneous distributed environment. With this architecture, we can explore the user interface design space between pure augmented reality and the popular ubiquitous computing paradigm. We report on our design philosophy centered around the notion of contexts and locales, as well as the underlying software and hardware architecture. Contexts encapsulate a live application together with 3D (visual) and other data, while locales are used to organize geometric reference systems. By separating geometric relationships (locales) from semantic relationships (contexts), we achieve a great amou...",
    "neighbors": [
      75,
      215
    ],
    "mask": "Train"
  },
  {
    "node_id": 966,
    "label": 4,
    "text": "Visualization Methods for Personal Photo Collections: Browsing and Searching in the PhotoFinder Software tools for personal photo collection management are proliferating, but they usually have limited searching and browsing functions. We implemented the PhotoFinder prototype to enable non-technical users of personal photo collections to search and browse easily. PhotoFinder provides a set of visual Boolean query interfaces, coupled with dynamic query and query preview features. It gives users powerful search capabilities. Using a scatter plot thumbnail display and dragand -drop interface, PhotoFinder is designed to be easy to use for searching and browsing photos.  Keywords : PhotoFinder, user interface, dynamic query, query preview, search, browsing, Boolean query, digital photo library.  1. INTRODUCTION  Digital cameras, scanners and personal computers are now common. But as collections grow in size, the need to organize, search, and browse digital photos increases [1]. There are many personal photo collection management tools available either commercially or non-commercially. ...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 967,
    "label": 0,
    "text": "Cross-Entropy Guided Ant-like Agents Finding Cyclic Paths in Scarcely Meshed Networks Telecommunication network owners and operators have for half a century been well aware of the potential loss of revenue if a major trunk is damaged, thus dependability at high cost has been implemented. A simple, effective and common dependability scheme is 1:1 protection with 100% capacity redundancy in the network. A growing number of applications in need of dependable connections with specific requirements to bandwidth and delay have started using the internet (which only provides best effort transport) as their base communication service. In this paper we adopt the 1:1 protection scheme and incorporate it as part of a routing system applicable for internet infrastructures. 100% capacity redundancy is no longer required. A distributed stochastic path finding (routing) algorithm based on Rubinstein's Cross Entropy method for combinatorial optimisation is presented. Early results from Monte Carlo simulations indeed indicate that the algorithm is capable of finding pairs of independent primary and backup paths satisfying specific bandwidth a constraints.",
    "neighbors": [
      104
    ],
    "mask": "Train"
  },
  {
    "node_id": 968,
    "label": 2,
    "text": "Breadth-First Search Crawling Yields High-Quality Pages This paper examines the average page quality over time of pages downloaded during a web crawl of 328 million unique pages. We use the connectivity-based metric PageRank to measure the quality of a page. We show that traversing the web graph in breadth-first search order is a good crawling strategy, as it tends to discover high-quality pages early on in the crawl.",
    "neighbors": [
      1,
      53,
      235,
      649,
      763,
      1005,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 969,
    "label": 4,
    "text": "BUILD-IT: a computer vision-based interaction technique of a planning tool for construction and design It is time to go beyond the established approaches in human-computer interaction. With the  Augmented Reality (AR) design strategy humans are able to behave as much as possible in a natural way: behavior  of humans in the real world with other humans and/or real world objects. Following the fundamental  constraints of natural way of interacting we derive a set of recommendations for the next generation of user  interfaces: the Natural User Interface (NUI). The concept of NUI is presented in form of a runnable demonstrator:  a computer vision-based interaction technique for a planning tool for construction and design tasks.  KEYWORDS augmented reality, natural user interface, computer vision-based interaction  1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 970,
    "label": 3,
    "text": "From Databases to Information Systems - Information Quality Makes the Difference Research and business is currently moving from centralized databases towards in-formation systems integrating distributed and autonomous data sources. Simultane-ously, it is a well acknowledged fact that consideration of information quality - IQ-reasoning - is an important issue for large-scale integrated information systems. We show that IQ-reasoning can be the driving force of the current shift from databases to integrated information systems. In this paper, we explore the implications and consequences of this shift. All areas of answering user queries are affected - from user input, to query planning and query optimization, and finally to building the query result. The application of IQ-reasoning brings both challenges, such as new cost models for optimization, and opportunities, such as improved query planning. We highlight several emerging aspects and suggest solutions toward a pervasion of information quality in information systems.",
    "neighbors": [
      549,
      553
    ],
    "mask": "Train"
  },
  {
    "node_id": 971,
    "label": 3,
    "text": "Design and Implementation of a Deductive Query Language for ODMG Compliant Object Databases Introduction  Deductive object-oriented databases (DOODs) seek to provide the combined support for the expressive modelling features available in the object-oriented data model and the powerful query language features available in deductive databases. When successfully engineered, this combination can broaden the spectrum of declarative queries that can be supported by the DBMS, and ease their implementation due to the increased functionality in the query capabilities of the resulting system. The extra leverage obtained from support for deductive functionality is relevant to building database middleware for distributed information systems [19], managing semistructured data [11], and for building decision support and knowledge discovery systems [4].  Unlike deductive relational database systems (DRDBs), which were designed and implemented based on the formal denition of the relational data model by Codd, and on the widely researched deductive query language model (language cons",
    "neighbors": [
      907
    ],
    "mask": "Test"
  },
  {
    "node_id": 972,
    "label": 3,
    "text": "Discovery-driven Exploration of OLAP Data Cubes .  Analysts predominantly use OLAP data cubes to identify regions of anomalies that may represent problem areas or new opportunities. The current OLAP systems support hypothesis-driven exploration of data cubes through operations such as drill-down, roll-up, and selection. Using these operations, an analyst navigates unaided through a huge search space looking at large number of values to spot exceptions. We propose a new discovery-driven exploration paradigm that mines the data for such exceptions and summarizes the exceptions at appropriate levels in advance. It then uses these exceptions to lead the analyst to interesting regions of the cube during navigation. We present the statistical foundation underlying our approach. We then discuss the computational issue of finding exceptions in data and making the process efficient on large multidimensional data bases.  1 Introduction  On-Line Analytical Processing (OLAP) characterizes the operations of summarizing, consolidating, viewing, a...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 973,
    "label": 1,
    "text": "Support Vector Machine - Reference Manual this document will describe these programs. To find out more about SVMs, see the bibliography. We will not describe how SVMs work here. The first program we will describe is the paragen program, as it specifies all parameters needed for the SVM. 3 paragen When using the support vector machine for any given task, it is always necessary to specify a set of parameters. These parameters include information such as whether you are interested in pattern recognition or regression estimation, what kernel you are using, what scaling is to be done on the data, etc... paragen generates parameter files used by the SVM program, if no file was generated the user will be asked interactively.",
    "neighbors": [
      30,
      134,
      727
    ],
    "mask": "Validation"
  },
  {
    "node_id": 974,
    "label": 1,
    "text": "Selecting a Fuzzy Logic Operation from the DNF-CNF Interval: How Practical Are the Resulting Operations? In classical (two-valued) logic, CNF and DNF  forms of each propositional formula are equivalent  to each other. In fuzzy logic, CNF and  DNF forms are not equivalent, they form an interval  that contains the fuzzy values of all classically  equivalent propositional formulas. If we  want to select a single value from this interval,  then it is natural to select a linear combination  of the interval's endpoints. In particular, we  can do that for CNF and DNF forms of \"and\"  and \"or\", thus designing natural fuzzy analogues  of classical \"and\" and \"or\" operations.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 975,
    "label": 1,
    "text": "Extended Experimental Explorations Of The Necessity Of User Guidance In Case-Based Learning This is an extended report focussing on experimental results to explore the necessity of user guidance in case-based knowledge acquisition. It is covering a collection of theoretical investigations as well. The methodology of our approach is quite simple: We choose a well-understood area which is tailored to case-based knowledge acquisition. Furthermore, we choose a prototypical case-based learning algorithm which is obviously suitable for the problem domain under consideration. Then, we perform a number of knowledge acquisition experiments. They clearly exhibit essential limitations of knowledge acquisition from randomly chosen cases. As a consequence, we develop scenarios of user guidance. Based on these theoretical concepts, we prove a few theoretical results characterizing the power of our approach. Next, we perform a new series of more constrained results which support our theoretical investigations. The main experiments deal with the difficulties of learning from randomly arrange...",
    "neighbors": [
      776,
      1154,
      1259,
      1268
    ],
    "mask": "Train"
  },
  {
    "node_id": 976,
    "label": 1,
    "text": "The Self-Organizing Map in Industry Analysis The Self-Organizing Map (SOM) is a powerful neural network method for the analysis and visualization of high-dimensional data. It maps nonlinear statistical relationships between high-dimensional measurement data into simple geometric relationships, usually on a two-dimensional grid. The mapping roughly preserves the most important topological and metric relationships of the original data elements and, thus, inherently clusters the data. The need for visualization and clustering occurs, for instance, in the data analysis of complex processes or systems. In various engineering applications, entire fields of industry can be investigated using SOM based methods. The data exploration tool presented in this chapter allows visualization and analysis of large data bases of industrial systems. Forest industry is the \u00f8rst chosen application for the tool. To illustrate the global nature of forest industry, the example case is used to cluster the pulp and paper mills of the world.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 977,
    "label": 3,
    "text": "Tertiary Storage Organization for Large Multidimensional Datasets",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 978,
    "label": 0,
    "text": "Flying Emulator: Rapid Building and Testing of Networked Applications for Mobile Computers This paper presents a mobile-agent framework for building and testing  mobile computing applications. When a portable computing device is moved into  and attached to a new network, the proper functioning of an application running  on the device often depends on the resources and services provided locally in the  current network. To solve this problem, this framework provides an applicationlevel  emulator of portable computing devices. Since the emulator is constructed  as a mobile agent, it can carry target applications across networks on behalf of  a device, and it allows the applications to connect to local servers in its current  network in the same way as if they were moved with and executed on the device  itself. This paper also demonstrates the utility of this framework by describing  the development of typical location-dependent applications in mobile computing  settings.",
    "neighbors": [
      465
    ],
    "mask": "Train"
  },
  {
    "node_id": 979,
    "label": 4,
    "text": "Interactive Music for Instrumented Dancing Shoes We have designed and built a pair of sneakers that each sense 16 different tactile and free-gesture parameters.  These include continuous pressure at 3 points in the forward sole, dynamic pressure at the heel, bidirectional  bend of the sole, height above instrumented portions of the floor, 3-axis orientation about the Earth's magnetic  field, 2-axis gravitational tilt and low-G acceleration, 3-axis shock, angular rate about the vertical, and  translational position via a sonar transponder. Both shoes transfer these parameters to a base station across an  RF link at 50 Hz State updates. As they are powered by a local battery, there are no tethers or wires running off  the shoe. A PC monitors the data streaming off both shoes and translates it into real-time interactive music. The  shoe design is introduced, and the interactive music mappings that we have developed for dance performances  are discussed.  1) Introduction  A trained dancer is capable of expressing highly dexterous control...",
    "neighbors": [
      33,
      474
    ],
    "mask": "Train"
  },
  {
    "node_id": 980,
    "label": 5,
    "text": "Incremental Recompilation of Knowledge Approximating a general formula from above and below by Horn formulas (its Horn  envelope and Horn core, respectively) was proposed in [22] as a form of \"knowledge compilation,  \" supporting rapid approximate reasoning; on the negative side, this scheme is  static in that it supports no updates, and has certain complexity drawbacks pointed out  in [17]. On the other hand, the many frameworks and schemes proposed in the literature  for theory update and revision are plagued by serious complexity-theoretic impediments,  even in the Horn case, as was pointed out in [6], and is further demonstrated in the present  paper. More fundamentally, these schemes are not inductive, in that they may lose in a  single update any positive properties of the represented sets of formulas (small size, Horn  structure, etc.). In this paper  1  we propose a new scheme, incremental recompilation, which  combines Horn approximation and model-based updates; this scheme is inductive and very  efficient, free of...",
    "neighbors": [
      207,
      449
    ],
    "mask": "Train"
  },
  {
    "node_id": 981,
    "label": 0,
    "text": "A Reactive-Deliberative Model of Dialogue Agency . For an agent to engage in substantive dialogues with other agents, there are several complexities which go beyond the scope of standard models of rational agency. In particular, an agent must reason about social attitudes that span more than one agent, as well as the dynamic and fallible process of plan execution. In this paper we sketch a theory of plan execution which allows the representation of failure and repair, extend the underlying agency model with social attitudes of mutual belief, obligation, and multi-agent plan execution, and describe an implemented dialogue agent which uses these notions, reacting to its environment and mental state, and deliberating and planning action only when more pressing concerns are absent. 1 Overview  For autonomous agents that operate in a realm of heterogeneous agents (including human agents), an agent theory should allow many of the features of natural language dialogue. The agent communication protocols should allow flexible turn-taking and ...",
    "neighbors": [
      178
    ],
    "mask": "Test"
  },
  {
    "node_id": 982,
    "label": 1,
    "text": "A Neural Network Diagnosis Model without Disorder Independence Assumption . Generally, the disorders in a neural network diagnosis model are assumed independent each other. In this paper, we propose a neural network model for diagnostic problem solving where the disorder independence assumption is no longer necessary. Firstly, we characterize the diagnostic tasks and the causal network which is used to represent the diagnostic problem, then we describe the neural network diagnosis model, finally, some experiment results will be given.  1 Introduction  Finding explanations for a given set of events is an important aspect of general intelligent behaviour. The process of finding the best explanation was defined as  Abduction by the philosopher C. S. Peirce [8]. Diagnosis is a typical abductive problem. For a set of manifestations(observations), the diagnostic inference is to find the most plausible faults or disorders which can explain the manifestations observed. In general, an individual fault or disorder can explain only a portion of the manifestations. Ther...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 983,
    "label": 1,
    "text": "Soft Computing and Fault Management Soft computing is a partnership between A.I. techniques that are tolerant of imprecision, uncertainty and partial truth, with the aim of obtaining a robust solution for complex systems. Telecommunication systems are built with extensive redundancy and complexity to ensure robustness and quality of service. To facilitate this requires complex fault identification and management systems. Fault identification and management is generally handled by reducing the amount of alarm events (symptoms) presented to the operating engineer through monitoring, filtering and masking. The ultimate goal is to determine and present the actual underlying fault. Fault Management is a complex task subject to uncertainty in the 'symptoms' presented and as such is ideal for treatment by soft computing techniques. The aim of this paper is to present a soft computing approach to fault management in telecommunication systems. Two key approaches are considered; AI & soft computing rule discovery and techniques to attempt to present less symptoms with greater diagnostic assistance for the more traditional rule based system approach and a hybrid soft computing approach which utilises a genetic algorithm to learn Bayesian belief networks (BBNs). It is also highlighted that research and development of the two target Fault Management Systems are complementary. Keywords: Network management, fault management, knowledge discovery, Bayesian belief networks, genetic algorithms, soft computing. 1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 984,
    "label": 3,
    "text": "Efficient Goal Directed Bottom-up Evaluation of Logic Programs This paper introduces a new strategy for the efficient goal directed bottomup evaluation of logic programs. Instead of combining a standard bottomup evaluation strategy with a Magic-set transformation, the evaluation strategy is specialized for the application to Magic-set programs which are characterized by clause bodies with a high degree of overlapping. The approach is similar to other techniques which avoid re-computation by maintaining and reusing partial solutions to clause bodies. However, the overhead is considerably reduced as these are maintained implicitly by the underlying Prolog implementation. The technique is presented as a simple meta-interpreter for goal directed bottom-up evaluation. No Magic-set transformation is involved as the dependencies between calls and answers are expressed directly within the interpreter. The proposed technique has been implemented and shown to provide substantial speed-ups in applications of semantic based program analysis based on bottom-up...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 985,
    "label": 5,
    "text": "Towards Text Knowledge Engineering We introduce a methodology for automating the maintenance of domain-specific taxonomies based on natural language text understanding. A given ontology is incrementally updated as new concepts are acquired from real-world texts. The acquisition process is centered around the linguistic and conceptual \"quality\" of various forms of evidence underlying the generation and refinement of concept hypotheses. On the basis of the quality of evidence, concept hypotheses are ranked according to credibility and the most credible ones are selected for assimilation into the domain knowledge base. Appeared in:  AAAI'98 - Proceedings of the 15th National Conference on Artificial Intelligence, July 26-30, 1998, Madison, Wisconsin (forthcoming)  Towards Text Knowledge Engineering  Udo Hahn & Klemens Schnattinger  L F Computational Linguistics Group Text Knowledge Engineering Lab Freiburg University Werthmannplatz 1, D-79085 Freiburg, Germany  http://www.coling.uni-freiburg.de Abstract  We introduce a me...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 986,
    "label": 1,
    "text": "Modeling Spatial Dependencies for Mining Geospatial Data: An Introduction Spatial data mining is a process to discover interesting, potentially useful and high utility  patterns embedded in spatial databases. Efficient tools for extracting information from spatial  data sets can be of importance to organizations which own, generate and manage large spatial  data sets. The current approach towards solving spatial data mining problems is to use classical  data mining tools after \"materializing\" spatial relationships. However, the key property of  spatial data is that of spatial autocorrelation. Like temporal data, spatial data values are  influenced by values in their immediate vicinity. Ignoring spatial autocorrelation in the modeling  process leads to results which are a poor-fit and unreliable. In this chapter we will first review  spatial statistical techniques which explictly model spatial autocorrelation. Second, we will  propose PLUMS(Predicting Locations Using Map Similarity), a new approach for supervised  spatial data mining problems. PLUMS searches the space of solutions using a map-similarity  measure which is more appropriate in the context of spatial data. We will show that compared  to state-of-the-art spatial statistics approaches, PLUMS achives comparable accuracy but at  a fraction of the computational cost. Furthermore, PLUMS provides a general framework for  specializing other data mining techniques for mining spatial data.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 987,
    "label": 0,
    "text": "Statistical Modeling of Human Interactions In this paper we describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task. The system is particularly concerned with detecting when interactions between people occur, and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth.  Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach. We propose and compare two different state-based learning architectures, namely HMMs and CHMMs, for modeling behaviors and interactions. The CHMM model is shown to work much more efficiently and accurately.  Finally, a synthetic agent training system is used to develop a priori models for recognizing human behaviors and interactions. We demonstrate the ability to use these a priori models to accurately classify real human beha...",
    "neighbors": [
      497,
      665
    ],
    "mask": "Test"
  },
  {
    "node_id": 988,
    "label": 1,
    "text": "Bounded Explanation and Inductive Refinement For Acquiring Control Knowledge One approach to learning control knowledge  from a problem solving trace consists  of generating explanations for the local  decisions made during the search process.",
    "neighbors": [
      137,
      626
    ],
    "mask": "Train"
  },
  {
    "node_id": 989,
    "label": 4,
    "text": "ICrafter: A Service Framework for Ubiquitous Computing Environments In this paper, we propose ICrafter, a framework for services  and their user interfaces in a class of ubiquitous computing environments.",
    "neighbors": [
      764
    ],
    "mask": "Validation"
  },
  {
    "node_id": 990,
    "label": 2,
    "text": "Evaluating Strategies for Similarity Search on the Web Finding pages on the Web that are similar to a query page (Related Pages) is an important component of modern search engines. A variety of strategies have been proposed for answering Related Pages queries, but comparative evaluation by user studies is expensive, especially when large strategy spaces must be searched (e.g., when tuning parameters). We present a technique for automatically evaluating strategies using Web hierarchies, such as Open Directory, in place of user feedback. We apply this evaluation methodology to a mix of document representation strategies, including the use of text, anchor-text, and links. We discuss the relative advantages and disadvantages of the various approaches examined. Finally, we describe how to efficiently construct a similarity index out of our chosen strategies, and provide sample results from our index.",
    "neighbors": [
      536,
      638,
      1000,
      1017,
      1099
    ],
    "mask": "Train"
  },
  {
    "node_id": 991,
    "label": 5,
    "text": "A glimpse into the future of ID Cyberspace is a complex dimension of both enabling and inhibiting data flows in electronic data networks. Current generation intrusion detection (ID) systems are not technologically advanced enough to create the situational knowledge required to manage these networks. Next generation ID system will fuse data, combining both short-term sensor data with long-term knowledge databases, to create cyberspace situational awareness. This article offers a glimpse into the foggy crystal ball of future ID systems. Before diving into the technical discussion we ask the reader to keep in mind the generic model of a datagram traversing the Internet. Figure 1 illustrates an IP datagram moving in a store-and-forward environment from source to destination; routed based on a destination address with a uncertain source address decrementing the datagram time-to-live (TTL) at every router hop [1]. The datagram is routed through major Internets and IP transit providers. There is striking similarity between the transit of a datagram in the Internet and an airplane through airspace; future network management and air traffic control. At a very high abstract level, the concepts used to monitor objects in airspace apply to monitoring objects in networks. The Federal Aviation Administration (FAA) divides airspace management into two distinct entities. On the one hand, local controllers guide aircraft into and out of the air space surrounding an airport. Their job is to maintain awareness of the location of all aircraft in their vicinity, ensure proper separation, identify threats to aircraft, and manage the overall safety of passengers. Functionally, this is similar to the role of network controllers who must control the environment within their administrative domains. The network administrator must ensure the proper ports are open and the information is not delayed, the collisions are kept to a minimum, and the integrity of the delivery systems are not compromised. This is naturally similar to the situational awareness required in current generation air traffic control (ATC).",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 992,
    "label": 3,
    "text": "How to Avoid Building DataBlades That Know the Value of Everything and the Cost of Nothing The object-relational database management system (ORDBMS) offers many potential benefits for scientific, multimedia and financial applications. However, work remains in the integration of domain-specific class libraries (data cartridges, extenders, DataBlades  \u00ae  ) into ORDBMS query processing. A major problem is that the standard mechanisms for query selectivity estimation, taken from relational database systems, rely on properties specific to the standard data types; creation of new mechanisms remains extremely difficult because the software interfaces provided by vendors are relatively low-level. In this paper, we discuss extensions of the generalized search tree, or GiST, to support a higher-level but less type-specific approach. Specifically, we discuss the computation of selectivity estimates with confidence intervals using a variety of index-based approaches and present results from an experimental comparison of these methods with several estimators from the literature. 1. Intro...",
    "neighbors": [
      518,
      842,
      863
    ],
    "mask": "Test"
  },
  {
    "node_id": 993,
    "label": 3,
    "text": "Detection and Correction of Conflicting Concurrent Data Warehouse Updates Data integration over multiple heterogeneous data sources has become increasingly important for modern applications. The integrated data is usually stored in materialized views to allow better access, performance and high availability. Materialized view must be maintained after the data sources change. In a loosely-coupled environment, such as the Data Grid, the data sources are autonomous. Hence the source updates can be concurrent and cause erroneous maintenance results. State-of-the-art maintenance strategies apply compensating queries to correct such errors, making the restricting assumption that all source schemata remain static over time. However, in such dynamic environments, the data sources may change not only their data but also their schema, query capabilities or semantics. Consequently, either the maintenance queries or compensating queries would fail. We now propose a novel solution that handles both concurrent data and schema changes. First, we analyze the concurrency between source updates and classify them into different classes of dependencies. We then propose Dyno, a two-pronged strategy composed of dependency detection and correction algorithms to handle these new classes of concurrency. Our techniques are not tied to specific maintenance algorithms nor to a particular data model. To our knowledge, this is the first comprehensive solution to the view maintenance concurrency problems in loosely-coupled environments. Our experimental results illustrate that Dyno imposes an almost negligible overhead on existing maintenance algorithms for data updates while now allowing for this extended functionality.",
    "neighbors": [
      455,
      718
    ],
    "mask": "Train"
  },
  {
    "node_id": 994,
    "label": 1,
    "text": "Closing the Loop: an Agenda- and Justification-Based Framework for Selecting the Next Discovery Task to Perform We propose and evaluate an agenda- and justificationbased  architecture for discovery systems that contains  a mechanism for selecting the next task to perform.  This framework has many desirable properties: (1) its  use of heuristics to perform and propose tasks  facilitates the use of general discovery strategies that  are able to use a variety of background knowledge, (2)  through the use of justifications its mechanism for  selecting the next task to perform is able to reason  about the appropriateness of the tasks being  considered, and (3) its mechanism for selecting the  next task to perform also considers the users interests,  allowing a discovery program to tailor its behavior  toward them.  We evaluate the extent to which both reasons and  estimates of interestingness contribute to performance  in the domain of protein crystallization. With both  aspects contributing to task selection, a high fraction  of discoveries by the HAMB prototype were judged  interesting by an expert (21% interesting and novel;  45% interesting but rediscoveries).  1.",
    "neighbors": [
      954
    ],
    "mask": "Train"
  },
  {
    "node_id": 995,
    "label": 1,
    "text": "Flexibly Instructable Agents This paper presents an approach to learning from situated, interactive tutorial instruction within an ongoing agent. Tutorial instruction is a exible (and thus powerful) paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise. To support this exibility, however, the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions. Our approach, called situated explanation, achieves such learning through a combination of analytic and inductive techniques. It combines a form of explanation-based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations. The approach is implemented in an agent called Instructo-Soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions. Instructo-Soar meets three key requirements of exible instructability that distinguish it from previous systems: (1) it can take known or unknown commands at any instruction point; (2) it can handle instructions that apply to either its current situation or to a hypothetical situation speci ed in language (as in, for instance, conditional instructions); and (3) it can learn, from instructions, each class of knowledge it uses to perform tasks. 1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 996,
    "label": 0,
    "text": "A Scalable, Distributed Middleware Service Architecture to Support Mobile Internet Applications Middleware layers placed between user clients and application servers have been used to perform a variety of functions. In previous work we have used middleware to perform a new capability, application session handoff, using a single Middleware Server to provide all functionality. However, to improve the scalability of our architecture, we have designed an efficient distributed Middleware Service layer that properly maintains application session handoff semantics while being able to service a large number of clients. We show that this service layer improves the scalability of general clientto -application server interaction as well as the specific case of application session handoff. We detail protocols involved in performing handoff and analyse an implementation of the architecture that supports the use of a real medical teaching tool. From experimental results it can be seen that our Middleware Service effectively provides scalability as a response to increased workload.  1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 997,
    "label": 1,
    "text": "Spatial Cognition and Neuro-Mimetic Navigation: A Model of Hippocampal Place Cell Activity . A computational model of hippocampal activity during spatial cognition and navigation tasks is presented. The spatial representation in our model of the rat hippocampus is built on-line during exploration via two processing streams. An allothetic vision-based representation is built by unsupervised Hebbian learning extracting spatio-temporal properties of the environment from visual input. An idiothetic representation is learned based on internal movement-related information provided by path integration. On the level of the hippocampus, allothetic and idiothetic representations are integrated to yield a stable representation of the environment by a population of localized overlapping CA3-CA1 place fields. The hippocampal spatial representation is used as a basis for goal-oriented spatial behavior. We focus on the neural pathway connecting the hippocampus to the nucleus accumbens. Place cells drive a population of locomotor action neurons in the nucleus accumbens. Reward-based learnin...",
    "neighbors": [
      408
    ],
    "mask": "Train"
  },
  {
    "node_id": 998,
    "label": 3,
    "text": "Evaluating Top-k Queries over Web-Accessible Databases A query to a web search engine usually consists of a list of keywords, to which the search engine responds with the best or \u201ctop \u201d k pages for the query. This top-k query model is prevalent over multimedia collections in general, but also over plain relational data for certain applications. For example, consider a relation with information on available restaurants, including their location, price range for one diner, and overall food rating. A user who queries such a relation might simply specify the user\u2019s location and target price range, and expect in return the best 10 restaurants in terms of some combination of proximity to the user, closeness of match to the target price range, and overall food rating. Processing top-k queries efficiently is challenging for a number of reasons. One critical such reason is that, in many web applications, the relation attributes might not be available other than through external web-accessible form interfaces, which we will have to query repeatedly for a potentially large set of candidate objects. In this article, we study how to process top-k queries efficiently in this setting, where the attributes for which users specify target values might be handled by external, autonomous sources with a variety of access interfaces. We present a sequential algorithm for processing such queries, but observe that any sequential top-k query processing strategy is bound to require unnecessarily long query processing times, since web accesses exhibit high and variable latency. Fortunately, web sources can be probed in parallel, and each source can typically process concurrent requests, although sources may impose some restrictions on the type and number of probes that they are willing to accept. We adapt our sequential query processing technique and introduce an efficient algorithm that maximizes source-access parallelism to minimize query response time, while satisfying source-access constraints. We evaluate",
    "neighbors": [
      419
    ],
    "mask": "Test"
  },
  {
    "node_id": 999,
    "label": 4,
    "text": "Multimodal Interactions with Agents in Virtual Worlds Introduction  World Wide Web allows interactions and transactions through Web pages using speech and language, either by inanimate or live agents, image interpretation and generation, and, of course the more traditional ways of presenting explicitly predefined information of text, tables, figures, pictures, audio, animation and video. In a task- or domain-oriented way of interaction current technology allows the recognition and interpretation of rather natural speech and language in dialogues. However, rather than the current two-dimensional web-pages, many interesting parts of the Web will become three-dimensional, allowing the building of virtual worlds inhabited by user and task agents, with which the user can interact using different types of modalities, including speech and language interpretation and generation. Agents can work on behalf of users, hence, human computer interaction will make use of `indirect management', rather than interacting through direct manipulation of data",
    "neighbors": [
      1002
    ],
    "mask": "Test"
  },
  {
    "node_id": 1000,
    "label": 2,
    "text": "Automatic Resource list Compilation by Analyzing Hyperlink Structure and Associated Text We describe the design, prototyping and evaluation of ARC, a system for automatically compiling a list of authoritative web resources on any (sufficiently broad) topic. The goal of ARC is to compile resource lists similar to those provided by Yahoo! or Infoseek. The fundamental difference is that these services construct lists either manually or through a combination of human and automated effort, while ARC operates fully automatically. We describe the evaluation of ARC, Yahoo!, and Infoseek resource lists by a panel of human users. This evaluation suggests that the resources found by ARC frequently fare almost as well as, and sometimes better than, lists of resources that are manually compiled or classified into a topic. We also provide examples of ARC resource lists for the reader to examine.",
    "neighbors": [
      1,
      43,
      70,
      168,
      235,
      247,
      457,
      490,
      496,
      536,
      572,
      578,
      583,
      596,
      662,
      774,
      845,
      933,
      990,
      1005,
      1031,
      1059,
      1099,
      1104,
      1183,
      1228,
      1247
    ],
    "mask": "Train"
  },
  {
    "node_id": 1001,
    "label": 5,
    "text": "Towards a Comprehensive Topic Hierarchy for News To date, a comprehensive, Yahoo-like hierarchy of topics has yet to be offered for the domain of news. The Yahoo approach of managing such a hierarchy --- hiring editorial staff to read documents and correctly assign them to topics --- is simply not practical in the domain of news. Far too many stories are written and made available online everyday.  While many Machine Learning methods exist for organising documents into topics, these methods typically require a large number of labelled training examples before performing accurately. When managing a large and ever-changing topic hierarchy, it is unlikely that there would be enough time to provide many examples per topic. For this reason, it would be useful to identify extra information within the domain of news that could be harnessed to minimise the number of labelled examples required to achieve reasonable accuracy.  To this end, the notion of a semi-labelled document is introduced. These documents, which are partially labelled by th...",
    "neighbors": [
      341,
      865
    ],
    "mask": "Train"
  },
  {
    "node_id": 1002,
    "label": 4,
    "text": "Jacob project - Documentation The Jacob software system has been built as part of the Jacob project, which is a pilot project of the Virtual Reality Valley Twente initiative. The Jacob project investigates the application of virtual reality techniques and involves the design and construction of an animated agent in a 3-dimensional virtual environment. The project focuses on software engineering aspects, multimodal interaction, and the use of agent technology. In the current version of the Jacob system, an agent called Jacob teaches the user the Towers of Hanoi game; interaction takes place through natural language and manipulations of objects in the virtual environment. The purpose of this report is to provide information needed for further development of the Jacob system. It describes a number of technical details, including the file and directory structure, the software architecture, the event handling mechanism, and the integration of the dialogue management system.  3  Table of Contents 1 Introduction ...........",
    "neighbors": [
      999
    ],
    "mask": "Test"
  },
  {
    "node_id": 1003,
    "label": 2,
    "text": "Experiences with Selecting Search Engines using Meta-Search Search engines are among the most useful and high profile resources on the Internet. The problem of finding information on the Internet has been replaced with the problem of knowing where search engines are, what they are designed to retrieve and how to use them. This paper describes and evaluates SavvySearch, a meta-search engine designed to intelligently select and interface with multiple remote search engines. The primary meta-search issue examined is the importance of carefully selecting and ranking remote search engines for user queries. We studied the efficacy of SavvySearch's incrementally acquired meta-index approach to selecting search engines by analyzing the effect of time and experience on performance. We also compared the meta-index approach to the simpler categorical approach and showed how much experience is required to surpass the simple scheme.  1 Introduction  Search engines are powerful tools for assisting the otherwise unmanageable task of navigating the rapidly ex...",
    "neighbors": [
      115,
      404,
      433,
      435,
      496,
      510,
      526,
      627,
      1124,
      1253
    ],
    "mask": "Train"
  },
  {
    "node_id": 1004,
    "label": 3,
    "text": "A First-Order Approach to Unsupervised Learning . This paper deals with learning first-order logic rules from data lacking an explicit classification predicate. Consequently, the learned rules are not restricted to predicate definitions as in supervised Inductive Logic Programming. First-order logic offers the ability to deal with structured, multi-relational knowledge. Possible applications include first-order knowledge discovery, induction of integrity constraints in databases, multiple predicate learning, and learning mixed theories of predicate definitions and integrity constraints. One of the contributions of our work is a heuristic measure of confirmation, trading off satisfaction and novelty of the rule. The approach has been implemented in the Tertius system. The system performs an optimal best-first search, finding the k most confirmed hypotheses. It can be tuned to many different domains by setting its parameters, and it can deal either with individual-based representations as in propositional learning or with general logi...",
    "neighbors": [
      877
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1005,
    "label": 2,
    "text": "Background Readings for Collection Synthesis",
    "neighbors": [
      216,
      224,
      281,
      291,
      410,
      536,
      662,
      774,
      968,
      1000,
      1017,
      1099,
      1264
    ],
    "mask": "Train"
  },
  {
    "node_id": 1006,
    "label": 4,
    "text": "Advanced Interaction in Context . Mobile information appliances are increasingly used in numerous  different situations and locations, setting new requirements to their interaction  methods. When the user's situation, place or activity changes, the functionality  of the device should adapt to these changes. In this work we propose a layered  real-time architecture for this kind of context-aware adaptation based on  redundant collections of low-level sensors. Two kinds of sensors are  distinguished: physical and logical sensors, which give cues from environment  parameters and host information. A prototype board that consists of eight  sensors was built for experimentation. The contexts are derived from cues using  real-time recognition software, which was constructed after experiments with  Kohonen's Self-Organizing Maps and its variants. A personal digital assistant  (PDA) and a mobile phone were used with the prototype to demonstrate  situational awareness. On the PDA font size and backlight were changed  depending...",
    "neighbors": [
      99,
      307,
      460,
      769,
      910,
      1177
    ],
    "mask": "Train"
  },
  {
    "node_id": 1007,
    "label": 2,
    "text": "An Efficient Boosting Algorithm for Combining Preferences We study the problem of learning to accurately rank a set of objects by combining a given collection  of ranking or preference functions. This problem of combining preferences arises in several  applications, such as that of combining the results of different search engines, or the \"collaborativefiltering  \" problem of ranking movies for a user based on the movie rankings provided by other  users. In this work, we begin by presenting a formal framework for this general problem. We then  describe and analyze an efficient algorithm called RankBoost for combining preferences based on  the boosting approach to machine learning. We give theoretical results describing the algorithm's  behavior both on the training data, and on new test data not seen during training. We also describe  an efficient implementation of the algorithm for a particular restricted but common case. We next  discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment,  we used the algorithm to combine different web search strategies, each of which is a query  expansion for a given domain. The second experiment is a collaborative-filtering task for making  movie recommendations.",
    "neighbors": [
      143,
      596,
      674,
      951,
      1068,
      1141
    ],
    "mask": "Test"
  },
  {
    "node_id": 1008,
    "label": 0,
    "text": "Behavior Coordination Mechanisms - State-of-the-art In behavior-based robotics the control of a robot is shared between a set of purposive perception-action units, called behaviors. Based on selective sensory information, each behavior produces immediate reactions to control the robot with respect to a particular objective, i.e., a narrow aspect of the robot's overall task such as obstacle avoidance or wall following. Behaviors with di erent and possibly incommensurable objectives may produce con icting actions that are seemingly irreconcilable. Thus a major issue in the design of behavior-based control systems is the formulation of e ective mechanisms for coordination of the behaviors' activities into strategies for rational and coherent behavior. This is known as the action selection problem (also refereed to as the behavior coordination problem) and is the primary focus of this overview paper. Numerous action selection mechanisms have been proposed over the last decade and the main objective of this document istogive a qualitative overview of these approaches. 2 1",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1009,
    "label": 3,
    "text": "Backtracking Algorithms for Disjunctions of Temporal Constraints We extend the framework of simple temporal problems studied originally by Dechter, Meiri and Pearl to consider constraints of the form x1 \\Gamma y1  r1  : : :  xn \\Gamma  yn  rn , where x1 : : : xn ; y1 : : : yn are variables ranging over the real numbers, r1 : : : rn are real constants, and n  1. We have implemented four progressively more efficient algorithms for the consistency checking problem for this class of temporal constraints. We have partially ordered those algorithms according to the number of visited search nodes and the number of performed consistency checks. Finally, we have carried out a series of experimental results on the location of the hard region. The results show that hard problems occur at a critical value of the ratio of disjunctions to variables. This value is between 6 and 7. Introduction  Reasoning with temporal constraints has been a hot research topic for the last fifteen years. The importance of this problem has been demonstrated in many areas of artifici...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1010,
    "label": 2,
    "text": "Stylistic Experiments For Information Retrieval . A discussion on various experiments to utilize stylistic variation among texts for information retrieval purposes. 1. Stylistics  Texts vary in many ways. Authors make choices when they write a text: they decide how to organize the material they have planned to introduce; they make choices between synonyms and syntactic constructions; they choose an intended audience for the text. Authors will make these choices in various ways and for various reasons: based on personal preferences, on their view of the reader, and on what they know and like about other similar texts.  A style is a consistent and distinguishable tendency to make some of these linguistic choices. Style is, on a surface level, very obviously detectable as the choice between items in a vocabulary, between types of syntactical constructions, between the various ways a text can be woven from the material it is made of. It is the information carried in a text when compared to other texts, or in a sense compared to language...",
    "neighbors": [
      298,
      903
    ],
    "mask": "Train"
  },
  {
    "node_id": 1011,
    "label": 3,
    "text": "Generalizing Temporal Dependencies for Non-Temporal Dimensions Recently, there has been a lot of interest in temporal granularity, and its applications in temporal dependency  theory and data mining. Generalization hierarchies used in multi-dimensional databases and OLAP serve a role  similar to that of time granularity in temporal databases, but they also apply to non-temporal dimensions, like  space.",
    "neighbors": [
      1237
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1012,
    "label": 3,
    "text": "Indexing Moving Points We propose three indexing schemes for storing a set S of N points in the plane, each moving  along a linear trajectory, so that a query of the following form can be answered quickly: Given  a rectangle R and a real value t q , report all K points of S that lie inside R at time t q . We  first present an indexing structure that, for any given constant \" ? 0, uses O(N=B) disk blocks,  where B is the block size, and answers a query in O((N=B)  1=2+\"  + K=B) I/Os. It can also  report all the points of S that lie inside R during a given time interval. A point can be inserted  or deleted, or the trajectory of a point can be changed, in O(log  2  B N) I/Os. Next, we present a  general approach that improves the query time if the queries arrive in chronological order, by  allowing the index to evolve over time. We obtain a tradeoff between the query time and the  number of times the index needs to be updated as the points move. We also describe an indexing  scheme in which the number of I/Os required to answer a query depends monotonically on the  difference between t q and the current time. Finally, we develop an efficient indexing scheme to  answer approximate nearest-neighbor queries among moving points.  An extended abstract of this paper appeared in the Proceedings of the 19th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems.  y  Center for Geometric Computing, Department of Computer Science, Duke University, Box 90129, Durham, NC 27708--0129; pankaj@cs.duke.edu; http://www.cs.duke.edu/  pankaj. Supported in part by National Science Foundation grants EIA--9870734, EIA--9972879, and CCR--9732787, by Army Research Of fice MURI grant DAAH04-- 96--1--0013, by a Sloan fellowship, and by a grant from the U.S.-Israeli Binational Science Foundation.  z  Center ...",
    "neighbors": [
      27,
      56,
      151,
      458,
      544
    ],
    "mask": "Train"
  },
  {
    "node_id": 1013,
    "label": 1,
    "text": "Evolving Materialized Views in Data Warehouse A data warehouse contains multiple views accessed by queries. One of the most important decisions in designing a data warehouse is the selection of materialized views for the purpose of efficiently implementing decision making. The search space for the selection of materialized views is exponentially large, therefore, heuristics have been used to search a small fraction of the space to get a near optimal solution. In this paper, we explore the use of a genetic algorithm for the selection of materialized views based on multiple global processing plans for many queries. Our experimental studies indicate that the genetic algorithm delivers better solutions than some heuristics. 1 Introduction  A Data Warehouse (DW) is a repository of integrated information available for querying and analysis. DW is an approach to the integration of data from multiple, possibly very large, distributed, heterogeneous databases and other information sources. In this paper, DW data model is based on SPJ (Sele...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1014,
    "label": 5,
    "text": "A hybrid projection based and radial basis function architecture: Initial values and global optimization We introduce a hybrid architecture of projection based units and radial basis functions  as a general function estimation scheme. In particular, we introduce an optimization  scheme which has several steps and assures a convergence to a useful solution.  During training a determination whether a Gaussian unit should be removed is applied.  This results in a final architecture with much smaller number of units. The  proposed global constrained optimization does not lead to overfitting which happens  when the RBF width becomes too small, this is achieved by a regularization mechanism.  Classification and regression results are demonstrated on various benchmark data  sets and compared with several variants of RBF networks. The most striking performance  improvement is achieved on the vowel data set [5].  Keywords: Projection units, RBF Units, Hybrid Network Architecture, SMLP, Clustering, Regularization.  1 Introduction  The duality between projection-based approximation and radial kernel...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1015,
    "label": 3,
    "text": "A Data Model and Algebra for Probabilistic Complex Values . We present a probabilistic data model for complex values. More precisely, we introduce  probabilistic complex value relations, which combine the concept of probabilistic relations with  the idea of complex values in a uniform framework. We elaborate a model-theoretic definition of  probabilistic combination strategies, which has a rigorous foundation on probability theory. We  then define an algebra for querying database instances, which comprises the operations of selection,  projection, renaming, join, Cartesian product, union, intersection, and difference. We prove that  our data model and algebra for probabilistic complex values generalizes the classical relational data  model and algebra. Moreover, we show that under certain assumptions, all our algebraic operations  are tractable. We finally show that most of the query equivalences of classical relational algebra carry  over to our algebra on probabilistic complex value relations. Hence, query optimization techniques  for class...",
    "neighbors": [
      237
    ],
    "mask": "Test"
  },
  {
    "node_id": 1016,
    "label": 1,
    "text": "Intelligent Data Analysis in Medicine Extensive amounts of knowledge and data stored in medical databases require the  development of specialized tools for storing and accessing of data, data analysis, and effective  use of stored knowledge and data. This paper focuses on methods and tools for  intelligent data analysis, aimed at narrowing the increasing gap between data gathering  and data comprehension. The paper sketches the history of research that led to the  development of current intelligent data analysis techniques, discusses the need for intelligent  data analysis in medicine, and proposes a classification of intelligent data analysis  methods. The scope of the paper covers temporal data abstraction methods and data  mining methods. A selection of methods is presented and illustrated in medical problem  domains. Presently data abstraction and data mining are attracting considerable research  interest. However the two technologies, in spite of the fact that they share their central  objective, namely the intelligen...",
    "neighbors": [
      344
    ],
    "mask": "Train"
  },
  {
    "node_id": 1017,
    "label": 2,
    "text": "The Anatomy of a Large-Scale Hypertextual Web Search Engine In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/  To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date.  Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.",
    "neighbors": [
      1,
      43,
      70,
      112,
      115,
      168,
      219,
      233,
      235,
      247,
      255,
      322,
      411,
      427,
      447,
      490,
      496,
      510,
      526,
      536,
      572,
      578,
      583,
      587,
      596,
      638,
      649,
      653,
      751,
      763,
      819,
      845,
      859,
      867,
      892,
      923,
      933,
      951,
      968,
      990,
      1005,
      1031,
      1056,
      1096,
      1098,
      1099,
      1104,
      1124,
      1134,
      1183,
      1189,
      1228
    ],
    "mask": "Train"
  },
  {
    "node_id": 1018,
    "label": 3,
    "text": "Dynamic Pipeline Scheduling for Improving Interactive Query Performance Interactive query performance is becoming an  important criterion for online systems where  delivering query results in a timely fashion  is critical. Pipelined execution is a promising  query execution style that can produce  the initial portion of the result early and in  a continuous fashion. In this paper we propose  techniques for delivering results faster in  a pipelined query plan. We distinguish between  two cases. For cases where the tuples  in the query result are of the same importance  we propose a dynamic rate-based  pipeline scheduling policy that produces more  results during the early stages of query execution.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1019,
    "label": 3,
    "text": "Automatic Reclustering of Objects in Very Large Databases for High Energy Physics In the very large object database systems planned for some future particle physics experiments, typical physics analysis jobs will traverse millions of read-only objects, many more objects than fit in the database cache. Thus, a good clustering of objects on disk is highly critical to database performance. We present the implementation and performance measurements of a prototype reclustering mechanism which was developed to optimise I/O performance under the changing access patterns in a high energy physics database. Reclustering is done automatically and on-line. The methods used by our prototype differ greatly from those commonly found in proposed general-purpose reclustering systems. By exploiting some special characteristics of the access patterns of physics analysis jobs, the prototype manages to keep database I/O throughput close to the optimum throughput of raw sequential disk access.  Keywords: Object oriented databases, object clustering, object reclustering, automatic reclust...",
    "neighbors": [
      68,
      232,
      562
    ],
    "mask": "Train"
  },
  {
    "node_id": 1020,
    "label": 3,
    "text": "A Framework for Designing and Implementing the User Interface of a Geographic Digital Library . Geographic data are useful for a large set of applications, such as urban planning and environmental control. These data are, however, very expensive to acquire and maintain. Moreover, their use is often restricted due to a lack of dissemination mechanisms. Digital libraries are a good approach for increasing data availability and therefore reducing costs, since they provide e#cient storage and access to large volumes of data. One major drawback to this approach is that it creates the necessity of providing facilities for a large and heterogeneous community of users to search and interact with these geographic libraries. We present a solution to this problem, based on a framework that allows the design and construction of customizable user interfaces for applications based on Geographic Digital Libraries (GDL). This framework relies on two main concepts: a geographic user interface architecture and a geographic digital library model.  Key words: Digital libraries -- User interfaces -...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1021,
    "label": 4,
    "text": "OZONE: A Zoomable Interface for Navigating Ontology Information We present OZONE (Zoomable Ontology Navigator), for searching and browsing ontological  information. OZONE visualizes query conditions and provides interactive, guided browsing for  DAML (DARPA Agent Markup Language) ontologies. To visually represent objects in DAML, we  define a visual model for its classes, properties and relationships between them. Properties can be  expanded into classes for query refinement. The visual query can be formulated incrementally as  users explore class and property structures interactively. Zoomable interface techniques are  employed for effective navigation and usability.  Keywords: Ontology, DAML, Browsing, Zoomable User Interface (ZUI), Jazz, WWW.",
    "neighbors": [
      347,
      450
    ],
    "mask": "Test"
  },
  {
    "node_id": 1022,
    "label": 3,
    "text": "Architecture and Quality in Data Warehouses: an Extended Repository Approach This paper makes two",
    "neighbors": [
      225
    ],
    "mask": "Train"
  },
  {
    "node_id": 1023,
    "label": 4,
    "text": "Integrating Sounds and Motions in Virtual Environments Sounds are often the result of motions of virtual objects in a virtual environment. Therefore, sounds and the motions that caused them should be treated in an integrated way. When sounds and motions do not have the proper correspondence, the resultant confusion can lessen the effects of each. In this paper, we present an integrated system for modeling, synchronizing, and rendering sounds for virtual environments. The key idea of the system is the use of a functional representation of sounds, called timbre trees. This representation is used to model sounds that are parameterizable. These parameters can then be mapped to the parameters associated with the motions of objects in the environment. This mapping allows the correspondence of motions and sounds in the environment. Representing arbitrary sounds using timbre trees is a difficult process that we do not address in this paper. We describe approaches for creating some timbre trees including the use of genetic algorithms. Rendering the...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1024,
    "label": 2,
    "text": "Maximum Entropy Markov Models for Information Extraction and Segmentation Hidden Markov models (HMMs) are a powerful  probabilistic tool for modeling sequential data,  and have been applied with success to many  text-related tasks, such as part-of-speech tagging,  text segmentation and information extraction. In  these cases, the observations are usually modeled  as multinomial distributions over a discrete  vocabulary, and the HMM parameters are set  to maximize the likelihood of the observations.  This paper presents a new Markovian sequence  model, closely related to HMMs, that allows observations  to be represented as arbitrary overlapping  features (such as word, capitalization, formatting,  part-of-speech), and defines the conditional  probability of state sequences given observation  sequences. It does this by using the  maximum entropy framework to fit a set of exponential  models that represent the probability of a  state given an observation and the previous state.  We present positive experimental results on the  segmentation of FAQ's.  1. Introdu...",
    "neighbors": [
      875,
      956,
      1082,
      1122
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1025,
    "label": 3,
    "text": "A Comparative Study of Version Management Schemes for XML Documents The problem of managing multiple versions for XML documents and semistructured data is of significant  interest in many DB applications and web-related services. Traditional document version control  schemes, such as RCS, suffer from the following two problems. At the logical level, they conceal the  structure of the documents by modeling them as sequences of text lines, and storing a document's evolution  as a line-edit script. At the physical level, they can incur in severe storage or processing costs  because of their inability to trade-off storage with computation. To solve these problems, we propose  version management strategies that preserve the structure of the original document, and apply and extend  DB techniques to minimize storage and processing costs. Therefore, we propose and compare three  schemes for XML version management, namely, the Usefulness-Based Copy Control, the Multiversion  B-Tree, and the Partially Persistent List Method. A common characteristic of these schemes is that  they cluster data using the notion of page usefulness, which by selectively copying current information  from obsolete pages provides for fast version reconstruction with minimal storage overhead. The cost  and performance of these version management schemes are evaluated and compared through extensive  analysis and experimentation.",
    "neighbors": [
      336,
      402,
      889
    ],
    "mask": "Train"
  },
  {
    "node_id": 1026,
    "label": 3,
    "text": "FeedbackBypass: A New Approach to Interactive Similarity Query Processing In recent years, several methods have been  proposed for implementing interactive similarity  queries on multimedia databases. Common to all  these methods is the idea to exploit user feedback  in order to progressively adjust the query parameters  and to eventually converge to an \"optimal\" parameter  setting. However, all these methods also  share the drawback to \"forget\" user preferences  across multiple query sessions, thus requiring the  feedback loop to be restarted for every new query,  i.e. using default parameter values. Not only is  this proceeding frustrating from the user's point  of view but it also constitutes a significant waste  of system resources.  In this paper we present FeedbackBypass, a new  approach to interactive similarity query processing.  It complements the role of relevance feedback  engines by storing and maintaining the query  parameters determined with feedback loops over  time, using a wavelet-based data structure (the  Simplex Tree). For each query, a favorable set of  query parameters can be determined and used to  either \"bypass\" the feedback loop completely for  already-seen queries, or to start the search process  from a near-optimal configuration.  FeedbackBypass can be combined well with  all state-of-the-art relevance feedback techniques  working in high-dimensional vector spaces. Its  storage requirements scale linearly with the dimensionality  of the query space, thus making even  sophisticated query spaces amenable. Experimen-  Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large ...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1027,
    "label": 4,
    "text": "Making Context Explicit in Communicating Objects INTRODUCTION  One can speak about context only with reference to something (no definition of context out of context): the context of an object, the context of interaction, the context of a problem solving, etc. However, only the context of interaction between agents seems of interest because it is in this context that other contexts are referenced or evolve. For example, if an object, as a telephone, could provide you the context in which the called person is (free, in meeting, phone on voice recorder), you could balance your wish to establish your communication versus the availability of the called person.  Several domains have already elaborated their own working definition of context. In human-machine interaction, a context is a set of information that could be used to define and interpret a situation in which interact agents. In the context-aware applications community, the context is composed of a set of information for characterizing the situation in which interact humans, applic",
    "neighbors": [
      785
    ],
    "mask": "Train"
  },
  {
    "node_id": 1028,
    "label": 0,
    "text": "Artificial Agents and Logic Programming . Artificial agents represent a new paradigm in software engineering and Artificial Intelligence. As complex software-controlled systems they are capable of flexible autonomous behavior in dynamic and unpredictable environments. Over the past few years, researchers in computer science have begun to recognise that the technology of artificial agents provides the key to solving many problems in distributed computing and intelligent control, for which traditional software engineering techniques offer no solution. The field of logic programming includes many important concepts, such as declarativity, unification, meta-logic programming, and deduction rules, from which the new technology of multiagent systems can benefit. 1 Introduction Although the idea of agent systems is intuitively appealing, and there are a number of implemented systems that claim to realize this popular idea, the basic concepts underlying these systems are often not well-understood, and no attempt is made to define t...",
    "neighbors": [
      663,
      1222
    ],
    "mask": "Train"
  },
  {
    "node_id": 1029,
    "label": 4,
    "text": "Towards Proximity Group Communication Group communication will undoubtedly be a useful paradigm for many applications of wireless networking in which reliability and timeliness are requirements. Moreover, location-awareness is clearly central to mobile applications such as traffic management and smart spaces. In this paper we introduce our definition of proximity groups in which group membership depends on location and then discuss some requirements for a group management service suitable for proximity groups.",
    "neighbors": [
      21,
      737
    ],
    "mask": "Train"
  },
  {
    "node_id": 1030,
    "label": 0,
    "text": "Incremental Reinforcement Learning for designing Multi-Agent Systems Designing individual agents so that, when put together, they reach a given global goal is not an easy task. One solution to automatically build such large Multi-Agent Systems is to use decentralized learning: each agent learns by itself its own behavior. To that purpose, Reinforcement Learning methods are very attractive as they do not require a solution of the problem to be known before hand. Nevertheless, many hard points need to be solved for such a learning process to be viable. Among others, the credit assignement problem, combinatorial explosion and local perception of the world seem the most crucial and prevent optimal behavior. In this paper, we propose a framework based on a gradual learning of harder and harder tasks until the desired global behavior is reached. The applicability of our paradigm is tested on computer experiments where many agents have to coordinate to reach a global goal. Our results show that incremental learning leads to better performances than more classical techniques. We then discuss several improvements which could lead to even better performances.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1031,
    "label": 2,
    "text": "Information Retrieval on the Web In this paper we review studies on the growth of the Internet and technologies which are useful for information search and retrieval on the Web. We present data on the Internet from several dierent sources, e.g., current as well as projected number of users, hosts and Web sites. Although numerical gures vary, overall trends cited by the sources are consistent and point to exponential growth in the past and in the coming decade. As such, it is not surprising that about 85% of Internet users surveyed claim to be using search engines and search services to nd speci c information of interest. The same surveys show, however, that users are not satis ed with the performance of the current generation of search engines; the slow speed of retrieval, communication delays, and poor quality of retrieved results (e.g., noise and broken links) are commonly cited problems. We discuss the development of new techniques which are targeted to resolve some of the problems associated with Web-...",
    "neighbors": [
      219,
      224,
      382,
      587,
      897,
      1000,
      1017
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1032,
    "label": 2,
    "text": "Grouper: A Dynamic Clustering Interface to Web Search Results Users of Web search engines are often forced to sift through the long ordered list of document \"snippets\" returned by the engines. The IR community has explored document clustering as an alternative method of organizing retrieval results, but clustering has yet to be deployed on most major search engines. The NorthernLight search engine organizes its output into \"custom folders\" based on pre-computed document labels, but does not reveal how the folders are generated or how well they correspond to users' interests.  In this paper, we introduce Grouper -- an interface to the results of the HuskySearch meta-search engine, which dynamically groups the search results into clusters labeled by phrases extracted from the snippets. In addition, we report on the first empirical comparison of user Web search behavior on a standard ranked-list presentation versus a clustered presentation. By analyzing HuskySearch logs, we are able to demonstrate substantial differences in the number of documents f...",
    "neighbors": [
      84,
      168,
      494,
      1099
    ],
    "mask": "Train"
  },
  {
    "node_id": 1033,
    "label": 0,
    "text": "Task-Oriented Dialogs with Animated Agents in Virtual Reality We are working towards animated agents that can carry on tutorial, task-oriented dialogs with human students. The agent's objective is to help students learn to perform physical, procedural tasks, such as operating and maintaining equipment. Although most research on such dialogs has focused on verbal communication, nonverbal communication can play many important roles as well. To allow a wide variety of interactions, the student and our agent cohabit a threedimensional, interactive, simulated mock-up of the student's work environment. The agent, Steve, can generate and recognize speech, demonstrate actions, use gaze and gestures, answer questions, adapt domain procedures to unexpected events, and remember past actions. This paper focuses on Steve's methods for generating multi-modal behavior, contrasting our work with prior work in task-oriented dialogs, multimodal explanation generation, and animated conversational characters. Introduction  We are working towards animated agents that...",
    "neighbors": [
      1191
    ],
    "mask": "Train"
  },
  {
    "node_id": 1034,
    "label": 1,
    "text": "Introspective Multistrategy Learning: Constructing a Learnung Strategy under Reasoning Failure Officer  praised dog  for barking at  object.\"  Enables  Detect  Drugs out FK  Initiates  Retrieval    5  6  Missing  Figure 10. Forgetting to fill the tank with gas  A=actual intention; E=expectation; Q=question; C=context; I=index; G=goal  Tank  Out of  Gas  Tank  Full  Tank  Low  Fill Tank  Should have filled  up with gas when tank low Expectation  What  Action  to Do?    KEY: G = goal; I = index;  C = context; Q = question;  E = expectation; A = actual intention  Results  At  Store  connections with related concepts. Other learning goals take multiple arguments. For instance, a knowledge differentiation goal (Cox & Ram, 1995) is a goal to determine a change in a body of knowledge such that two items are separated conceptually. In contrast, a knowledge reconciliation goal (Cox & Ram, 1995) is one that seeks to merge two items that were mistakenly considered separate entities. Both expansion goals and reconciliation goals may include or spawn a knowledge organization goal (Ram, 1993) that seeks to reorganize the existing knowledge so that it is made available to the reasoner at the appropriate time, as well as modify the structure or content of a concept itself. Such reorganization of knowledge affects the conditions under which a particular piece of knowledge is retrieved or the kinds of indexes associated with an item in memory.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1035,
    "label": 5,
    "text": "Learning Maps for Indoor Mobile Robot Navigation Autonomous robots must be able to learn and maintain models of their environments. Research on mobile robot navigation has produced two major paradigms for mapping indoor environments: grid-based and topological. While grid-based methods produce accurate metric maps, their complexity often prohibits efficient planning and problem solving in large-scale indoor environments. Topological maps, on the other hand, can be used much more efficiently, yet accurate and consistent topological maps are often difficult to learn and maintain in large-scale environments, particularly if momentary sensor data is highly ambiguous. This paper describes an approach that integrates both paradigms: grid-based and topological. Grid-based maps are learned using artificial neural networks and naive Bayesian integration. Topological maps are generated on top of the grid-based maps, by partitioning the latter into coherent regions. By combining both paradigms, the approach presented here gains advantages from both worlds: accuracy/consistency and efficiency. The paper gives results for autonomous exploration, mapping and operation of a mobile robot in populated multi-room environments.",
    "neighbors": [
      1194
    ],
    "mask": "Test"
  },
  {
    "node_id": 1036,
    "label": 4,
    "text": "A Wearable Computer System with Augmented Reality to Support Terrestrial Navigation To date augmented realities are typically operated in only a small defined area, in the order of a large room. This paper reports on our investigation into expanding augmented realities to outdoor environments. The project entails providing visual navigation aids to users. Awearable computer system with a see-through display, digital compass, and a differential GPS are used to provide visual cues while performing a standard orienteering task. This paper reports the outcomes of a set of trials using an off the shelf wearable computer, equipped with a custom built navigation software package, \"map-in-the-hat.\"",
    "neighbors": [
      886,
      1043
    ],
    "mask": "Test"
  },
  {
    "node_id": 1037,
    "label": 3,
    "text": "Kaleidoquery: A Visual Query Language for Object Databases In this paper we describe Kaleidoquery, a visual query language for object databases with the same expressive power as OQL. We will describe the design philosophy behind the filter flow nature of Kaleidoquery and present each of the language's constructs, giving examples and relating them to OQL. The Kaleidoquery language is described independent of any implementation details, but a brief description of a 3D interface currently under construction for Kaleidoquery is presented. The queries in this implementation of the language are translated into OQL and then passed to the object database O 2 for evaluation.  KEYWORDS: Visual query language, OQL, object databases, three-dimensional interface.  INTRODUCTION  The lack of a generally accepted and widely supported query language has probably had a significant effect in slowing the uptake of early commercial object-oriented databases. However, the emergence of the Object Query Language (OQL) which is being standardised by the Object Databas...",
    "neighbors": [
      907
    ],
    "mask": "Test"
  },
  {
    "node_id": 1038,
    "label": 0,
    "text": "Mobile Agents in Distributed Information Retrieval A mobile agent is an executing program that can migrate during execution from machine to machine in a heterogeneous network. On each machine, the agent interacts with stationary service agents and other resources to accomplish its task. Mobile agents are particularly attractive in distributed informationretrieval applications. By moving to the location of an information resource, the agent can search the resource locally, eliminating the transfer of intermediate results across the network and reducing end-toend latency. In this chapter, we rst discuss the strengths of mobile agents, and argue that although none of these strengths are unique to mobile agents, no competing technique shares all of them. Next, after surveying several representative mobile-agent systems, we examine one speci c information-retrieval application, searching distributed collections of technical reports, and consider how mobile agents can be used to implement this application e ciently and easily. Then we spend the bulk of the chapter describing two planning services that allow mobile agents to deal with dynamic network environments and information resources: (1) planning algorithms that let an agent choose the best migration path through the network, given its current task and the current network conditions, and (2) planning algorithms that tell an agent howto observe achanging set of documents in a way that detects changes as soon as possible while minimizing overhead. Finally, we consider the types of errors that can occur when information from multiple sources is merged and ltered, and argue that the structure of a mobile-agent application determines the extent to which these errors a ect the nal result. 1",
    "neighbors": [
      38,
      415,
      1227
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1039,
    "label": 1,
    "text": "Will We Have a Wet Summer? Soft Computing Models for Long-term Rainfall Forecasting Long-term rainfall prediction is very important to countries thriving on agro-based economy. In general, climate and rainfall are highly non-linear phenomena in nature giving rise to what is known as \"butterfly effect\". The parameters that are required to predict the rainfall are enormously complex and subtle so that uncertainty in a prediction using all these parameters is enormous even for a short period. Soft computing is an innovative approach to construct computationally intelligent systems that are supposed to possess humanlike expertise within a specific domain, adapt themselves and learn to do better in changing environments, and explain how they make decisions. Unlike conventional artificial intelligence techniques the guiding principle of soft computing is to exploit tolerance for imprecision, uncertainty, robustness, partial truth to achieve tractability, and better rapport with reality (Zadeh 1998). In this paper, we analysed 87 years of rainfall data in Kerala state, the southern part of Indian Peninsula situated at latitude-longitude pairs (8029 ' N - 76057 ' E). We attempted to train 5 soft computing based prediction models with 40 years of rainfall data. For performance evaluation, network predicted outputs were compared with the actual rainfall data. Simulation results reveal that soft computing techniques are promising and efficient.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1040,
    "label": 1,
    "text": "Classification on Pairwise Proximity Data We investigate the problem of learning a classification task on data represented in terms of their pairwise proximities. This representation does not refer to an explicit feature representation of the data items and is thus more general than the standard approach of using Euclidean feature vectors, from which pairwise proximities can always be calculated. Our first approach is based on a combined linear embedding and classification procedure resulting in an extension of the Optimal Hyperplane algorithm to pseudo-Euclidean data. As an alternative we present another approach based on a linear threshold model in the proximity values themselves, which is optimized using Structural Risk Minimization. We show that prior knowledge about the problem can be incorporated by the choice of distance measures and examine different metrics w.r.t. their generalization. Finally, the algorithms are successfully applied to protein structure data and to data from the cat's cerebral cortex. They show bette...",
    "neighbors": [
      139
    ],
    "mask": "Train"
  },
  {
    "node_id": 1041,
    "label": 5,
    "text": "Variorum: A Multimedia-Based Program Documentation System Conventional software documentation systems are mostly based on textutal descriptions that explain or annotate the program's source code. Typically they also support interactive browsing of high-level control flows, and name-based searching of program primitives such as variable declarations and function definitions. Because these systems rely solely on texts, it is difficult for program authors to describe overall algorithm structures and detailed implementation considerations of the programs in an interactive and flexible fashion. Variorum is a novel software documentation system that allows program authors to record the process of \"walking through\" their own code using multimedia technology, specifically, text, audio, and digital pen drawing. This approach greatly improves the interactivity and flexibility in the software documentation process. In addition, to broaden its applicability and to reduce the implementation complexity,  Variorum is designed to inter-operate with the WWW t...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1042,
    "label": 4,
    "text": "Rotating Virtual Objects with Real Handles Times for virtual object rotations reported in the literature are of the order of ten seconds or more and this is far longer than it takes to manually orient a \"real\" object, such as a cup. This is a report of a series of experiments designed to investigate the reasons for this difference and to help design interfaces for object manipulation. The results suggest that two major factors are important. Having the hand physically in the same location as the virtual object being manipulated is one. The other is based on whether the object is being rotated to a new, randomly determined orientation, or is always rotated to the same position. Making the object held in the hand have the same physical shape as the object being visually manipulated was not found to be a significant factor. The results are discussed in the context of interactive virtual environments. Categories and Subject Descriptors: H.1.2 [Models and Principles]: User/Machine Systems -- human factors; I.3.6 [Computer Graphics]:...",
    "neighbors": [
      444
    ],
    "mask": "Train"
  },
  {
    "node_id": 1043,
    "label": 4,
    "text": "Exploring MARS: Developing Indoor and Outdoor User Interfaces to a Mobile Augmented Reality System We describe an experimental mobile augmented reality system (MARS) testbed that employs different user interfaces to allow outdoor and indoor users to access and manage information that is spatially registered with the real world. Outdoor users can experience spatialized multimedia presentations that are presented on a head-tracked, see-through, head-worn display used in conjunction with a hand-held pen-based computer. Indoor users can get an overview of the outdoor scene and communicate with outdoor users through a desktop user interface or a head- and hand-tracked immersive augmented reality user interface.  Key words: Augmented Reality. Wearable Computing. Mobile Computing. Hypermedia.  GPS.  1 Introduction  As computers increase in power and decrease in size, new mobile and wearable computing applications are rapidly becoming feasible, promising users access to online resources always and everywhere. This new flexibility makes possible a new kind of application---one that exploits ...",
    "neighbors": [
      75,
      215,
      680,
      886,
      1036
    ],
    "mask": "Train"
  },
  {
    "node_id": 1044,
    "label": 1,
    "text": "Prediction with Local Patterns using Cross-Entropy Sets of local patterns in the forms of rules and co-occurrence counts are produced by many data mining methods such as association rule algorithms. While such patterns can yield useful insights it is not obvious how to synthesize local sparse information into a coherent global predictive model. We study the use of a cross-entropy approach to combining local patterns. Each local pattern is viewed as a constraint on an appropriate high-order joint distribution of interest. Typically, a set of patterns returned by a data mining algorithm under-constrains the high-order model. The cross-entropy criterion is used to select a specific distribution in this constrained family relative to a prior. We review the iterative-scaling algorithm which is an iterative technique for finding a joint distribution given constraints. We then illustrate the application of this method to two specific problems. The first problem is combining information about frequent itemsets. We show that the cross-entropy a...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1045,
    "label": 1,
    "text": "A Cognitive Bias Approach to Feature Selection and Weighting for Case-Based Learners . Research in psychology, psycholinguistics, and cognitive science has discovered and examined numerous psychological constraints on human information processing. Short term memory limitations, a focus of attention bias, and a preference for the use of temporally recent information are three examples. This paper shows that psychological constraints such as these can be used e#ectively as domain-independent sources of bias to guide feature set selection and weighting for case-based learning algorithms.  We first show that cognitive biases can be automatically and explicitly encoded into the baseline instance representation: each bias modifies the representation by changing features, deleting features, or modifying feature weights. Next, we investigate the related problems of cognitive bias selection and cognitive bias interaction for the feature weighting approach. In particular, we compare two cross-validation algorithms for bias selection that make di#erent assumptions about the indep...",
    "neighbors": [
      1075,
      1135
    ],
    "mask": "Test"
  },
  {
    "node_id": 1046,
    "label": 4,
    "text": "TouchCounters: Designing Interactive Electronic Labels for Physical Containers We present TouchCounters, an integrated system of electronic modules, physical storage containers, and shelving surfaces for the support of collaborative physical work. Through physical sensors and local displays, TouchCounters record and display usage history information upon physical storage containers, thus allowing access to this information during the performance of real-world tasks. A distributed communications network allows this data to be exchanged with a server, such that users can access this information from remote locations as well.  Based upon prior work in ubiquitous computing and tangible interfaces, TouchCounters incorporate new techniques, including usage history tracking for physical objects and multi-display visualization. This paper describes the components, interactions, implementation, and conceptual approach of the TouchCounters system.  Keywords  Tangible interfaces, ubiquitous computing, distributed sensing, visualization  INTRODUCTION  For decades, research i...",
    "neighbors": [
      710
    ],
    "mask": "Train"
  },
  {
    "node_id": 1047,
    "label": 3,
    "text": "Query Unnesting in Object-Oriented Databases There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way. This paper presents a new query unnesting algorithm that generalizes many unnesting techniques proposed recently in the literature. Our system is capable of removing  any form of query nesting using a very simple and efficient algorithm. The simplicity of the system is due to the use of the monoid comprehension calculus as an intermediate form for OODB queries. The monoid comprehension calculus treats op...",
    "neighbors": [
      302,
      493,
      692,
      788
    ],
    "mask": "Train"
  },
  {
    "node_id": 1048,
    "label": 5,
    "text": "On Using Regression for Range Data Fusion In the paper, we consider an occupancy-based approach for range data fusion, as it is used in mobile robotics. We identify two major problems of this approach. The first problem deals with the combination rule which in many cases assumes the independence of range data, contrary to the usual situation. The second problem concerns the redundancy of stored and processed data, which results from using the grid representation of the occupancy function and which is the main obstacle to building 3D occupancy world models. We propose a solution to these problems by proposing a new range data fusion technique based on regression. This technique uses the evidence theory in assigning occupancy values, which we argue is advantageous for fusion, and builds the occupancy function by fitting the sample data provided by a sensor with a piecewise linear function. Having developed a general framework for our approach, we apply it to building 3D world models from visual range data, where 3D world models ...",
    "neighbors": [
      603
    ],
    "mask": "Train"
  },
  {
    "node_id": 1049,
    "label": 2,
    "text": "Document Categorization and Query Generation on the World Wide Web Using WebACE We present WebACE, an agent for exploring and categorizing documents on the World Wide Web based on a user profile. The heart of the agent is an unsupervised categorization of a set of documents, combined with a process for generating new queries that is used to search for new related documents and for filtering the resulting documents to extract the ones most closely related to the starting set. The document categories are not given a priori. We present the overall architecture and describe two novel algorithms which provide significant improvement over Hierarchical Agglomeration Clustering and AutoClass algorithms and form the basis for the query generation and search component of the agent. We report on the results of our experiments comparing these new algorithms with more traditional clustering algorithms and we show that our algorithms are fast and scalable. y  Authors are listed alphabetically.  1 Introduction  The World Wide Web is a vast resource of information and services t...",
    "neighbors": [
      291,
      599,
      616,
      893,
      947,
      1126
    ],
    "mask": "Train"
  },
  {
    "node_id": 1050,
    "label": 3,
    "text": "Query Formulation from High-level Concepts for Relational Databases A new query formulation system based on a semantic graph model is presented. The graph provides a semantic model for the data in the database with userdefined relationships. The query formulator allows users to specify their requests and constraints in highlevel concepts. The query candidates are formulated based on the user input by a graph search algorithm and ranked according to a probabilistic information measure. English-like query descriptions can also be provided for users to resolve ambiguity when multiple queries are formulated from a user input. For complex queries, we introduce an incremental approach, which assists users to achieve a complex query goal by formulating a series of simple queries. A prototype system with a multimodal interface using the high-level query formulation techniques has been implemented on top of a cooperative database system (CoBase) at UCLA.  1 Introduction  Many database applications require users to formulate ad-hoc queries instead of invocation ...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1051,
    "label": 5,
    "text": "A Hybrid Model For Sharing Information Between Fuzzy, Uncertain And Default Reasoning Models In Multi-Agent Systems This paper develops a hybrid model which provides a unified framework for the fol-  lowing four kinds of reasoning: 1) Zadeh's fuzzy approximate reasoning; 2) truthqualification  uncertain reasoning with respect to fuzzy propositions; 3) fuzzy default  reasoning (proposed, in this paper, as an extension of Reiter's default reasoning); and 4)  truth-qualification uncertain default reasoning associated with fuzzy statements (developed  in this paper to enrich fuzzy default reasoning with uncertain information). Our  hybrid model has the following characteristics: 1) basic uncertainty is estimated in terms  of words or phrases in natural language and basic propositions are fuzzy; 2) uncertainty,  linguistically expressed, can be handled in default reasoning; and 3) the four kinds of rea-  soning models mentioned above and their combination models will be the special cases of  our hybrid model. Moreover, our model allows the reasoning to be performed in the case  in which the information is fuzzy, uncertain and partial. More importantly, the problems  of sharing the information among heterogeneous fuzzy, uncertain and default reasoning  models can be solved efficiently by using our model. Given this, our framework can be  used as a basis for information sharing and exchange in knowledge-based multi-agent  systems for practical applications such as automated group negotiations. Actually, to  build such a foundation is the motivation of this paper",
    "neighbors": [
      263,
      296,
      557,
      964
    ],
    "mask": "Train"
  },
  {
    "node_id": 1052,
    "label": 5,
    "text": "Placing a Robot Manipulator Amid Obstacles for Optimized Execution This paper presents an efficient algorithm for optimizing the base location of a robot manipulator in an environment cluttered with obstacles, in order to execute specified tasks as fast as possible. The algorithm uses randomized motion planning techniques and exploits geometric \"coherence \" in configuration space to achieve fast computation. The performance of the algorithm is demonstrated on both synthetic examples and real-life CAD data from the automotive industry. The computation time ranges from under a minute for simple problems to a few minutes for more complex ones. 1 Introduction The base placement of a robot manipulator is an important issue in many robotics applications. Given a description of a robot manipulator and its environment, the goal is to find a base location for the manipulator so that specified tasks are executed as efficiently as possible. In this paper, we present an algorithm that makes use of randomized motion planning techniques to compute simultaneously ...",
    "neighbors": [
      446
    ],
    "mask": "Train"
  },
  {
    "node_id": 1053,
    "label": 3,
    "text": "Introduction to the Relationlog System Advanced applications require construction, efficient access and management of large databases with rich data structures and inference mechanisms. However, such capabilities are not directly supported by the existing database systems. In this paper, we describe Relationlog, a persistent deductive database system that is able to directly support the storage, efficient access and inference of data with complex structures. 1 Introduction  Advanced applications require construction, efficient access and management of large databases with rich data structures and inference mechanisms. However, such capabilities are not directly supported by the existing database systems. Deductive databases have the potential to meet the demands of advanced applications. They grew out of the integration of logic programming and relational database technologies. They are intended to combine the best of the two approaches, such as representational and operational uniformity, inference capabilities, recursion,...",
    "neighbors": [
      861,
      1061
    ],
    "mask": "Train"
  },
  {
    "node_id": 1054,
    "label": 0,
    "text": "Agent Factory: Generative Migration of Mobile Agents in Heterogeneous Environments In most of today's agent systems migration of agents requires homogeneity in the programming language and/or agent platform in which an agent has been designed. In this paper an approach is presented with which heterogeneity is possible: agents can migrate between non-identical platforms, and need not be written in the same language. Instead of migrating the \"code\" (including data and state) of an agent, a blueprint of an agent's functionality is transferred. An agent factory generates new code on the basis of this blueprint. This approach of generative mobility not only has implications for interoperability but also for security, as discussed in this paper.",
    "neighbors": [
      333,
      942,
      1227
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1055,
    "label": 3,
    "text": "View-based Query Processing and Constraint Satisfaction View-based query processing requires to answer a query posed to a database only on the basis of the information on a set of views, which are again queries over the same database. This problem is relevant in many aspects of database management, and has been addressed by means of two basic approaches, namely, query rewriting and query answering. In the former approach, one tries to compute a rewriting of the query in terms of the views, whereas in the latter, one aims at directly answering the query based on the view extensions. We study view-based query processing for the case of regular-path queries, which are the basic querying mechanisms for the emergent field of semistructured data. Based on recent results, we first show that a rewriting is in general a co-NP function wrt to the size of view extensions. Hence, the problem arises of characterizing which instances of the problem admit a rewriting that is PTIME. A second contribution of the work is to establish a tight connection between view-based query answering and constraint-satisfaction problems, which allows us to show that the above characterization is going to be difficult. As a third contribution of our work, we present two methods for computing PTIME rewritings of specific forms. The first method, which is based on the established connection with constraint-satisfaction problems, gives us rewritings expressed in Datalog with a fixed number of variables. The second method, based on automata-theoretic techniques, gives us rewritings that are formulated as unions of conjunctive regular-path queries with a fixed number of variables.",
    "neighbors": [
      585,
      697,
      1187
    ],
    "mask": "Test"
  },
  {
    "node_id": 1056,
    "label": 2,
    "text": "Learning to Create Customized Authority Lists The proliferation of hypertext and the popularity  of Kleinberg's HITS algorithm have  brought about an increased interest in link  analysis. While HITS and its older relatives  from the Bibliometrics provide a method for  finding authoritative sources on a particular  topic, they do not allow individual users to  inject their own opinions on what sources are  authoritative. This paper presents a technique  for learning a user's internal model of  authority. We present experimental results  based on Cora on-line index, a database of  approximately one million on-line computer  science literature references.  1. Introduction  Bibliometrics (White & McCain, 1989; Small, 1973) involves studying the structure that emerges from sets of linked documents. Traditionally, these links have taken the form of citations among journal articles, although Kleinberg (1997) and others (e.g., Brin & Page, 1998) have found that they adapt well to sets of hyperlinked documents. Bibliometric techniques exis...",
    "neighbors": [
      774,
      1017,
      1141
    ],
    "mask": "Train"
  },
  {
    "node_id": 1057,
    "label": 3,
    "text": "On the Integration of IR and Databases : Integration of information retrieval (IR) in database management  systems (DBMSs) has proven di#cult. Previous attempts to integration su#ered  from inherent performance problems, or lacked desirable separation between  logical and physical data models. To overcome these problems, we discuss a  database approach based on structural object-orientation. We implement IR  techniques using extensions in an object algebra called MOA. MOA has been  implemented on top of the database backend Monet, a state-of-the-art highperformance  database kernel with a binary relational interface. Our prototype  implementation of the inference network retrieval model using MOA and Monet  demonstrates the feasibility of this approach. We conclude with a discussion of  the advantages of our database design.  INTRODUCTION  Information retrieval (IR) is concerned with the retrieval of (usually text) documents that are likely to be relevant to the user's information need as expressed by his request (van Rijsb...",
    "neighbors": [
      78,
      1128
    ],
    "mask": "Test"
  },
  {
    "node_id": 1058,
    "label": 3,
    "text": "Handling Temporal Grouping and Pattern-Matching Queries in a Temporal Object Model This paper presents a language for expressing temporal pattern-matching queries, and a set of temporal grouping operators for structuring histories following calendar-based criteria. Pattern-matching queries are shown to be useful for reasoning about successive events in time while temporal grouping may be either used to aggregate data along the time dimension or to display histories. The combination of these capabilities allows to express complex queries involving succession in time and calendar-based conditions simultaneously. These operators are embedded into the TEMPOS temporal data model and their use is illustrated through examples taken from a geographical application. The proposal has been validated by a prototype on top of the O 2 DBMS.  Keywords: temporal databases, temporal object model, temporal query language, pattern-matching queries, temporal grouping, calendar, granularity, O 2 .  1 Introduction  In most modern DBMS, time is one of the primitive datatypes provided for d...",
    "neighbors": [
      98,
      535,
      1255
    ],
    "mask": "Train"
  },
  {
    "node_id": 1059,
    "label": 2,
    "text": "Accelerated Focused Crawling through Online Relevance Feedback The organization of HTML into a tag tree structure, which is rendered by browsers as roughly rectangular regions with embedded text and HREF links, greatly helps surfers locate and click on links that best satisfy their information need. Can an automatic program emulate this human behavior and thereby learn to predict the relevance of an unseen HREF target page w.r.t. an information need, based on information limited to the HREF source page? Such a capability would be of great interest in focused crawling and resource discovery, because it can fine-tune the priority of unvisited URLs in the crawl frontier, and reduce the number of irrelevant pages which are fetched and discarded.",
    "neighbors": [
      281,
      382,
      457,
      1000,
      1099
    ],
    "mask": "Train"
  },
  {
    "node_id": 1060,
    "label": 1,
    "text": "Support Vector Machines for Face Authentication The paper studies Support Vector Machines (SVMs) in the context of face  authentication. Our study supports the hypothesis that the SVM approach  is able to extract the relevant discriminatory information from the training  data. We believe this is the main reason for its superior performance over  benchmark methods. When the representation space already captures and  emphasises the discriminatory information content as in the case of Fisherfaces,  SVMs loose their superiority. SVMs can also cope with illumination  changes, provided these are adequately represented in the training data.  However, on data which has been sanitised by feature extraction (Fisherfaces)  and/or normalisation, SVMs can get over-trained, resulting in the loss of the  ability to generalise. SVMs involve many parameters and can employ different  kernels. This makes the optimisation space rather extensive, without the  guarantee that it has been fully explored to find the best solution.  1 Introduction  Verificat...",
    "neighbors": [
      5,
      1108
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1061,
    "label": 3,
    "text": "Overview of Datalog Extensions with Tuples and Sets Datalog (with negation) is the most powerful query language for relational database with a well-defined declarative semantics based on the work in logic programming. However, Datalog only allows inexpressive flat structures and cannot directly support complex values such as nested tuples and sets common in novel database applications. For these reasons, Datalog has been extended in the past several years to incorporate tuple and set constructors. In this paper, we examine four different Datalog extensions: LDL, COL, Hilog and Relationlog. 1 Introduction  Databases and logic programming are two independently developed areas in computer science. Database technology has evolved in order to effectively and efficiently organize, manage and maintain large volumes of ever increasingly complex data reliably in various memory devices. The underlying structure of databases has been the primary focus of research which leads to the development of data models. The most well-known and widely used da...",
    "neighbors": [
      861,
      1053
    ],
    "mask": "Train"
  },
  {
    "node_id": 1062,
    "label": 2,
    "text": "XML Conceptual Modeling using UML . The eXtensible Markup Language (XML) is increasingly finding  acceptance as a standard for storing and exchanging structured and  semi-structured information. With its expressive power, XML enables a  great variety of applications relying on such structures - notably product  catalogs, digital libraries, and electronic data interchange (EDI). As  the data schema, an XML Document Type Definition (DTD) is a means  by which documents and objects can be structured. Currently, there is  no suitable way to model DTDs conceptually. Our approach is to model  DTDs and thus classes of documents on the basis of UML (Unified Modeling  Language). We consider UML to be the connecting link between  software engineering and document design, i.e., it is possible to design  object-oriented software together with the necessary XML structures. For  this reason, we describe how to transform the static part of UML, i.e.  class diagrams, into XML DTDs. The major challenge for the transformation  is to defi...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1063,
    "label": 3,
    "text": "Overview of the ROL2 Deductive Object-Oriented Database System This paper presents an overview of ROL2, a novel deductive object-oriented database system developed at the University of Regina. ROL2 supports in a rule-based framework nearly all important object-oriented features such as object identity, complex objects, typing, information hiding, rule-based methods, encapsulation of such methods, overloading, late binding, polymorphism, class hierarchies, multiple structural and behavioral inheritance with overriding, blocking, and conflict handling. It is so far the only deductive system that supports all these features in a pure rule-based framework.",
    "neighbors": [
      350
    ],
    "mask": "Train"
  },
  {
    "node_id": 1064,
    "label": 1,
    "text": "Initialization of Iterative Refinement Clustering Algorithms Iterative refinement clustering algorithms (e.g. K-Means, EM) converge to one of numerous local minima. It is known that they are especially sensitive to initial conditions. We present a procedure for computing a refined starting condition from a given initial one that is based on an efficient technique for estimating the modes of a distribution. The refined initial  starting condition leads to convergence to \"better\" local minima. The procedure is applicable to a wide class of clustering algorithms for both discrete and continuous data. We demonstrate the application of this method to the Expectation Maximization (EM) clustering algorithm and show that refined initial points indeed lead to improved solutions. Refinement run time is considerably lower than the time required to cluster the full database. The method is scalable and can be coupled with a scalable clustering algorithm to address the large-scale clustering in data mining.  1 Background  Clustering has been formulated in var...",
    "neighbors": [
      616
    ],
    "mask": "Test"
  },
  {
    "node_id": 1065,
    "label": 0,
    "text": "Reliable Communication for Highly Mobile Agents . The provision of a reliable communication infrastructure for mobile agents is still an open research issue. The challenge to reliability we address in this work does not come from the possibility of faults, but rather from the mere presence of mobility, which complicates the problem of ensuring the delivery of information even in a fault-free network. For instance, the asynchronous nature of message passing and agent migration may cause situations where messages forever chase a mobile agent that moves frequently from one host to another. Current solutions rely on conventional technologies that either do not provide a solution for the aforementioned problem, because they were not designed with mobility in mind, or enforce continuous connectivity with the message source, which in many cases defeats the very purpose of using mobile agents.  In this paper, we propose an algorithm that guarantees delivery to highly mobile agents using a technique similar to a distributed snapshot. A numbe...",
    "neighbors": [
      246,
      269,
      1103,
      1227
    ],
    "mask": "Train"
  },
  {
    "node_id": 1066,
    "label": 2,
    "text": "Recommending Web Documents Based on User Preferences Making recommendations requires treating users as individuals. In this paper, we describe a metasearch engine available at NEC Research Institute that allows individual search strategies to be used. Each search strategy consists of a different set of sources, different query modification rules and a personalized ordering policy. We combine these three features with a dynamic interface that allows users to see the \"current best\" recommendations displayed at all times, and allows results to be displayed immediately upon retrieval. We present several examples where a single query produces different results, ordered based on different factors, accomplished without the use of training, or a local database.",
    "neighbors": [
      224,
      471,
      587,
      897
    ],
    "mask": "Train"
  },
  {
    "node_id": 1067,
    "label": 0,
    "text": "Agent Communication Languages: The Current Landscape this article--- suggest a paradigm for software development that emphasizes autonomy both at design time and runtime, adaptivity, and cooperation. This approach seems appealing in a world of distributed, heterogeneous systems. Languages for communicating agents promise to play the role that natural languages played for their human counterparts. An agent communication language that allows agents to interact while hiding the details of their internal workings will result in agent communities that tackle problems no individual agent could.",
    "neighbors": [
      208,
      308,
      367,
      521,
      663,
      955,
      1208
    ],
    "mask": "Test"
  },
  {
    "node_id": 1068,
    "label": 2,
    "text": "Probabilistic Models for Unified Collaborative and Content-Based Recommendation in Sparse-Data Environments Recommender systems leverage product and  community information to target products to  consumers. Researchers have developed collaborative  recommenders, content-based recommenders,  and a few hybrid systems. We propose  a unified probabilistic framework for merging  collaborative and content-based recommendations.  We extend Hofmann's (1999) aspect  model to incorporate three-way co-occurrence  data among users, items, and item content. The  relative influence of collaboration data versus  content data is not imposed as an exogenous parameter,  but rather emerges naturally from the  given data sources. However, global probabilistic  models coupled with standard EM learning algorithms  tend to drastically overfit in the sparsedata  situations typical of recommendation applications.  We show that secondary content information  can often be used to overcome sparsity.  Experiments on data from the ResearchIndex  library of Computer Science publications  show that appropriate mixture models incorporating  secondary data produce significantly better  quality recommenders than k-nearest neighbors  (k-NN). Global probabilistic models also allow  more general inferences than local methods like  k-NN.",
    "neighbors": [
      427,
      524,
      722,
      864,
      937,
      1007,
      1141
    ],
    "mask": "Train"
  },
  {
    "node_id": 1069,
    "label": 3,
    "text": "Representing and Querying Changes in Semistructured Data Semistructured data may be irregular and incomplete and does not necessarily conform to a fixed schema. As with structured data, it is often desirable to maintain a history of changes to data, and to query over both the data and the changes. Representing and querying changes in semistructured data is more difficult than in structured data due to the irregularity and lack of schema. We present a model for representing changes in semistructured data and a language for querying over these changes. We discuss implementation strategies for our model and query language. We also describe the design and implementation of a \"query subscription service\" that permits standing queries over changes in semistructured information sources. 1 Introduction  Semistructured data is data that has some structure, but it may be irregular and incomplete and does not necessarily conform to a fixed schema (e.g, HTML documents). Recently, there has been increased interest in data models and query languages for s...",
    "neighbors": [
      336,
      402,
      705,
      912,
      1254
    ],
    "mask": "Train"
  },
  {
    "node_id": 1070,
    "label": 3,
    "text": "Navigational Plans For Data Integration We consider the problem of building data integration systems when the data sources are webs of data, rather than sets of relations. Previous approaches to modeling data sources are inappropriate in this context because they do not capture the relationships between linked data and the need to navigate through paths in the data source in order to obtain the data. We describe a language for modeling data sources in this new context. We show that our language has the required expressive power, and that minor extensions to it would make query answering intractable. We provide a sound and complete algorithm for reformulating a user query into a query over the data sources, and we show how to create query execution plans that both query and navigate the data sources.  Introduction  The purpose of data integration is to provide a uniform  interface to a multitude of data sources. Data integration applications arise frequently as corporations attempt to provide their customers and employees wit...",
    "neighbors": [
      876
    ],
    "mask": "Test"
  },
  {
    "node_id": 1071,
    "label": 1,
    "text": "Heterogeneity in the Coevolved Behaviors of Mobile Robots: The Emergence of Specialists Many mobile robot tasks can be most efficiently  solved when a group of robots is utilized. The type  of organization, and the level of coordination and  communication within a team of robots affects the  type of tasks that can be solved. This paper examines  the tradeoff of homogeneity versus heterogeneity  in the control systems by allowing a team of  robots to coevolve their high-level controllers given  different levels of difficulty of the task. Our hypothesis  is that simply increasing the difficulty of a task  is not enough to induce a team of robots to create  specialists. The key factor is not difficulty per se,  but the number of skill sets necessary to successfully  solve the task. As the number of skills needed  increases, the more beneficial and necessary heterogeneity  becomes. We demonstrate this in the  task domain of herding, where one or more robots  must herd another robot into a confined space.  1",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1072,
    "label": 1,
    "text": "Learning Motor Skills By Imitation: A Biologically Inspired Robotic Model. This article presents a biologically inspired model for motor skills imitation. The model is composed of modules whose functinalities are inspired by corresponding brain regions responsible for the control of movement in primates. These modules are high-level abstractions of the spinal cord, the primary and premotor cortexes (M1 and PM), the cerebellum, and the temporal cortex. Each module is modeled at a connectionist level. Neurons in PM respond both to visual observation of movements and to corresponding motor commands produced by the cerebellum. As such, they give an abstract representation of mirror neurons. Learning of new combinations of movements is done in PM and in the cerebellum. Premotor cortexes and cerebellum are modeled by the DRAMA neural architecture which allows learning of times series and of spatio-temporal invariance in multimodal inputs. The model is implemented in a mechanical simulation of two humanoid avatars, the imitator and the imitatee. Three types of sequences learning are presented: (1) learning of repetitive patterns of arm and leg movements; (2) learning of oscillatory movements of shoulders and elbows, using video data of a human demonstration; 3) learning of precise movements of the extremities for grasp and reach",
    "neighbors": [
      509
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1073,
    "label": 3,
    "text": "Decomposition of Database Classes under Path Functional Dependencies and Onto Constraints Based on F-logic, we specify an advanced data model with object-oriented and  logic-oriented features that substantially extend the relational approach. For this  model we exhibit and study the counterpart to the well-known decomposition of  a relation scheme according to a nontrivial nonkey functional dependency. For  decomposing a class of a database schema the transformation of pivoting is used.  Pivoting separates apart some attributes of the class into a newly generated class.  This new class is declared to be a subclass of the result class of the so-called pivot  attribute. Moreover the pivot attribute provides the link between the original class  and the new subclass. We identify the conditions for the result of pivoting being  equivalent with its input: the expressive power of path functional dependencies,  the validity of the path functional dependency between the pivot attribute and  the transplanted attributes, and the validity of the onto-constraint guaranteeing  that value...",
    "neighbors": [
      77,
      773
    ],
    "mask": "Train"
  },
  {
    "node_id": 1074,
    "label": 4,
    "text": "Designing PETS: A Personal Electronic Teller of Stories in Robots For Kids, Morgan Kaufmann, San Francisco, CA. Druin, A. and Hendler J. (eds.)  Page 2  Who, or What, Is PETS? Figure 1: PETS, Spaceship, and MyPETS software What Does PETS Do?  PETS is a Personal Electronic Teller of Stories, a robotic story telling environment for elementary school age children (Druin et al. 1999a). The PETS kit contains a box of fuzzy stuffed animal parts and an authoring application on a personal computer (Figures 1 and 3). Children can build a robotic animal, or pet, by connecting animal parts such as torso, head, paws, ears, and wings. After they construct their pet, they can write and tell stories using the My PETS software. Just as the robotic animal is constructed from discrete components, My PETS is also constructive. This application enables children to create emotions, draw emotive facial expressions, name their robotic companion, and compile a library of stori",
    "neighbors": [
      648,
      811,
      946
    ],
    "mask": "Train"
  },
  {
    "node_id": 1075,
    "label": 1,
    "text": "Using Decision Trees to Improve Case-Based Learning This paper shows that decision trees can be used to improve the performance of casebased learning (CBL) systems. We introduce a performance task for machine learning systems called semi-flexible prediction that lies between the classification task performed by decision tree algorithms and the flexible prediction task performed by conceptual clustering systems. In semi-flexible prediction, learning should improve prediction of a specific set of features known a priori rather than a single known feature (as in classification) or an arbitrary set of features (as in conceptual clustering). We describe one such task from natural language processing and present experiments that compare solutions to the problem using decision trees, CBL, and a hybrid approach that combines the two. In the hybrid approach, decision trees are used to specify the features to be included in k-nearest neighbor case retrieval. Results from the experiments show that the hybrid approach outperforms both the decision ...",
    "neighbors": [
      611,
      731,
      1045,
      1090,
      1135
    ],
    "mask": "Train"
  },
  {
    "node_id": 1076,
    "label": 1,
    "text": "Personalized Web-Document Filtering Using Reinforcement Learning Abstract- Document filtering is increasingly deployed in Web environments to reduce information overload of users. We formulate online information filtering as a reinforcement learning problem, i.e. TD(0). The goal is to learn user profiles that best represent his information needs and thus maximize the expected value of user relevance feedback. A method is then presented that acquires reinforcement signals automatically by estimating user\u2019s implicit feedback from direct observations of browsing behaviors. This \u201clearning by observation \u201d approach is contrasted with conventional relevance feedback methods which require explicit user feedbacks. Field tests have been performed which involved 10 users reading a total of 18,750 HTML documents during 45 days. Compared to the existing document filtering techniques, the proposed learning method showed superior performance in information quality and adaptation speed to user preferences in online filtering. 1",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1077,
    "label": 4,
    "text": "Evaluating Guidelines for Reducing User Disorientation When Navigating in Virtual Environments Navigation in virtual environments can be difficult. One contributing factor is user disorientation.",
    "neighbors": [
      376,
      786,
      1105
    ],
    "mask": "Train"
  },
  {
    "node_id": 1078,
    "label": 5,
    "text": "An Autonomous Spacecraft Agent Prototype . This paper describes the New Millennium Remote Agent (NMRA) architecture for autonomous spacecraft control systems. The architecture supports challenging requirements of the autonomous spacecraft domain not usually addressed in mobile robot architectures, including highly reliable autonomous operations over extended time periods in the presence of tight resource constraints, hard deadlines, limited observability, and concurrent activity. A hybrid architecture, NMRA integrates traditional real-time monitoring and control with heterogeneous components for constraint-based planning and scheduling, robust multi-threaded execution, and model-based diagnosis and reconfiguration. Novel features of this integrated architecture include support for robust closed-loop generation and execution of concurrent temporal plans and a hybrid procedural/deductive executive. We implemented a prototype autonomous spacecraft agent within the architecture and successfully demonstrated the prototype in the c...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1079,
    "label": 4,
    "text": "The Conceptual Basis for Mediation Services Mediator modules comprise a layer of intelligent middleware services in information systems, linking data resources and application programs. Earlier programs that led to the concept of mediation were either constructed to support specific applications or provided extended services from databases. Intelligent mediators are being built now by careful domain knowledge acquisition and hand crafting the required code. In this paper we present the conceptual underpinning for automating the mediation process. Automation does not extend to fully automatic code generation, since additional knowledge is necessary to provide added value. The generation concept is based on the extraction of a hierarchical domain model out of the general network representing the available resources. Associated with the method are domain ontologies. Ontologies list the terms used by the models, and document their relationships. These terms provide the semantic foundation needed to perform the generation. This paper...",
    "neighbors": [
      185,
      761
    ],
    "mask": "Test"
  },
  {
    "node_id": 1080,
    "label": 4,
    "text": "Mixed-Initiative Interaction spect of effective multiagent collaboration to solve problems or perform tasks. In our minimal human-computer configuration, such tasks could include systems designed to interact with a user to design a kitchen, find the best airfare, coordinate an emergency relief mission, or teach the user how to use new equipment. Mixed-initiative  refers to a flexible interaction strategy, where each agent can contribute to the task what it does best. Furthermore, in the most general cases, the agents' roles are not determined in advance, but opportunistically negotiated between them as the problem is being solved. At any one time, one agent might have the initiative---controlling the interaction---while the other works to assist it, contributing to the interaction as required. At other times, the roles are reversed, and at other times again the agents might be working independently, assisting each other only when specifically asked. The agents dynamically adapt their interaction st",
    "neighbors": [
      926
    ],
    "mask": "Train"
  },
  {
    "node_id": 1081,
    "label": 1,
    "text": "A Comparison of Prediction Accuracy, Complexity, and Training Time of Thirty-three Old and New Classification Algorithms Twenty-two decision tree, nine statistical, and two neural network algorithms are compared on thirty-two datasets in terms of classification accuracy, training time, and (in the case of trees) number of leaves. Classification accuracy is measured by mean error rate and mean rank of error rate. Both criteria place a statistical, spline-based, algorithm called POLYCLASS at the top, although it is not statistically significantly different from twenty other algorithms. Another statistical algorithm, logistic regression, is second and third with respect to the two accuracy criteria. The most accurate decision tree algorithm is QUEST with linear splits, which ranks fourth and fifth, respectively. Although spline-based statistical algorithms tend to have good accuracy, they also require relatively long training times. POLYCLASS, for example, is third last in terms of median training time. It often requires hours of training compared to seconds for other algorithms. The QUEST and logistic re...",
    "neighbors": [
      608
    ],
    "mask": "Train"
  },
  {
    "node_id": 1082,
    "label": 2,
    "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data We present conditional random elds, a framework  for building probabilistic models to segment  and label sequence data. Conditional random  fields offer several advantages over hidden  Markov models and stochastic grammars  for such tasks, including the ability to relax  strong independence assumptions made in those  models. Conditional random fields also avoid  a fundamental limitation of maximum entropy  Markov models (MEMMs) and other discriminative  Markov models based on directed graphical  models, which can be biased towards states  with few successor states. We present iterative  parameter estimation algorithms for conditional  random fields and compare the performance of  the resulting models to HMMs and MEMMs on  synthetic data.  1. Introduction  The need to segment and label sequences arises in many different problems in several scientific fields. Hidden Markov models (HMMs) and stochastic grammars are well understood and widely used probabilistic models for such problems. I...",
    "neighbors": [
      1024
    ],
    "mask": "Train"
  },
  {
    "node_id": 1083,
    "label": 2,
    "text": "An Evaluation of Statistical Approaches to Text Categorization Abstract. This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.",
    "neighbors": [
      348,
      571,
      1090
    ],
    "mask": "Train"
  },
  {
    "node_id": 1084,
    "label": 5,
    "text": "Simulated Annealing Algorithms For Continuous Global Optimization INTRODUCTION  In this paper we consider Simulated Annealing algorithms (SA in what follows) applied to continuous global optimization problems, i.e. problems with the following form  f  = min  x2X  f(x); (1.1) where X ` !  n  is a continuous domain, often assumed to be compact, which, combined with the continuity or lower semicontinuity of f , guarantees the existence of the minimum value f  . SA algorithms are based on an analogy with a physical phenomenon: while at high temperatures the molecules in a liquid move freely, if the temperature is slowly decreased the thermal mobility of the molecules is lost and they form a pure crystal which also corresponds to a state of minimum energy. If the temperature is decreased too quickly (the so called quenching) a liquid metal rather ends up in a polycrystalline or amorphous state with",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1085,
    "label": 2,
    "text": "Ontobroker: Or How to Enable Intelligent Access to the WWW . The World Wide Web (WWW) is currently one of the most important  electronic information sources. However, its query interfaces and the provided  reasoning services are rather limited. Ontobroker consists of a number of  languages and tools that enhance query access and inference service in the  WWW. It provides languages to annotate web documents with ontological  information, to represent ontologies, and to formulate queries. The tool set of  Ontobroker allows us to access information and knowledge from the web and to  infer new knowledge with an inference engine based on techniques from logic  programming. This article provides several examples that illustrate these  languages and tools and the kind of service that is provided. We also discuss the  bottlenecks of our approach that stem from the fact that the applicability of  Ontobroker requires two time-consuming activities: (1) developing shared  ontologies that reflect the consensus of a group of web users and (2) annotating  we...",
    "neighbors": [
      456,
      767,
      851
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1086,
    "label": 2,
    "text": "Semantic Web Services hose properties, capabilities,  interfaces, and effects are encoded in an unambiguous,  machine-understandable form.  The realization of the Semantic Web is underway  with the development of new AI-inspired content  markup languages, such as OIL,  3  DAML+OIL  (www.daml.org/2000/10/daml-oil), and DAML-L (the  last two are members of the DARPA Agent Markup  Language (DAML) family of languages).  4  These languages  have a well-defined semantics and enable the  markup and manipulation of complex taxonomic and  logical relations between entities on the Web. A fundamental  component of the Semantic Web will be the markup of Web services to make them computer-interpretable, use-apparent, and agent-ready. This article addresses precisely this component.  We present an approach to Web service markup that provides an agent-independent declarative API capturing the data and metadata associated with a service together with specifications of its pro",
    "neighbors": [
      321,
      914
    ],
    "mask": "Train"
  },
  {
    "node_id": 1087,
    "label": 5,
    "text": "Damasio, Descartes, Alarms and Meta-management This paper discusses some of the requirements for the control architecture of an intelligent human-like agent with multiple independent dynamically changing motives in a dynamically changing only partly predictable world. The architecture proposed includes a combination of reactive, deliberative and meta-management mechanisms along with one or more global \"alarm\" systems. The engineering design requirements are discussed in relation our evolutionary history, evidence of brain function and recent theories of Damasio and others about the relationships between intelligence and emotions.  1. INTRODUCTION  Stan Franklin, the organiser of this symposium, wrote \"Minds are the control structures of autonomous agents\" [5, p 412]. The claim that minds are essentially concerned with control, echoing the seminal ideas of Norbert Wiener [16] is one with which I strongly concur though as argued in [11], we need to go far beyond the early idea of control systems with fixed architecture and changes on...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1088,
    "label": 4,
    "text": "Forms/3: A First-Order Visual Language to Explore the Boundaries of the Spreadsheet Paradigm Although detractors of functional programming sometimes claim that functional programming is too difficult or counterintuitive for most programmers to understand and use, evidence to the contrary can be found by looking at the popularity of spreadsheets. The spreadsheet paradigm, a first-order subset of the functional programming paradigm, has found wide acceptance among both programmers and end users. Still, there are many limitations with most spreadsheet systems.  In this paper, we discuss language features that eliminate several of these limitations without deviating from the first-order, declarative evaluation model. The language used to illustrate these features is a research language called Forms/3. Using Forms/3, we show that procedural abstraction, data abstraction, and graphics output can be supported in the spreadsheet paradigm. We show that, with the addition of a simple model of time, animated output and GUI I/O also become viable. To demonstrate generality, we also presen...",
    "neighbors": [
      1186
    ],
    "mask": "Train"
  },
  {
    "node_id": 1089,
    "label": 5,
    "text": "A Probabilistic Formulation for Hausdorff Matching Matching images based on a Hausdorff measure has become popular for computer vision applications. However, no probabilistic model has been used in these applications. This limits the formal treatment of several issues, such as feature uncertainties and prior knowledge. In this paper, we develop a probabilistic formulation of image matching in terms of maximum likelihood estimation that generalizes a version of Hausdorff matching. This formulation yields several benefits with respect to previous Hausdorff matching formulations. In addition, we show that the optimal model position in a discretized pose space can be located efficiently in this formation and we apply these techniques to a mobile robot self-localization problem. 1 Introduction  The use of variants of the Hausdorff distance has recently become popular for image matching applications (see, for example, [6, 9, 11, 16, 18, 19]). While these methods have been largely successful, they have lacked a probabilistic formulation of th...",
    "neighbors": [
      922
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1090,
    "label": 1,
    "text": "Machine Learning and Natural Language Processing In this report, some collaborative work between the fields of Machine Learning (ML) and Natural Language Processing (NLP) is presented. The document is structured in two parts. The first part includes a superficial but comprehensive survey covering the state--of--the--art of machine learning techniques applied to natural language learning tasks. In the second part, a particular problem, namely Word Sense Disambiguation (WSD), is studied in more detail. In doing so, four algorithms for supervised learning, which belong to different families, are compared in a benchmark corpus for the WSD task. Both qualitative and quantitative conclusions are drawn. This document stands for the complementary documentation for the conference \"Aprendizaje autom 'atico aplicado al procesamiento del lenguaje natural\", given by the author within the course: \"Curso de Industrias de la Lengua: La Ingenier'ia Lingu'istica en la Sociedad de la Informaci'on\", Fundaci'on Duques de Soria. Soria. July 2000. 1 Con...",
    "neighbors": [
      341,
      347,
      609,
      855,
      865,
      1075,
      1083,
      1133,
      1178
    ],
    "mask": "Train"
  },
  {
    "node_id": 1091,
    "label": 3,
    "text": "Abduction with Negation as Failure for Active and Reactive Rules . Recent work has suggested abductive logic programming as a  suitable formalism to represent active databases and intelligent agents. In  particular, abducibles in abductive logic programs can be used to represent  actions, and integrity constaints in abductive logic programs can be  used to represent active rules of the kind encountered in active databases  and reactive rules incorporating reactive behaviour in agents. One would  expect that, in this approach, abductive proof procedures could provide  the engine underlying active database management systems and the behaviour  of agents. We analyse existing abductive proof procedures and  argue that they are inadequate in handling these applications. The inadequacy  is due to the inappropriate treatment of negative literals in  integrity constraints. We propose a new abductive proof procedure and  give examples of how this proof procedure can be used to achieve active  behaviour in (deductive) databases and reactivity in agents. Final...",
    "neighbors": [
      400,
      588
    ],
    "mask": "Test"
  },
  {
    "node_id": 1092,
    "label": 2,
    "text": "Location Oriented Integration of Internet Information - Mobile Info Search - Information on the Internet is becoming more attractive and  useful for our daily life. It provides things on the town, happenings on  the city, and learning of the real world. If we can utilize such information  for the interaction between the human and the city, it can enhance the  value and the function of the city. In this paper we introduce the research  project \"Mobile Info Search\" in which we study the method of integrating  heterogeneous information in a location-oriented way for providing it in  a handy form with mobile computing. We have a prototype of Mobile  Info Search at http://www.kokono.net/, a location-based \"search engine\".  Local information such as yellow pages, maps, and relevant Web pages  at any location of Japan are provided with a simple interface. From the  analysis of test services, we will discuss the user issues and information  source issues; What kind of local information is welcomed? What can  we learn from collected documents? Through the experience of handling  various contents related to the real-world, we describe the potential of  the Internet information for the digital city efforts.  1",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1093,
    "label": 3,
    "text": "Point-based Temporal Extensions of SQL and their Efficient Implementation . This chapter introduces a new approach to temporal extensions of SQL. The main difference from most of the current proposals is the use single time points, rather than intervals or various other complexvalues for references to time, while still achieving efficient query evaluation. The proposed language, SQL/TP, extends the syntax of SQL/92 to handle temporal data in a natural way: it adds a single data type to represent a linearly ordered universe of time instants. The semantics of the new language naturally extends the standard SQL semantics and eliminates or fixes many of the problems connected with defining a precise semantics to temporal query languages based on explicit interval-valued temporal attributes. The efficient query evaluation procedure is based on a compilation technique that translates SQL/TP queries to SQL/92. Therefore existing off-shelf database systems can be used as back-ends for implementations based on this approach to manage temporal data. 1 Why another temp...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1094,
    "label": 2,
    "text": "Relevance Feedback and Personalization: A Language Modeling Perspective Many approaches to personalization involve learning short-term and long-term user models. The user models provide context for queries and other interactions with the information system. In this paper, we discuss how language models can be used to represent context and support context-based techniques such as relevance feedback and query disambiguation.  1. Overview  From some perspectives, personalization has been studied in information retrieval for some time. If the goal of personalization is to improve the effectiveness of information access by adapting to individual users' needs, then techniques such as relevance feedback and filtering would certainly be considered to support personalization. There has also been considerable research done, mostly in the 1980s, on user modeling for information retrieval. This research had essentially the same goal as current research on personalization, which is to build a model of a user's interests and preferences over time. Filtering systems, too...",
    "neighbors": [
      341
    ],
    "mask": "Test"
  },
  {
    "node_id": 1095,
    "label": 5,
    "text": "Learning Lateral Interactions for Feature Binding and Sensory Segmentation We present a new approach to the supervised learning of lateral interactions  for the competitive layer model (CLM) dynamic feature binding  architecture. The method is based on consistency conditions, which were  recently shown to characterize the attractor states of this linear threshold  recurrent network. For a given set of training examples the learning problem  is formulated as a convex quadratic optimization problem in the lateral  interaction weights. An efficient dimension reduction of the learning  problem can be achieved by using a linear superposition of basis interactions.  We show the successful application of the method to a medical  image segmentation problem of fluorescence microscope cell images.  1",
    "neighbors": [
      362
    ],
    "mask": "Train"
  },
  {
    "node_id": 1096,
    "label": 2,
    "text": "High-Performance Web Crawling SRC\u2019s charter is to advance the state of the art in computer systems by doing basic and applied research in support of our company\u2019s business objectives. Our interests and projects span scalable systems (including hardware, networking, distributed systems, and programming-language technology), the Internet (including the Web, e-commerce, and information retrieval), and human/computer interaction (including user-interface technology, computer-based appliances, and mobile computing). SRC was established in 1984 by Digital Equipment Corporation. We test the value of our ideas by building hardware and software prototypes and assessing their utility in realistic settings. Interesting systems are too complex to be evaluated solely in the abstract; practical use enables us to investigate their properties in depth. This experience is useful in the short term in refining our designs and invaluable in the long term in advancing our knowledge. Most of the major advances in information systems have come through this approach, including personal computing, distributed systems, and the Internet. We also perform complementary work of a more mathematical character. Some of",
    "neighbors": [
      763,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 1097,
    "label": 2,
    "text": "Compression of Inverted Indexes For Fast Query Evaluation Compression reduces both the size of indexes and the time needed to evaluate queries. In this paper, we revisit the compression of inverted lists of document postings that store the position and frequency of indexed terms, considering two approaches to improving retrieval efficiency: better implementation and better choice of integer compression schemes. First, we propose several simple optimisations to well-known integer compression schemes, and show experimentally that these lead to significant reductions in time. Second, we explore the impact of choice of compression scheme on retrieval efficiency. In experiments on large collections of data, we show two surprising results: use of simple byte-aligned codes halves the query evaluation time compared to the most compact Golomb-Rice bitwise compression schemes; and, even when an index fits entirely in memory, byte-aligned codes result in faster query evaluation than does an uncompressed index, emphasising that the cost of transferring data from memory to the CPU cache is less for an appropriately compressed index than for an uncompressed index. Moreover, byte-aligned schemes have only a modest space overhead: the most compact schemes result in indexes that are around 10 % of the size of the collection, while a byte-aligned scheme is around 13%. We conclude that fast byte-aligned codes should be used to store integers in inverted lists.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1098,
    "label": 2,
    "text": "Rank Aggregation Methods for the Web We consider the problem of combining ranking results from various sources. In the context of the Web, the main applications include building meta-search engines, combining ranking functions, selecting documents based on multiple criteria, and improving search precision through word associations. Wedevelop a set of techniques for the rank aggregation problem and compare their performance to that of well-known methods. A primary goal of our work is to design rank aggregation techniques that can effectively combat \"spam,\" a serious problem in Web searches. Experiments show that our methods are simple, efficient, and effective.  Keywords: rank aggregation, ranking functions, metasearch, multi-word queries, spam  1.",
    "neighbors": [
      774,
      951,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 1099,
    "label": 2,
    "text": "Topical Locality in the Web Most web pages are linked to others with related content. This idea, combined with another that says that text in, and possibly around, HTML anchors describe the pages to which they point, is the foundation for a usable WorldWide Web. In this paper, we examine to what extent these ideas hold by empirically testing whether topical locality mirrors spatial locality of pages on the Web. In particular, we find that the likelihood of linked pages having similar textual content to be high; the similarity of sibling pages increases when the links from the parent are close together; titles, descriptions, and anchor text represent at least part of the target page; and that anchor text may be a useful discriminator among unseen child pages. These results show the foundations necessary for the success of many web systems, including search engines, focused crawlers, linkage analyzers, and intelligent web agents.",
    "neighbors": [
      235,
      536,
      638,
      662,
      774,
      845,
      897,
      990,
      1000,
      1005,
      1017,
      1032,
      1059
    ],
    "mask": "Test"
  },
  {
    "node_id": 1100,
    "label": 0,
    "text": "Side Constraints and Non-Price Attributes in Markets In most real-world (electronic) marketplaces, there  are other considerations besides maximizing immediate  economic value. We present a sound way  of taking such considerations into account via side  constraints and non-price attributes. Side constraints  have a significant impact on the complexity  of market clearing. Budget constraints, a limit on  the number of winners, and XOR-constraints make  even noncombinatorial markets ##-complete to  clear. The latter two make markets ##-complete  to clear even if bids can be accepted partially. This  is surprising since, as we show, even combinatorial  markets with a host of very similar side constraints  can be cleared in polytime. An extreme equality  constraint makes combinatorial markets polytime  clearable even if bids have to be accepted entirely  or not at all. Finally, we present a way to  take into account additional attributes using a bid  re-weighting scheme, and prove that it does not  change the complexity of clearing. All of the results  hold for auctions as well as exchanges, with  and without free disposal.  1",
    "neighbors": [
      589,
      716
    ],
    "mask": "Train"
  },
  {
    "node_id": 1101,
    "label": 2,
    "text": "One-way Functions are Essential for Single-Server Private Information Retrieval Private Information Retrieval (PIR) protocols allow a user to read information from a database without revealing to the server storing the database which information he has read. Kushilevitz and Ostrovsky [23] construct, based on the quadratic residuosity assumption, a single-server PIR protocol with small communication complexity. Cachin, Micali, and Stadler [5] present a single-server PIR protocol with a smaller communication complexity, based on the (new) \\Phihiding assumption. A major question, addressed in the present work, is what assumption is the minimal assumption necessary for the construction of single-server private information retrieval protocols with small communication complexity. We prove that if there is a (0-error) PIR protocol in which the server sends less than n bits then one-way functions exist (where n is the number of bits in the database). That is, even saving one bit compared to the naive protocol, in which the entire database is sent, already requires one-way...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1102,
    "label": 0,
    "text": "A Social-Psychological Model for Synthetic Actors In the Virtual Theater project, we provide synthetic actors that portray fictive characters by improvising their behavior in a multimedia environment. Actors are either autonomous or avatars directed by users. Their improvisation is based on the directions they receive and the context. Directions can take different forms: high-level scenarios, user commands, and personality changes in the character portrayed. In this paper, we look at this last form of direction. We propose a social-psychological model, in which we can define personality traits that depend on the values of moods and attitudes. We show how such a model can be exploited by synthetic actors to produce performances that are theatrically interesting, believable, and diverse. An application, the Cybercaf\u00e9, is used to test those features.  Content Areas: synthetic actors, improvisation, believability. ____________________________________________________________________________________________  3  1. Introduction  In the Virt...",
    "neighbors": [
      821,
      841,
      918
    ],
    "mask": "Train"
  },
  {
    "node_id": 1103,
    "label": 0,
    "text": "A Scalable and Secure Global Tracking Service for Mobile Agents Abstract. In this paper, we propose a global tracking service for mobile agents, which is scalable to the Internet and accounts for security issues as well as the particularities of mobile agents (frequent changes in locations). The protocols we propose address agent impersonation, malicious location updates, as well as security issues that arise from profiling location servers, and threaten the privacy of agent owners. We also describe the general framework of our tracking service, and some evaluation results of the reference implementation we made.",
    "neighbors": [
      593,
      1065
    ],
    "mask": "Train"
  },
  {
    "node_id": 1104,
    "label": 2,
    "text": "Computing Geographical Scopes of Web Resources Many information resources on the web are relevant primarily to limited geographical communities. For instance, web sites containing information on restaurants, theaters, and apartment rentals are relevant primarily to web users in geographical proximity to these locations. In contrast, other information resources are relevant to a broader geographical community. For instance, an on-line newspaper may be relevant to users across the United States. Unfortunately, most current web search engines largely ignore the geographical scope of web resources. In this paper, we introduce techniques for automatically computing the geographical scope of web resources, based on the textual content of the resources, as well as on the geographical distribution of hyperlinks to them. We report an extensive experimental evaluation of our strategies using real web data. Finally, we describe a geographically-aware search engine that we have built using our techniques for determining the geographical scope of web resources. 1",
    "neighbors": [
      457,
      490,
      496,
      1000,
      1017,
      1134
    ],
    "mask": "Train"
  },
  {
    "node_id": 1105,
    "label": 4,
    "text": "Using Cinematography Conventions to Inform Guidelines For the Design and Evaluation of Virtual Off-Screen Space Many usability problems are associated with navigation and exploration of virtual space. In an attempt to find methods that support navigation within virtual space, this paper describes an investigation of cinematography conventions. In particular, this will focus on conventions that suggest to spectators the existence of additional space other than that contained within the confines or borders of the projection screen. Referred to as off-screen space, this paper builds upon these conventions and proposes guidelines to inform the design of visual cues to suggest virtual off-screen space. Visual cues will appear natural and transparent, they will help to guide participants through the smooth and continuously animated VE, and thus, maintain the illusion of interacting within a larger 3D virtual space than that contained within the restricted Field-Of-View (FOV) of the display screen. Introduction The 3 rd dimension of a Virtual Environment (VE) creates a space. Within ...",
    "neighbors": [
      376,
      1077
    ],
    "mask": "Train"
  },
  {
    "node_id": 1106,
    "label": 2,
    "text": "CREAM - Creating relational metadata with a component-based, ontology-driven annotation framework Richly interlinked, machine-understandable data constitutes the basis for  the Semantic Web. Annotating web documents is one of the major techniques for creating  metadata on the Web. However, annotation tools so far are restricted in their  capabilities of providing richly interlinked and truely machine-understandable data.  They basically allow the user to annotate with plain text according to a template structure,  such as Dublin Core. We here present CREAM (Creating RElational, Annotationbased  Metadata), a framework for an annotation environment that allows to construct  relational metadata, i.e. metadata that comprises class instances and relationship instances.  These instances are not based on a fix structure, but on a domain ontology.  We discuss some of the requirements one has to meet when developing such a framework,  e.g. the integration of a metadata crawler, inference services, document management  and information extraction, and describe its implementation, viz. Ont-O-Mat  a component-based, ontology-driven annotation tool.",
    "neighbors": [
      188,
      835
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1107,
    "label": 1,
    "text": "An Evolutionary Algorithm Using Multivariate Discretization for Decision Rule Induction We describe EDRL-MD, an evolutionary algorithm-based  system, for learning decision rules from databases. The main novelty of  our approach lies in dealing with continuous - valued attributes. Most of  decision rule learners use univariate discretization methods, which search  for threshold values for one attribute at the same time. In contrast to  them, EDRL-MD simultaneously searches for threshold values for all  continuous-valued attributes, when inducing decision rules. We call this  approach multivariate discretization. Since multivariate discretization is  able to capture interdependencies between attributes it may improve  the accuracy of obtained rules. The evolutionary algorithm uses problem  specific operators and variable-length chromosomes, which allows it to  search for complete rulesets rather than single rules. The preliminary  results of the experiments on some real-life datasets are presented.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1108,
    "label": 3,
    "text": "Beyond Eigenfaces: Probabilistic Matching for Face Recognition We propose a novel technique for direct visual matching of images for the purposes of face recognition and database search. Specifically, we argue in favor of a probabilistic measure of similarity, in contrast to simpler methods which are based on standard L2 norms (e.g., template matching) or subspace-restricted norms (e.g., eigenspace matching). The proposed similarity measure is based on a Bayesian analysis of image differences: we model two mutually exclusive classes of variation between two facial images: intra-personal (variations in appearance of the same individual, due to different expressions or lighting) and extra-personal (variations in appearance due to a difference in identity). The high-dimensional probability density functions for each respective class are then obtained from training data using an eigenspace density estimation technique and subsequently used to compute a similarity measure based on the a posteriori probability of membership in the intrapersonal class,...",
    "neighbors": [
      671,
      1060,
      1218
    ],
    "mask": "Train"
  },
  {
    "node_id": 1109,
    "label": 3,
    "text": "Quasi-Stable Semantics of Logic Programs this paper, we introduce a new semantic theory for logic programs. We choose the Well-Founded Semantics (WFS) [23] as our starting point because it has many desirable features. For any logic program there exists a unique well-founded partial model which can be defined in a constructive way. WFS naturally extends the semantics for a large class of logic programs, including stratified and locally stratified programs (see [17] and [23]). Despite its merits, it can be argued that the WFS is too \"sceptical\". For many programs it gives the empty set as their intended meaning. It is thus silent on all atoms though it may be reasonable to expect that something should be concluded from the information given. For an illustrative example, see program P 1 in section 4. There are many proposals for extending the WFS such as the generalised well-founded semantics GWFS [1], the well-founded-by-case-semantics WFSC [20], the extended well-founded semantics WFSE [11], the strong well-founded semantics WFS S [5], and the O-semantics [15]. The relationship between these semantic theories for logic programming is also studied in [6]. Here we propose another nondeterministic extension of the WFS. The other dominant semantic model for logic programs is the stable model semantics [10]. Compared with the WFS, the stable model semantics is \"credulous\": it derives much more information than the WFS though this is non-deterministic or \"disjunctive\" in the sense that there may be several stable models for a given logic program. The chief drawback of this semantics is that a stable model is not defined for all logic programs. In addition stable model semantics gives rise to anomalies in some circumstances. See the program P 3 in section 4, borrowed from [23]. A modification of the stable model seman...",
    "neighbors": [
      843
    ],
    "mask": "Train"
  },
  {
    "node_id": 1110,
    "label": 5,
    "text": "A Web-based Intelligent Tutoring System Using Hybrid Rules as its Representational Basis In this paper, we present the architecture and describe the functionality of a Web-based Intelligent Tutoring System (ITS), which uses neurules for knowledge representation. Neurules are a type of hybrid rules integrating symbolic rules with neurocomputing. The use of neurules as the knowledge representation basis of the ITS results in a number of advantages.",
    "neighbors": [
      1257
    ],
    "mask": "Train"
  },
  {
    "node_id": 1111,
    "label": 1,
    "text": "A Variable Depth Search Algorithm for the Generalized Assignment Problem : A variable depth search procedure (abbreviated as VDS) is a generalization of the local search method, which was rst successfully applied by Lin and Kernighan to the traveling salesman problem and the graph partitioning problem. The main idea is to adaptively change the size of neighborhood so that it can eectively traverse larger search space while keeping the amount of computational time reasonable. In this paper, we propose a heuristic algorithm based on VDS for the generalized assignment problem, which is one of the representative combinatorial optimization problems known to be NP-hard. To the authors' knowledge, most of the previously proposed algorithms (with some exceptions) conduct the search within the feasible region; however, there are instances for which the search within feasible region is not advantageous because the feasible region is very small or is combinatorially complicated to search. Therefore, we allow in our algorithm to search into the infeasible region as w...",
    "neighbors": [
      338
    ],
    "mask": "Train"
  },
  {
    "node_id": 1112,
    "label": 1,
    "text": "Calibrating Parameters of Cost Functionals We propose a new framework for calibrating parameters of energy  functionals, as used in image analysis. The method learns parameters  from a family of correct examples, and given a probabilistic construct  for generating wrong examples from correct ones. We introduce a  measure of frustration to penalize cases in which wrong responses are  preferred to correct ones, and we design a stochastic gradient algorithm  which converges to parameters which minimize this measure of  frustration. We also present a first set of experiments in this context,  and introduce extensions to deal with data-dependent energies.  keywords: Learning, variational method, parameter estimation, image reconstruction, Bayesian image models  1  1 Description of the method  Many problems in computer vision are addressed through the minimization of a cost functional U . This function is typically defined on a large, finite, set \\Omega  (for example the set of pictures with fixed dimensions), and the minimizer of x ...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1113,
    "label": 0,
    "text": "Feasible Formations of Multi-Agent Systems Formations of multi-agent systems, such as mobile robots, satellites. and aircraft, require individual agents to satisfy their kinematic equations while constantly maintaining inter-agent constraints. In this paper, we develop a systematic framework for studying formation feasibility of multi-agent systems. In particular, we consider undirected formations for centralized formations, and directed formations for decentralized formations. In each case, we determine differential geometric conditions that guarantee formation feasibility given the individual agent kinematics. Our framework also enables us to extract a smaller control system that describes the group kinematics while maintaining all formation constraints.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1114,
    "label": 2,
    "text": "Medical Document Information Retrieval through Active User Interfaces This paper reports our preliminary design and implementation towards the development of Kavanah, a system to help users retrieve information and discover knowledge for a medical domain application. The goal of this system is to adaptively react to the dynamic changes in the user's interests and preferences in searching for information within the context of the on-going information retrieval task. The context in which the user seeks information is modeled by an active user interface through analyzing the user's interactions with the system to dynamically construct an ontology of concepts representing the user's information seeking context. We implement the system using Unified Medical Language System knowledge base as a test bed.",
    "neighbors": [
      607,
      780
    ],
    "mask": "Train"
  },
  {
    "node_id": 1115,
    "label": 4,
    "text": "Agent-based Distributed Planning and Scheduling in Global Manufacturing Scheduling and resource allocation problems are pervasive and important in the management of industrial and government organizations. With advent of new technology and fast evolvement in industry, the enterprise is gradually moving toward global manufacturing for efficient operational management and competent strategic decision making. In the past two decades, researchers and practitioners have been applying various techniques (such as artificial intelligence, optimization methodologies, information system design, human factors, etc.) to design and develop planning and scheduling methodologies and systems for different applications. However, the inherent complexity of problems, short life-cycle of planner/scheduler, unrealistic investment of generous-purpose systems, and profligacy of scattering computing resource, accessibility, integration and re-configurability become the essential factors for new planners/schedulers in global manufacturing. The specific objectives of this research ...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1116,
    "label": 3,
    "text": "Querying as an Enabling Technology in Software Reengineering In this paper it is argued that different kinds of reengineering technologies can be based on querying. Several reengineering technologies are presented as being integrated into a technically oriented reengineering taxonomy. The usefulness of querying is pointed out with respect to these reengineering technologies.  To impose querying as a base technology in reengineering examples are given with respect to the EER/GRAL approach to conceptual modeling and implementation. This approach is presented together with GReQL as its query part. The different reengineering technologies are finally reviewed in the context of the GReQL query facility.  1 Introduction  Reengineering may be viewed as any activity that either improves the understanding of a software or else improves the software itself [2].  According to this view software reengineering can be \"partitioned\" into two kinds of activities. The first kind of activities is concerned with understanding such as source code retrieval, browsin...",
    "neighbors": [
      1195
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1117,
    "label": 5,
    "text": "A New Clustering Algorithm For Segmentation Of Magnetic Resonance Images",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1118,
    "label": 1,
    "text": "Face Recognition Using Shape and Texture We introduce in this paper a new face coding and recognition method which employs the Enhanced FLD (Fisher Linear Discrimimant) Model (EFM) on integrated shape (vector) and texture (`shape-free' image) information. Shape encodes the feature geometry of a face while texture provides a normalized shape-free image by warping the original face image to the mean shape, i.e., the average of aligned shapes. The dimensionalities of the shape and the texture spaces are first reduced using Principal Component Analysis (PCA). The corresponding but reduced shape and texture features are then integrated through a normalization procedure to form augmented features. The dimensionality reduction procedure, constrained by EFM for enhanced generalization, maintains a proper balance between the spectral energy needs of PCA for adequate representation, and the FLD discrimination requirements, that the eigenvalues of the within-class scatter matrix should not include small trailing values after the dimensionality reduction procedure as they appear in the denominator.",
    "neighbors": [
      330,
      949
    ],
    "mask": "Train"
  },
  {
    "node_id": 1119,
    "label": 0,
    "text": "Design-to-Criteria Scheduling: Real-Time Agent Control Design-to-Criteria builds custom schedules for agents that  meet hard temporal constraints, hard resource constraints,  and soft constraints stemming from soft task interactions or  soft commitments made with other agents. Design-to-Criteria  is designed specifically for online application -- it copes with  exponential combinatorics to produce these custom schedules  in a resource bounded fashion. This enables agents to respond  to changes in problem solving or the environment as  they arise.  Introduction  Complex autonomous agents operating in open, dynamic environments must be able to address deadlines and resource limitations in their problem solving. This is partly due to characteristics of the environment, and partly due to the complexity of the applications typically handled by software agents in our research. In open environments, requests for service can arrive at the local agent at any time, thus making it difficult to fully plan or predict the agent's future workload. In dyn...",
    "neighbors": [
      495,
      500,
      847
    ],
    "mask": "Train"
  },
  {
    "node_id": 1120,
    "label": 3,
    "text": "Web Log Data Warehousing and Mining for Intelligent Web Caching We introduce intelligent web caching algorithms that employ predictive models of web requests; the general idea is to extend the LRU policy of web and proxy servers by making it sensible to web access models extracted from web log data using data mining techniques. Two approaches have been studied in particular, frequent patterns and decision trees. The experimental results of the new algorithms show substantial improvement over existing LRU-based caching techniques, in terms of hit rate. We designed and developed a prototypical system, which supports data warehousing of web log data, extraction of data mining models and simulation of the web caching algorithms.",
    "neighbors": [
      9,
      112
    ],
    "mask": "Train"
  },
  {
    "node_id": 1121,
    "label": 1,
    "text": "A General Learning Approach to Multisensor Based Control using Statistic Indices We propose a concept for integrating multiple sensors in real-time robot control. To increase the controller robustness under diverse uncertainties, the robot systematically generates series of sensor data (as robot state) while memorising the corresponding motion parameters. From the collection of (multi-) sensor trajectories, statistical indices like principal components for each sensor type can be extracted. If the sensor data are preselected as output relevant, these principal components can be used very efficiently to approximately represent the original perception scenarios. After this dimension reduction procedure, a non-linear fuzzy controller, e.g. a B-spline type, can be trained to map the subspace projection into the robot control parameters. We apply the approach to a real robot system with two arms and multiple vision and force/torque sensors. These external sensors are used simultaneously to control the robot arm performing insertion and screwing operations. The successful experiments show that the robustness as well as the precision of robot control can be enhanced by integrating multiple additional sensors using this concept. 1",
    "neighbors": [
      79
    ],
    "mask": "Train"
  },
  {
    "node_id": 1122,
    "label": 2,
    "text": "Active Learning of Partially Hidden Markov Models We consider the task of learning hidden Markov models (HMMs) when only partially (sparsely) labeled observation sequences are available for training. This setting is motivated by the information extraction problem, where only few tokens in the training documents are given a semantic tag while most tokens are unlabeled. We first describe the partially hidden Markov model together with an algorithm for learning HMMs from partially labeled data. We then present an active learning algorithm that selects \"difficult\" unlabeled tokens and asks the user to label them. We study empirically by how much active learning reduces the required data labeling effort, or increases the quality of the learned model achievable with a given amount of user effort.",
    "neighbors": [
      201,
      279,
      379,
      875,
      1024
    ],
    "mask": "Train"
  },
  {
    "node_id": 1123,
    "label": 1,
    "text": "Continuous-based Heuristics for Graph and Tree Isomorphisms, with Application to Computer Vision We present a new (continuous) quadratic programming approach for graph- and tree-isomorphism problems which is based on an equivalent maximum clique formulation. The approach is centered around a fundamental result proved by Motzkin and Straus in the mid-1960s, and recently expanded in various ways, which allows us to formulate the maximum clique problem in terms of a standard quadratic program. The attractive feature of this formulation is that a clear one-to-one correspondence exists between the solutions of the quadratic programs and those in the original, combinatorial problems. To approximately solve the program we use the so-called \"replicator\" equations, a class of straightforward continuous- and discrete-time dynamical systems developed in various branches of theoretical biology. We show how, despite their inherent inability to escape from local solutions, they nevertheless provide experimental results which are competitive with those obtained using more sophisticated mean-fiel...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1124,
    "label": 2,
    "text": "A Methodology to Retrieve Text Documents from Multiple Databases In this paper, we present a methodology for finding the n most similar documents across multiple text  databases for any given query and for any positive integer n. This methodology consists of two steps.  First, the contents of databases are indicated approximately by database representatives. Databases  are ranked using their representatives in a certain order with respect to the given query. We provide  a necessary and sufficient condition to rank the databases optimally. In order to satisfy this necessary  and sufficient condition, we provide three estimation methods. One estimation method is intended for  short queries; the other two are for all queries. Second, we provide an algorithm, OptDocRetrv, to  retrieve documents from the databases according to their rank and in a particular way. We show that  if the databases containing the n most similar documents for a given query are ranked ahead of other  databases, our methodology will guarantee the retrieval of the n most similar d...",
    "neighbors": [
      224,
      271,
      435,
      477,
      502,
      696,
      792,
      897,
      931,
      1003,
      1017,
      1165
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1125,
    "label": 2,
    "text": "Monadic Datalog and the Expressive Power of Languages for Web Information Extraction Research on information extraction from Web pages (wrapping) has seen much activity in recent times (particularly systems implementations), but little work has been done on formally studying the expressiveness of the formalisms proposed or on the theoretical foundations of wrapping. In this paper, we first study monadic datalog as a wrapping language (over ranked or unranked tree structures). Using previous work by Neven and Schwentick, we show that this simple language is equivalent to full monadic second order logic (MSO) in its ability to specify wrappers. We believe that MSO has the right expressiveness required for Web information extraction and thus propose MSO as a yardstick for evaluating and comparing wrappers. Using the above result, we study the kernel fragment Elog- of the Elog wrapping language used in the Lixto system (a visual wrapper generator). The striking fact here is that Elog- exactly captures MSO, yet is easier to use. Indeed, programs in this language can be entirely visually specified. We also formally compare Elog to other wrapping languages proposed in the literature.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1126,
    "label": 2,
    "text": "Weight Adjustment Schemes for a Centroid Based Classifier In recent years we have seen a tremendous growth in the volume of text documents available on the Internet, digital  libraries, news sources, and company-wide intra-nets. Automatic text categorization, which is the task of assigning  text documents to pre-specified classes (topics or themes) of documents, is an important task that can help both in  organizing as well as in finding information on these huge resources. Similarity based categorization algorithms such  as k-nearest neighbor, generalized instance set and centroid based classification have been shown to be very effective  in document categorization. A major drawback of these algorithms is that they use all features when computing the  similarities. In many document data sets, only a small number of the total vocabulary may be useful for categorizing  documents. A possible approach to overcome this problem is to learn weights for different features (or words in  document data sets). In this report we present two fast iterativ...",
    "neighbors": [
      545,
      726,
      1049
    ],
    "mask": "Train"
  },
  {
    "node_id": 1127,
    "label": 3,
    "text": "OMS Rapid Prototyping System for the Development of Object-Oriented Database Application Systems We present an object-oriented data model and system that supports the development of database application systems through a combination of rapid prototyping and refinement. Prototyping is performed on an abstract application model and is independent of any implementation platform -- supporting not only the design stage, but also analysis and requirements modelling. The underlying model, OM, has a two-level structure which reflects the two aspects of object-oriented database application systems -- data modelling and programming. We are therefore able to cleanly integrate ideas from both the database and software engineering communities and achieve compatibility with object models used in both.  Keywords: Object-oriented database systems, rapid prototyping, database design. 1 Introduction  The design and implementation of object-oriented database application systems presents a software engineering challenge in that it encompasses both application program development and database design. ...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1128,
    "label": 3,
    "text": "Top N optimization issues in MM databases Introduction  In multi media (MM) DBMSs the usual way of operation in case of a MM retrieval query is to compute some ranking based on statistics and distances in feature spaces. The MM objects are then sorted by descending relevance relative to the given query. Since users are limited in their capabilities of reviewing all objects in that ranked list only a reasonable top of say N objects is returned.  However, this can turn out to be a quite time consuming process. The first reason is that the number of objects (i.e. documents) in the DBMS is usually very large (10  6  or even more). From the information retrieval field it is known that usually half of all objects (e.g. documents) contains at least one query term; so, even considering only these objects might be very time consuming. The same may hold for MM in general.  The problem of top N MM query optimization is to find techniques to limit the set of objects taken into consideration during the",
    "neighbors": [
      78,
      1057
    ],
    "mask": "Test"
  },
  {
    "node_id": 1129,
    "label": 4,
    "text": "Wearable Visual Robots This paper presents a wearable active visual sensor which is able to achieve a level of decoupling of camera movement from the wearer\u2019s posture and movements. This decoupling is the result of a combination of an active sensing approach, inertial information and visual sensor feedback. The issues of sensor placement, robot kinematics and their relation to wearability are discussed. The performance of the prototype head is evaluated for some essential visual tasks. The paper also discusses potential application scenarios for this kind of wearable robot. 1",
    "neighbors": [
      334,
      531
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1130,
    "label": 4,
    "text": "Grouplab at SkiGraph Collaboration among distributed workgroup members is hampered by the lack of good tools to support informal interactions. These tools either fail to provide teleawareness or enable smooth transitions into and out of informal interactions. Video media spaces---always-on video links---have been proposed as a solution to this problem. However, the \"always-on\" nature of video media spaces results in a conflict between the desire to provide awareness and the need to preserve privacy. The present study examines distortion filtration applied to always-on video as means of resolving this tension. Our discussions include the inter-related concepts of informal interactions, awareness, and privacy; and the treatment afforded by existing distributed collaboration support tools. We then outline the present study, where our goal is to understand the effect of distortion filtration on awareness and privacy.  Keywords  Tele-awareness, telepresence, privacy, informal interaction, video media spaces, di...",
    "neighbors": [
      628
    ],
    "mask": "Train"
  },
  {
    "node_id": 1131,
    "label": 5,
    "text": "Lessons Learned from the Scientist's Expert Assistant Project During the past two years, the Scientist's Expert Assistant (SEA) team has been prototyping proposal development tools for the Hubble Space Telescope in an effort to demonstrate the role of software in reducing support costs for the Next Generation Space Telescope (NGST). This effort has been a success. The Hubble Space Telescope has adopted two SEA prototype tools, the Exposure Time Calculator and Visual Target Tuner, for operational use. The Space Telescope Science Institute is building a new set of observing tools based on SEA technology. These tools will hopefully be a foundation that is easily adaptable to other observatories including NGST.  The SEA project has aggressively pursued the latest software technologies including Java, distributed computing, XML, Web distribution, and expert systems. Some technology experiments proved to be dead ends, while other technologies were unexpectedly beneficial. We have also worked with other projects to foster collaboration between the vario...",
    "neighbors": [
      1217
    ],
    "mask": "Train"
  },
  {
    "node_id": 1132,
    "label": 3,
    "text": "Indexing Spatio-Temporal Data Warehouses Spatio-temporal databases store information about the positions of individual objects over time. In many applications however, such as traffic supervision or mobile communication systems, only summarized data, like the average number of cars in an area for a specific period, or phones serviced by a cell each day, is required. Although this information can be obtained from operational databases, its computation is expensive, rendering online processing inapplicable. A vital solution is the construction of a spatiotemporal data warehouse. In this paper, we describe a framework for supporting OLAP operations over spatiotemporal data. We argue that the spatial and temporal dimensions should be modeled as a combined dimension on the data cube and present data structures, which integrate spatiotemporal indexing with pre-aggregation. While the well-known materialization techniques require a-priori knowledge of the grouping hierarchy, we develop methods that utilize the proposed structures for efficient execution of ad-hoc group-bys. Our techniques can be used for both static and dynamic dimensions.  1.",
    "neighbors": [
      863
    ],
    "mask": "Train"
  },
  {
    "node_id": 1133,
    "label": 2,
    "text": "Transductive Inference for Text Classification using Support Vector Machines This paper introduces Transductive Support Vector Machines (TSVMs) for text classification.  While regular Support Vector Machines  (SVMs) try to induce a general decision  function for a learning task, Transductive  Support Vector Machines take into account  a particular test set and try to minimize  misclassifications of just those particular  examples. The paper presents an analysis  of why TSVMs are well suited for text  classification. These theoretical findings are  supported by experiments on three test collections.  The experiments show substantial  improvements over inductive methods, especially  for small training sets, cutting the number  of labeled training examples down to a  twentieth on some tasks. This work also proposes  an algorithm for training TSVMs efficiently,  handling 10,000 examples and more.",
    "neighbors": [
      242,
      407,
      437,
      523,
      865,
      1090,
      1240
    ],
    "mask": "Test"
  },
  {
    "node_id": 1134,
    "label": 2,
    "text": "Techniques for Specialized Search Engines It is emerging that it is very difficult for the major search engines to provide a comprehensive and up-to-date search service of the Web. Even the largest search engines index only a small proportion of static Web pages and do not search the Web' s backend databases that are estimated to be 500 times larger than the static Web. The scale of such searching introduces both technical and economic problems. What is more, in many cases users are not able to retrieve the information they desire because of the simple and generic search interface provided by the major search engines. A necessary response to these search problems is the creation of specialized search engines. These search engines search just for information in a particular topic or category on the Web. Such search engines will have smaller and more manageable indexes and have a powerful domainspecific search interface. This paper discusses the issues in this area and gives an overview of the techniques for building specialized search engines. Keywords: specialized search engine, information retrieval, focused crawling, taxonomy, Web search. 1.",
    "neighbors": [
      433,
      466,
      587,
      595,
      653,
      1017,
      1104,
      1253
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1135,
    "label": 1,
    "text": "Examining Locally Varying Weights for Nearest Neighbor Algorithms . Previous work on feature weighting for case-based learning algorithms has tended to use either global weights or weights that vary over extremely local regions of the case space. This paper examines the use of coarsely local weighting schemes, where feature weights are allowed to vary but are identical for groups or clusters of cases. We present a new technique, called class distribution weighting (CDW), that allows weights to vary at the class level. We further extend CDW into a family of related techniques that exhibit varying degrees of locality, from global to local. The class distribution techniques are then applied to a set of eleven concept learning tasks. We find that one or more of the CDW variants significantly improves classification accuracy for nine of the eleven tasks. In addition, we find that the relative importance of classes, features, and feature values in a particular domain determines which variant is most successful. 1 Introduction  The k-nearest-neighbor (k-NN)...",
    "neighbors": [
      611,
      731,
      1045,
      1075
    ],
    "mask": "Train"
  },
  {
    "node_id": 1136,
    "label": 1,
    "text": "Evolutionary Design and Multi-objective Optimisation : In this paper we explore established methods for optimising multi-objective functions whilst addressing the problem of preliminary design. Methods from the literature are investigated and new ones introduced. All methods are evaluated within a collaborative project for whole system airframe design and the basic problems and difficulties of preliminary design methodology are discussed (Cvetkovic, Parmee and Webb 1998). Our Genetic Algorithm is expanded to integrate different methods for optimising multi--objective functions. All presented methods are also analysed in the context of whole system design, discussing their advantages and disadvantages. The problem of qualitative versus quantitative characterisation of relative importance of objectives (such as `objective A is much more important then objective B') in multi--objective optimisation framework is also addressed and some relationships with fuzzy preferences (Fodor and Roubens 1994) and preference ordering established. Several ...",
    "neighbors": [
      534
    ],
    "mask": "Train"
  },
  {
    "node_id": 1137,
    "label": 0,
    "text": "Using Motives and Artificial Emotion for Long-Term Activity of an Autonomous Robot To operate over a long period of time in the real world, autonomous mobile robots must have the capabilityofrecharging themselves whenever necessary. In addition to be able to nd and dockintoacharging station, robots must be able to decide when and for how long to recharge. This decision is inuenced by the energetic capacity of their batteries and the contingencies of their environments. To deal with this temporality issue and based on researchworks in psychology, this paper investigates the use of motives and articial emotions to regulate the recharging need of autonomous robots. A bipolar model of articial emotion is presented, designed to be generic and not specically congured for a particular task. The paper also describes the use of the approach in two specic applications, the AAAI Mobile Robot Challenge and experiments involving a group of robots share one charging station in an enclosed area.  1.",
    "neighbors": [
      123,
      353
    ],
    "mask": "Train"
  },
  {
    "node_id": 1138,
    "label": 2,
    "text": "The OLAC Metadata Set and Controlled Vocabularies As language data and associated  technologies proliferate and as the  language resources community rapidly  expands, it has become difficult to  locate and reuse existing resources.  Are there any lexical resources for  such-and-such a language? What  tool can work with transcripts in  this particular format? What is a  good format to use for linguistic data  of this type? Questions like these  dominate many mailing lists, since  web search engines are an unreliable  way to find language resources.  This paper describes a new digital  infrastructure for language resource  discovery, based on the Open Archives  Initiative, and called OLAC -- the  Open Language Archives Community.  The OLAC Metadata Set and the  associated controlled vocabularies  facilitate consistent description and  focussed searching. We report progress  on the metadata set and controlled  vocabularies, describing current issues  and soliciting input from the language  resources community.  1",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1139,
    "label": 0,
    "text": "Towards Multi-Swarm Problem Solving in Networks This paper describes how multiple interacting swarms of adaptive mobile agents can be used to solve problems in networks. The paper introduces a new architectural description for an agent that is chemically inspired and proposes chemical interaction as the principal mechanism for inter-swarm communication. Agents within a given swarm have behavior that is inspired by the foraging activities of ants, with each agent capable of simple actions and knowledge of a global goal is not assumed. The creation of chemical trails is proposed as the primary mechanism used in distributed problem solving arising from self-organization of swarms of agents. The paper proposes that swarm chemistries can be engineered in order to apply the principal ideas of the Subsumption Architecture in the domain of mobile agents. The paper presents applications of the new architecture in the domain of communications networks and describes the essential elements of a mobile agent framework that is being considered fo...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1140,
    "label": 1,
    "text": "Rule Induction of Computer Events Introduction  Monitoring systems are able to capture an assortment of different events from a network environment. A single event signals an abnormal situation on either one host, e.g., \"cpu utilization is above a critical threshold\", or the network, e.g., \"communication link is down\". A monitoring system can capture thousands of events in a short time period (several days); applying data analysis techniques (e.g., machine-learning, data-mining) on those events may reveal useful patterns characterizing a network problem. Data analysis techniques have proved useful in the area of network fault management.  In this paper we consider the following scenario. A computer network is under continuous monitoring; a user is interested in identifying what triggers a specific kind of events, which we refer to as target events. The user would like to know what events correlate to each target event within a fixed time window. We describe a data-mining technique that takes as input",
    "neighbors": [
      172
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1141,
    "label": 2,
    "text": "Learning to Order Things wcohen,schapire,singer\u00a1 There are many applications in which it is desirable to order rather than classify instances. Here we consider the problem of learning how to order, given feedback in the form of preference judgments, i.e., statements to the effect that one instance should be ranked ahead of another. We outline a two-stage approach in which one first learns by conventional means a preference function, of the form PREF\u00a2\u00a4\u00a3\u00a6\u00a5\u00a8\u00a7\ufffd \u00a9, which indicates whether it is advisable to rank \u00a3 before \u00a7. New instances are then ordered so as to maximize agreements with the learned preference function. We show that the problem of finding the ordering that agrees best with a preference function is NP-complete, even under very restrictive assumptions. Nevertheless, we describe a simple greedy algorithm that is guaranteed to find a good approximation. We then discuss an on-line learning algorithm, based on the \u201cHedge \u201d algorithm, for finding a good linear combination of ranking \u201cexperts.\u201d We use the ordering algorithm combined with the on-line learning algorithm to find a combination of \u201csearch experts, \u201d each of which is a domain-specific query expansion strategy for a WWW search engine, and present experimental results that demonstrate the merits of our approach. 1",
    "neighbors": [
      505,
      674,
      818,
      937,
      1007,
      1056,
      1068,
      1144
    ],
    "mask": "Train"
  },
  {
    "node_id": 1142,
    "label": 4,
    "text": "A Brief History of Human Computer Interaction Technology This article summarizes the historical development of major advances in humancomputer interaction technology, emphasizing the pivotal role of university research in the advancement of the field. Copyright 1996 --- Carnegie Mellon University A short excerpt from this article appeared as part of \"Strategic Directions in Human Computer Interaction,\" edited by Brad Myers, Jim Hollan, Isabel Cruz, ACM Computing Surveys, 28(4), December 1996 This research was partially sponsored by NCCOSC under Contract No. N66001-94-C-6037, Arpa Order No. B326 and partially by NSF under grant number IRI-9319969. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NCCOSC or the U.S. Government.  Keywords: Human Computer Interaction, History, User Interfaces, Interaction Techniques.  Brief History of HCI - 1 1. Introduction  Research in Human-Computer Interaction (HCI) has been spec...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1143,
    "label": 0,
    "text": "Sensory-Motor Primitives as a Basis for Imitation: Linking Perception to Action and Biology to Robotics ing away from the specific coding of the spinal fields, the examples from neurobiology provide the framework for a motor control system based on a small number of additive primitives (or basis behaviors) sufficient for a rich output movement repertoire. Our previous work (Matari'c 1995, Matari'c 1997), inspired by the same biological results, has successfully applied the idea of basis behaviors to control of mobile robots 6  by fitting it directly into the modular behavior-based control paradigm. Applictions of schema theory (Arbib 1992) to behavior-based mobile robots (Arkin 1987) have employed a similar notion of composable behaviors, stemming from foundations in neuroscience (Arbib 1981, Arbib 1989).  The idea of using such primitives for articulator control has been recently studied in robotics. Williamson (1996) and Marjanovi'c, Scassellati & Williamson (1996) developed a 6 DOF (degrees of freedom) robot arm controller. While in the biological and mobile robotics work primitives c...",
    "neighbors": [
      183,
      509
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1144,
    "label": 2,
    "text": "Integration of Heterogeneous Databases Without Common Domains Using Queries Based on Textual Similarity Most databases contain \"name constants\" like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. However, in many cases, this assumption does not hold; determining if two name constants should be considered identical can require detailed knowledge of the world, the purpose of the user's query, or both. In this paper, we reject the assumption that global domains can be easily constructed, and assume instead that the names are given in natural language text. We then propose a logic called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. We describe an efficient implementation of WHIRL and evaluate it experimentally on data extracted from the World Wide Web. We show that WHIR...",
    "neighbors": [
      322,
      395,
      507,
      1141,
      1233
    ],
    "mask": "Train"
  },
  {
    "node_id": 1145,
    "label": 3,
    "text": "Applications of Quick-Combine for Ranked Query Models In digital libraries queries are often based on the similarity of objects, using several feature attributes like colors, texture or full-text searches. Such multi-feature queries return a ranked result set instead of exact matches. Recently we presented a new algorithm called Quick-Combine [5] for combining multi-feature result lists, guaranteeing the correct retrieval of the k top-ranked results. As benchmarks on practical data promise that we can dramatically improve performance, we want to discuss interesting applications of Quick-Combine in different areas. The applications for the optimization in ranked query models are manifold. Generally speaking we believe that all kinds of federated searches can be supported like e.g. content-based retrieval, knowledge management systems or multi-classifier combination.",
    "neighbors": [
      507,
      555
    ],
    "mask": "Test"
  },
  {
    "node_id": 1146,
    "label": 5,
    "text": "A Randomized Approach to Planning Biped Locomotion with Prescribed Motions In this paper, we present a new scheme for planning a natural-looking locomotion of a human-like biped figure. Given start and goal positions in a virtual environment, our scheme finds a sequence of motions to move from the start position to the goal using a set of prescribed, live-captured motion clips. Our scheme consists of three parts: roadmap construction, roadmap search, and motion generation. We randomly sample a set of valid configurations of the biped figure for the environment to construct a directed graph, called a roadmap, that guides the locomotion of the figure. Every edge of the roadmap is attached with a live-captured motion clip. Traversing the roadmap, we obtain the sequence of footprints and that of motion clips. We finally adapt the motion sequence to the constraints specified by the footprint sequence to obtain the locomotion.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1147,
    "label": 3,
    "text": "Large Scale Terrain Visualization Using The Restricted Quadtree Triangulation Real-time rendering of triangulated surfaces has attracted growing interest in the last few years. However, interactive visualization of very large scale grid digital elevation models is still a hard problem. The graphics load must be controlled by an adaptive surface triangulation and by taking advantage of different levels of detail. Furthermore, the management of the visible scene requires efficient access to the terrain database. We describe a all-in-one visualization system which integrates adaptive triangulation, dynamic scene management and spatial data handling. The triangulation model is based on the restricted quadtree triangulation. Furthermore, we present new algorithms of the restricted quadtree triangulation. These include among others exact error approximation, progressive meshing, performance enhancements and spatial access.  Keywords algorithms, computer graphics, virtual reality, triangulated surfaces, terrain visualization, terascale visualization 1. Introduction  In...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1148,
    "label": 2,
    "text": "Structuring and Querying the Web through Graph-Oriented Languages . In order to pose effective queries to Web sites, some form of site data model must be implicitly or explicitly shared by users. Many approaches try to compensate for the lack of such a common model by considering the hypertextual structure of Web sites; unfortunately, this structure has usually little to do with data semantics. In this paper a different technique is proposed that allows for both navigational and data model description of Web sites, while allowing for graphical queries. The data model is based on WGlog, a description and query language based on the graph-oriented object database model of GOOD [Gys94] and G-log [Par95] allowing description of data manipulation primitives via graph transformations. WG-log description of the navigational part of a Web site schema is lexically based on standard hypermedia design languages, thus allowing for easy schema generation by current hypermedia authoring environments. The use of WG-log for queries allows graphic query construction ...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1149,
    "label": 1,
    "text": "Case-Based Classification Using Similarity-Based Retrieval Classification involves associating instances with particular classes by maximizing intra-class similarities and minimizing inter-class similarities. The paper presents a novel approach to case-based classification. The algorithm is based on a notion of similarity assessment and was developed for supporting flexible retrieval of relevant information. Validity of the proposed approach is tested on real world domains, and the system's performance is compared to that of other machine learning algorithms. 1 Introduction  Classification involves associating instances with particular classes; based on the object description, the classification system determines whether a given object belongs to a specified class. In general, this process consists of: first generating a set of categories and then classifying given objects into the created categories. For the purpose of this paper, it is assumed that the categories are known a priori from a prescribed domain theory [38]. Various reasoning tech...",
    "neighbors": [
      566
    ],
    "mask": "Train"
  },
  {
    "node_id": 1150,
    "label": 3,
    "text": "Supporting Imprecision in Multidimensional Databases Using Granularities On-Line Analytical Processing (OLAP) technologies are being used widely for business-data analysis, and these technologies are also being used increasingly in medical applications, e.g., for patient-data analysis. The lack of effective means of handling data imprecision, which occurs when exact values are not known precisely or are entirely missing, represents a major obstacle in applying OLAP technology to the medical domain, as well as many other domains. OLAP systems are mainly based on a multidimensional model of data and include constructs such as dimension hierarchies and granularities. This paper develops techniques for the handling of imprecision that aim to maximally reusing these already existing constructs. With imprecise data now available in the database, queries are tested to determine whether or not they may be answered precisely given the available data; if not, alternative queries that are unaffected by the imprecision are suggested. When a user elects to proceed with a query that is affected by imprecision, techniques are proposed that take into account the imprecision in the grouping of the data, in the subsequent aggregate computation, and in the presentation of the imprecise result to the user. The approach is capable of exploiting existing multidimensional query processing techniques such as pre-aggregation, yielding an effective approach with low computational overhead and that may be implemented using current technology. The paper illustrates how to implement the approach using SQL databases.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1151,
    "label": 1,
    "text": "The Spectral Independent Components Of Natural Scenes Abstract. We apply independent component analysis (ICA) for learning an efficient color image representation of natural scenes. In the spectra of single pixels, the algorithm was able to find basis functions that had a broadband spectrum similar to natural daylight, as well as basis functions that coincided with the human cone sensitivity response functions. When applied to small image patches, the algorithm found homogeneous basis functions, achromatic basis functions, and basis functions with overall chromatic variation along lines in color space. Our findings suggest that ICAmay be used to reveal the structure of color information in natural images. 1 Learning Codes for Color Images The efficient encoding of visual sensory information is an important task for image processing systems as well as for the understanding of coding principles in the visual cortex. Barlow [1] proposed that the goal of sensory information processing is to transform the input signals such that it reduces the redundancy",
    "neighbors": [
      642
    ],
    "mask": "Train"
  },
  {
    "node_id": 1152,
    "label": 1,
    "text": "Facial feature detection by Saccadic Exploration of the Gabor Decomposition The Gabor decomposition is a ubiquitous tool in computer vision. Nevertheless, it is generally considered computationally demanding for active vision applications. We suggest an attention-driven approach to feature detection inspired by the human saccadic system. A dramatic speedup is achieved by computing the Gabor decomposition only on the points of a sparse retinotopic grid. An application to eye detection is presented. Also, a real-time head detection and tracking system based on our approach is briefly discussed. The system features a novel eyeball-mounted camera designed to mimic the dynamic performance of the human eye and is, to the best of our knowledge, the first example of active vision system based on the Gabor decomposition.",
    "neighbors": [
      365
    ],
    "mask": "Train"
  },
  {
    "node_id": 1153,
    "label": 1,
    "text": "Text Classification from Labeled and Unlabeled Documents using EM  This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.  We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%.",
    "neighbors": [
      167,
      242,
      347,
      403,
      407,
      437,
      523,
      891
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1154,
    "label": 1,
    "text": "The Necessity Of User Guidance In Case-Based Knowledge Acquisition The intention of the present paper is to justify both theoretically and experimentally that user guidance is inevitable in case-based knowledge acquisition. The methodology of our approach is simple: We choose some paradigmatic idea of case-based learning which can be very briefly expressed as follows:  Given any CBR system, apply it. Whenever it works sucessfully, do not change it. Whenever it fails on some input case, add this experience to the case base. Don't do anything else. Then, we perform a number of knowledge acquisition experiments. They clearly exhibit essential limitations of knowledge acquisition from randomly chosen cases. As a consequence, we develop scenarios of user guidance. Based on these theoretical concepts, we prove a few theoretical results characterizing the power of our approach. Next, we perform a new series of more constrained results which support our theoretical investigations. This paper is based on more than 1 000 000 runs of case-based knowledge acquisi...",
    "neighbors": [
      776,
      975,
      1259,
      1268
    ],
    "mask": "Train"
  },
  {
    "node_id": 1155,
    "label": 2,
    "text": "Blobworld: A System for Region-Based Image Indexing and Retrieval . Blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects. Each image is automatically segmented into regions (\"blobs\") with associated color and texture descriptors. Querying is based on the attributes of one or two regions of interest, rather than a description of the entire image. In order to make large-scale retrieval feasible, we index the blob descriptions using a tree. Because indexing in the high-dimensional feature space is computationally prohibitive, we use a lower-rank approximation to the high-dimensional distance. Experiments show encouraging results for both querying and indexing. 1 Introduction  From a user's point of view, the performance of an information retrieval system can be measured by the quality and speed with which it answers the user's information need. Several factors contribute to overall performance:  -- the time required to run each individual query,  -- the quality (precision/recall) of each i...",
    "neighbors": [
      214
    ],
    "mask": "Train"
  },
  {
    "node_id": 1156,
    "label": 0,
    "text": "Agent Virtual Organizations within the Framework of Network Computing: a case study We study the concept of agent virtual organization and show how it relates to the paradigm of Network Based Computing [28]. We also discuss the paradigm of BDI-agent trying to show that sophisticated architecture of BDI-agent can not be efficiently applied for large worlds. As the working example of virtual organization we consider a model of virtual enterprise.  Key words: agent virtual organization, agent-based manufacturing, virtual enterprise formation. 1 Introduction  Autonomous, adaptive and cooperative software mobile agents are well suited for domains that require constant adaptation to changing distributed environment or changing demands. Actually cyberspace and manufacturing enterprise are such domains so that there is increasing interest in applying agent technologies there. Cyberspace, in the shape of the Internet, intranets, and the World Wide Web, has grown phenomenally in recent years. Cyberspace now contains enormous amounts of information and is also being increasingly...",
    "neighbors": [
      250,
      839
    ],
    "mask": "Train"
  },
  {
    "node_id": 1157,
    "label": 4,
    "text": "Reality Browsing: Using Information Interaction and Robotic Autonomy for Planetary Exploration . Reality browsing is a framework that enables distributed control of a team of planetary robots. In it, prioritized  user queries are serviced in a hierarchical data structure consisting of an Internet-accessible world model, data archives  on the remote robots and finally a multiple-robot planner that coordinates query-directed searches. This paper introduces  the reality browser concept and outlines important research issues required for implementation.",
    "neighbors": [
      55
    ],
    "mask": "Train"
  },
  {
    "node_id": 1158,
    "label": 3,
    "text": "Efficiently Querying Moving Objects with Pre-defined Paths in a Distributed Environment Due to the recent growth of the World Wide Web, numerous spatio-temporal applications can obtain their required information from publicly available web sources. We consider those sources maintaining moving objects with predefined paths and schedules, and investigate different plans to perform queries on the integration of these data sources efficiently. Examples of such data sources are networks of railroad paths and schedules for trains running between cities connected through these networks. A typical query on such data sources is to find all trains that pass through a given point on the network within a given time interval. We show that traditional filter+semi-join plans would not result in efficient query response times on distributed spatio-temporal sources. Hence, we propose a novel spatio-temporal filter, called deviation filter, that exploits both the spatial and temporal characteristics of the sources in order to improve the selectivity. We also report on our experiments in comparing the performances of the alternative query plans and conclude that the plan with spatio-temporal filter is the most viable and superior plan.",
    "neighbors": [
      147,
      881
    ],
    "mask": "Train"
  },
  {
    "node_id": 1159,
    "label": 5,
    "text": "Random Probability Functions Random probability functions are required in Monte Carlo simulations for expert systems testing. Here I give an empirical method for generating such functions. While this is a practical problem, its solution raises interesting philosophical questions. The design of expert systems is an important problem in the field of Artificial Intelligence. One task, for instance, is to give a general methodology for diagnosis which can be used to construct computer experts in a whole range of areas such as the diagnosis of hepatitis and fault-finding in circuit boards. Clearly before these artificial experts can be used to make critical decisions they must be thoroughly tested and their reliability ascertained. Two approaches to testing stand out: empirical experiments and Monte Carlo simulations. Empirical testing is certainly desirable, but there may not be enough empirical data readily available. To ascertain the reliability of a particular diagnostic expert system one often requires a considerab...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1160,
    "label": 4,
    "text": "A Context for Assisted Cognition Assisted Cognition is introduced as an idea which leverages four technologies, ubiquitous computing, state reduction, plan recognition and decision theory, to develop solutions to problems faced by Alzheimer patients and their caregivers. A patient population with General Dementia Scale rating of less than 4 is targeted, and user interface methods from clinical studies are identified. A review of the literature on the component technologies is surveyed and a general architecture for Assisted Cognition is described.",
    "neighbors": [
      189,
      319
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1161,
    "label": 2,
    "text": "Document Expansion for Speech Retrieval Advances in automatic speech recognition allow us to search large speech collections using traditional information retrieval  methods. The problem of \"aboutness\" for documents --- is a document about a certain concept --- has been at the core of document indexing for the entire history of IR. This problem is more difficult for speech indexing since automatic speech transcriptions often contain mistakes. In this study we show that document expansion can be successfully used to alleviate the effect of transcription mistakes on speech retrieval. The loss of retrieval effectiveness due to automatic transcription errors can be reduced by document expansion from 15--27% relative to retrieval from human transcriptions to only about 7--13%, even for automatic transcriptions with word error rates as high as 65%. For good automatic transcriptions (25% word error rate), retrieval effectiveness with document expansion is indistinguishable from retrieval from human transcriptions. This makes speech...",
    "neighbors": [
      11,
      1211
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1162,
    "label": 3,
    "text": "Quilt: An XML Query Language for Heterogeneous Data Sources The World Wide Web promises to transform human society by making virtually all types of information instantly available everywhere. Two prerequisites for this promise to be realized are a universal markup language and a universal query language. The power and flexibility of XML make it the leading candidate for a universal markup language. XML provides a way to label information from diverse data sources including structured and semi-structured documents, relational databases, and object repositories. Several XML-based query languages have been proposed, each oriented toward a specific category of information. Quilt is a new proposal that attempts to unify concepts from several of these query languages, resulting in a new language that exploits the full versatility of XML. The name Quilt suggests both the way in which features from several languages were assembled to make a new query language, and the way in which Quilt queries can combine information from diverse data sources into a query result with a new structure of its own.",
    "neighbors": [
      17,
      488,
      585,
      634,
      651,
      767
    ],
    "mask": "Train"
  },
  {
    "node_id": 1163,
    "label": 5,
    "text": "Matching in Description Logics: Preliminary Results Matching of concepts with variables (concept patterns) is a relatively new operation that has been introduced in the context of concept description languages (description logics), originally to help discard unimportant aspects of large concepts appearing in industrial-strength knowledge bases. This paper proposes a new approach to performing matching, based on a \"concept-centered\" normal form, rather than the more standard \"structural subsumption\" normal form for concepts. As a result, matching can be performed (in polynomial time) using  arbitrary concept patterns of the description language FL: , thus removing restrictions from previous work. The paper also addresses the question of matching problems with additional \"side conditions\", which were motivated by practical experience.  1 Introduction  The traditional inference problems for Description Logic (DL) systems (like subsumption) are now wellinvestigated. This means that algorithms are available for solving the subsumption proble...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1164,
    "label": 2,
    "text": "The Role of Information Extraction for Textual CBR Abstract. The benefits of CBR methods in domains where cases are text depend on the underlying text representation. Today, most TCBR approaches are limited to the degree that they are based on efficient, but weak IR methods. These do not allow for reasoning about the similarities between cases, which is mandatory for many CBR tasks beyond text retrieval, including adaptation or argumentation. In order to carry out more advanced CBR that compares complex cases in terms of abstract indexes, NLP methods are required to derive a better case representation. This paper discusses how state-of-the-art NLP/IE methods might be used for automatically extracting relevant factual information, preserving information captured in text structure and ascertaining negation. It also presents our ongoing research on automatically deriving abstract indexing concepts from legal case texts. We report progress toward integrating IE techniques and ML for generalizing from case texts to our CBR case representation. 1",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1165,
    "label": 2,
    "text": "Efficient and Effective Metasearch for Text Databases Incorporating Linkages among Documents Linkages among documents have a significant impact on the importance of documents, as it can be argued that important documents are pointed to by many documents or by other important documents. Metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources (text databases). In a large-scale metasearch engine, the contents of each local database is represented by a representative. Each user query is evaluated against the set of representatives of all databases in order to determine the appropriate databases to search. In previous works, the linkage information between documents has not been utilized in determining the appropriate databases to search. In this paper, such information is employed to determine the degree of relevance of a document with respect to a given query. This information is then stored in each database representative to facilitate the selection of databases for each given query. We establish a necessary an...",
    "neighbors": [
      216,
      271,
      433,
      477,
      696,
      792,
      897,
      1124
    ],
    "mask": "Train"
  },
  {
    "node_id": 1166,
    "label": 0,
    "text": "Social Mental Shaping: Modelling the Impact of Sociality on the Mental States of Autonomous Agents This paper presents a framework that captures how the social nature of agents that are situated in a multi-agent environment impacts upon their individual mental states. Roles and social relationships provide an abstraction upon which we develop the notion of social mental shaping. This allows us to extend the standard Belief-DesireIntention model to account for how common social phenomena (e.g. cooperation, collaborative problem-solving and negotiation) can be integrated into a unified theoretical perspective that reflects a fully explicated model of the autonomous agent's mental state.  Keywords: Multi-agent systems, agent interactions, BDI models, social influence.  3  1.",
    "neighbors": [
      557,
      724
    ],
    "mask": "Train"
  },
  {
    "node_id": 1167,
    "label": 4,
    "text": "Creating Creativity for Everyone: User Interfaces for Supporting Innovation :  A challenge for human-computer interaction researchers and user interface designers is to construct information technologies that support creativity. This ambitious goal can be attained by building on an adequate understanding of creative processes. This paper offers the four-phase genex framework for generating excellence: - Collect: learn from previous works stored in digital libraries - Relate: consult with peers and mentors at early, middle and late stages - Create: explore, compose, and evaluate possible solutions - Donate: disseminate the results and contribute to the digital libraries Within this integrated framework, this paper proposes eight activities that require humancomputer interaction research and advanced user interface design. A scenario about an architect illustrates the process of creative work within a genex environment.  1.",
    "neighbors": [
      429
    ],
    "mask": "Train"
  },
  {
    "node_id": 1168,
    "label": 1,
    "text": "Machine Learning for Modeling Dutch Pronunciation Variation This paper describes the use of rule induction techniques for the automatic extraction of phonemic knowledge and rules from pairs of pronunciation lexica. This extracted knowledge allows the adaptation of speech processing systems to regional variants of a language. As a case study, we apply the approach to Northern Dutch and Flemish (the variant of Dutch spoken in Flanders, a part of Belgium) , based on Celex and Fonilex, pronunciation lexica for Northern Dutch and Flemish, respectively. In our study, we compare two rule induction techniques, TransformationBased Error-Driven Learning (TBEDL) (Brill, 1995) and C5.0 (Quinlan, 1993), and evaluate the extracted knowledge quantitatively (accuracy) and qualitatively (linguistic relevance of the rules). We conclude that, whereas classication-based rule induction with C5.0 is more accurate, the transformation rules learned with TBEDL can be more easily interpreted.  1 Introduction  A central component of speech processing systems is a pronun...",
    "neighbors": [
      83
    ],
    "mask": "Train"
  },
  {
    "node_id": 1169,
    "label": 3,
    "text": "Design of RapidBase - an Active Measurement Database System In data-intensive industrial on-line applications utilizing live process data, one faces an unusual set of database requirements. The process measurement data need to be acquired at great speed, organized in time series and made available for time-based retrieval. Active capabilities and functional extensibility are needed to implement a flexible data-driven processing paradigm. An efficient transaction logging and recovery mechanism is needed in order not to impede the data acquisition flow. RapidBase is a system that meets these requirements. It utilizes a main-memory database, a unique temporal-relational data model for handling time series, and an elaborate trigger subsystem. It is implemented as a server program equipped with interfaces of high power of expression.  1 Introduction  Although the requirement for data management is omnipresent in various advanced applications, the main-stream notion of a database may be not a best choice in all cases. Certain application classes requ...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1170,
    "label": 5,
    "text": "First-Order Representation of Stable Models Turi (1991) introduced the important notion of a constrained atom: an atom with associated equality and disequality constraints on its arguments. A set of constrained atoms is a constrained interpretation. We investigate how non-ground representations of both the stable model semantics and the well-founded semantics may be obtained through Turi\u2019s approach. The practical implication of this is that the wellfounded model (or the set of stable models) may be partially pre-computed at compile-time, resulting in the association of each predicate symbol in the program to a constrained atom. Algorithms to create such models are presented, both for the well founded case, and the case of stable models. Query processing reduces to checking whether each atom in the query is true in a stable model (resp. well-founded model). This amounts to showing the atom is an instance of one of some constrained atom whose associated constraint is solvable. Various related complexity results are explored, and the impacts of these results are discussed from the point of view of implementing systems that incorporate the stable and well-founded semantics.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1171,
    "label": 3,
    "text": "A Generalized Modeling Framework for Schema Versioning Support Advanced object-oriented applications require the management of schema versions, in order to cope with changes in the structure of the stored data. Two types of versioning have been separately considered so far: branching and temporal. The former arose in application domains like CAD/CAM and software engineering, where different solutions have been proposed to support design schema versions (consolidated versions). The latter concerns temporal databases, where some works considered temporal schema versioning to fulfil advanced needs of other typical objectoriented applications like GIS and the multimedia ones.  In this work, we propose a general model which integrates the two approaches by supporting both design and temporal schema versions. The model is provided with a complete set of schema change primitives for full-fledged version manipulation whose semantics is described in the paper.  Keywords: Schema versioning, Schema evolution, OODBMS, Temporal databases  1 Introduction  In th...",
    "neighbors": [
      1234
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1172,
    "label": 5,
    "text": "Task Oriented Software Understanding The main factors that affect software understanding are the complexity of the problem solved by the program, the program text, the user's mental ability and experience and the task being performed. This paper describes a planning approach solution to the software understanding problem that focuses on the user's task and expertise. First, user questions about software artifacts have been studied and the most commonly asked questions are identified. These questions are organized into a question model and procedures for answering them are developed. Then, the patterns in user questions while performing certain tasks have been studied and these patterns are used to build generic task models. The explanation system uses these task models in several ways. The task model, along with a user model, is used to generate explanations tailored to the user's task and expertise. In addition, the task model allows the system to provide explicit task support in its interface.  Keywords  software explan...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1173,
    "label": 4,
    "text": "Finding Text Regions Using Localised Measures We present a method based on statistical properties of local image neighbourhoods for the location of text in real-scene images. This has applications in robot vision, and desktop and wearable computing. The statistical measures we describe extract properties of the image which characterise text, invariant to a large degree to the orientation, scale or colour of the text in the scene. The measures are employed by a neural network to classify regions of an image as text or non-text. We thus avoid the use of different thresholds for the various situations we expect, including when text is too small to read, or when the text plane is not fronto-parallel to the camera. We briefly discuss applications and the possibility of recovery of the text for optical character recognition. 1 Introduction Automatic location and digitisation of text in arbitrary scenes, where the text may or may not be fronto-parallel to the viewing plane, is an area of computer vision which has not yet been ...",
    "neighbors": [
      550,
      605
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1174,
    "label": 4,
    "text": "Experiences Developing a Thin-Client, Multi-Device Travel Planning Application Many applications now require access from diverse humancomputer interaction devices, such as desktop computers, web browsers, PDAs, mobile phones, pagers and so on. We describe our experiences developing a multi-device travel planning application built from reusable components, many of these developed from several different previous projects. We focus on key user interface design and component adaptation and integration issues as encountered in this problem domain. We report on the results of a useability evaluation of our prototype and our current research directions addressing HCI and interface development problems we encountered.",
    "neighbors": [
      1210
    ],
    "mask": "Test"
  },
  {
    "node_id": 1175,
    "label": 4,
    "text": "Event-Driven FRP Abstract. Functional Reactive Programming (FRP) is a high-level declarative language for programming reactive systems. Previous work on FRP has demonstrated its utility in a wide range of application domains, including animation, graphical user interfaces, and robotics. FRP has an elegant continuous-time denotational semantics. However, it guarantees no bounds on execution time or space, thus making it unsuitable for many embedded real-time applications. To alleviate this problem, we recently developed Real-Time FRP (RT-FRP), whose operational semantics permits us to formally guarantee bounds on both execution time and space. In this paper we present a formally verifiable compilation strategy from a new language based on RT-FRP into imperative code. The new language, called Event-Driven FRP (E-FRP), is more tuned to the paradigm of having multiple external events. While it is smaller than RT-FRP, it features a key construct that allows us to compile the language into efficient code. We have used this language and its compiler to generate code for a small robot controller that runs on a PIC16C66 micro-controller. Because the formal specification of compilation was crafted more for clarity and for technical convenience, we describe an implementation that produces more efficient code. 1",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1176,
    "label": 5,
    "text": "A Probabilistic Approach to Planning Biped Locomotion with Prescribed Motions Typical high-level directives for locomotion of human-like characters are encountered  frequently in animation scripts or interactive systems. In this paper, we  present a new scheme for planning natural-looking locomotion of a biped figure to  facilitate rapid motion prototyping and task-level motion generation. Given start  and goal positions in a virtual environment, our scheme gives a sequence of motions  to move from the start to the goal using a set of live-captured motion clips.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1177,
    "label": 4,
    "text": "How to Build Smart Appliances In this article smart appliances are characterized as devices that are attentive to their environment. We introduce a terminology for situation, sensor data, context, and context-aware applications because it is important to gain a thorough understanding of these concepts to successfully build such artifacts. In the article the relation between a real-world situation and the data read by sensors is discussed; furthermore, an analysis of available sensing technology is given. Then we introduce an architecture that supports the transformation from sensor data to cues then to contexts as a foundation to make context-aware applications. The article suggests a method to build context-aware devices; the method starts from situation analysis, offers a structured way for selection of sensors, and finally suggests steps to determine recognition and abstraction methods. In the final part of the article the question of how this influences the applications is raised and the areas of user int...",
    "neighbors": [
      728,
      1006,
      1226
    ],
    "mask": "Test"
  },
  {
    "node_id": 1178,
    "label": 1,
    "text": "Multistrategy Learning for Information Extraction Information extraction (IE) is the problem of filling out pre-defined structured summaries from text documents. We are interested in performing IE in non-traditional domains, where much of the text is often ungrammatical, such as electronic bulletin board posts and Web pages. We suggest that the best approach is one that takes into account many different kinds of information, and argue for the suitability of a multistrategy approach. We describe learners for IE drawn from three separate machine learning paradigms: rote memorization, term-space text classification, and relational rule induction. By building regression models mapping from learner confidence to probability of correctness and combining probabilities appropriately, it is possible to improve extraction accuracy over that achieved by any individual learner. We describe three different multistrategy approaches. Experiments on two IE domains, a collection of electronic seminar announcements from a university computer science de...",
    "neighbors": [
      133,
      142,
      379,
      855,
      1090,
      1233
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1179,
    "label": 5,
    "text": "Positioning a coarse-calibrated camera with respect to an unknown object by 2D 1/2 visual servoing In this paper we propose a new vision-based robot control approach halfway between the classical positionbased and image-based visual servoings. It allows to avoid their respective disadvantages. The homography between some planar feature points extracted from two images (corresponding to the current and desired camera poses) is computed at each iteration. Then, an approximate partial-pose, where the translational term is known only up to a scale factor, is deduced, from which can be designed a closed-loop control law controlling the six camera d.o.f.. Contrarily to the position-based visual servoing, our scheme does not need any geometric 3D model of the object. Furthermore and contrarily to the image-based visual servoing, our approach ensures the convergence of the control law in all the task space.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1180,
    "label": 1,
    "text": "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density This paper presents a method by which a reinforcement  learning agent can automatically discover  certain types of subgoals online. By creating  useful new subgoals while learning, the agent  is able to accelerate learning on the current task  and to transfer its expertise to other, related tasks  through the reuse of its ability to attain subgoals.  The agent discovers subgoals based on commonalities  across multiple paths to a solution. We  cast the task of finding these commonalities as  a multiple-instance learning problem and use the  concept of diverse density to find solutions. We  illustrate this approach using several gridworld  tasks.  1.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1181,
    "label": 2,
    "text": "Capturing Knowledge of User Preferences: Ontologies in Recommender Systems Tools for filtering the World Wide Web exist, but they are hampered by the difficulty of capturing user preferences in such a dynamic environment. We explore the acquisition of user profiles by unobtrusive monitoring of browsing behaviour and application of supervised machine-learning techniques coupled with an ontological representation to extract user preferences. A multi-class approach to paper classification is used, allowing the paper topic taxonomy to be utilised during profile construction. The Quickstep recommender system is presented and two empirical studies evaluate it in a real work setting, measuring the effectiveness of using a hierarchical topic ontology compared with an extendable flat list.",
    "neighbors": [
      453
    ],
    "mask": "Train"
  },
  {
    "node_id": 1182,
    "label": 0,
    "text": "Secret Computation with Secrets for Mobile Agent using One-time Proxy Signature As an application for electronic commerce, a mobile agent is now used to search for special products or services and is executed for a specific job designated by a customer in the server's environment on behalf of a customer. On the way of performing its role, a mobile agent can be vulnerable to several cryptographic attacks. These attacks can be more serious when done by malicious servers. Among schemes to resolve this problem, the concept of encrypted function for secret computation was proposed in [ST97, KBC00]. However, schemes that employ such encrypted functions enforce the server(host) to execute the functions of customer before verifying the mobile codes even in the case that the codes are maliciously modified. In this paper, we apply proxy signature scheme to the mobile agent system to enhance security and efficiency. Also, we suggest one-time proxy signature scheme to limit the signing power of the server.",
    "neighbors": [
      925
    ],
    "mask": "Test"
  },
  {
    "node_id": 1183,
    "label": 2,
    "text": "Information Retrieval on the Web: Selected Topics In this paper we review studies on the growth of the Internet and technologies which are useful for information search and retrieval on the Web. In the rst section, we present data on the Internet from several dierent sources, e.g., current as well as projected number of users, hosts and Web sites. Although the numerical gures vary, the overall trends cited by the sources are consistent and point to exponential growth during the coming decade. And Internet users are increasingly using search engines and search services to nd speci c information of interest. However, users are not satis ed with the performance of the current generation of search engines; the slow speed of retrieval, communication delays, and poor quality of retrieved results (e.g., noise and broken links) are commonly cited problems. The main body of our paper focuses on linear algebraic models and techniques for solving these problems. keywords: clustering, indexing, information retrieval, Internet, late...",
    "neighbors": [
      128,
      224,
      382,
      1000,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 1184,
    "label": 3,
    "text": "A Novel Server Selection Technique for Improving the Response Time of a Replicated Service Server replication is an approach often used to improve the ability of a service to handle a large number of clients. One of the important factors in the efficient utilization of replicated servers is the ability to direct client requests to the best server, according to some optimality criteria. In this paper we target an environment in which servers are distributed across the Internet, and clients identify servers using our application-layer anycasting service. Our goal is to allocate servers to clients in a way that minimizes a client's response time. To that end, we develop an approach for estimating the performance that a client would experience when accessing particular servers. Such information is maintained in a resolver that clients can query to obtain the identity of the server with the best response time. Our performance collection technique combines server push with client probes to estimate the expected response time. A set of experiments is used to demonstrate the propert...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1185,
    "label": 1,
    "text": "Design of a Classification System for Rectangular Shapes Using a Co-Design Environment Pattern localization and classification are CPU time intensive, being normally implemented in software. Custom implementations in hardware allow real-time processing. In practice, in ASIC or FPGA implementations, the digitization process introduces errors that should be taken into account. This paper presents initially the state-of-the-art in this field, analyzing the performance and implementation of each work. After we propose a system for rectangular shapes localization and classification using reconfigurable devices (FPGA) and a signal processor (DSP) available in a flexible codesign platform. The system will be described using C and VHDL languages, for the software and hardware parts respectively. Finally, it is described the classification block of the system, implemented as an Artificial Neural Network (ANN) in a rapid prototyping platform.  1.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1186,
    "label": 4,
    "text": "Exception Handling in the Spreadsheet Paradigm Exception handling is widely regarded as a necessity in programming languages today, and almost every programming language currently used for professional software development supports some form of it. However, spreadsheet systems, which may be the most widely used type of \"programming language\" today in terms of number of users using it to create \"programs\" (spreadsheets), have traditionally had only extremely limited support for exception handling. Spreadsheet system users range from end users to professional programmers, and this wide range suggests that an approach to exception handling for spreadsheet systems needs to be compatible with the equational reasoning model of spreadsheet formulas, yet feature expressive power comparable to that found in other programming languages.",
    "neighbors": [
      1088
    ],
    "mask": "Train"
  },
  {
    "node_id": 1187,
    "label": 3,
    "text": "Query Rewriting using Semistructured Views We address the problem of query rewriting for MSL, a semistructured language developed at Stanford in the TSIMMIS project for information integration. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting  queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing containment mappings, the chase, and unification -- techniques which were developed for structured, relational data. At the same time we develop an algorithm for equivalence checking of MSL queries. We show that the rewriting algorithm is sound and complete, i.e., it always finds every conjunctive MSL rewriting query of q, and we discuss its complexity. We currently incorporate the algorithm in the TSIMMIS system. 1 Introduction  Recently, many semistructured data models, query and view definition languages have been proposed [GM  +  97, MAG  +  97, BDHS96, AV97a, MM97, KS95, PGMU96,...",
    "neighbors": [
      708,
      1055
    ],
    "mask": "Test"
  },
  {
    "node_id": 1188,
    "label": 2,
    "text": "Internet Agents for Telemedicine Services Telemedicine can be viewed as the telematic support for collaboration among distant medical professionals, which cooperate on shared resources of various kind. In light of this, attention to telematics and informatics concepts particularly oriented towards collaboration should be paid: in particular, the recently appeared agent paradigm seems suitable for the analysis, design and development of telemedicine services because of its committment to intercommunication and sharing of resources. The present paper is aimed at introducing the agent paradigm, from the theoretical basis up to the technological issues, and at describing an agent-based approach to telemedicine, specifically applied to telepathology applications. The described system is based on an agent-based tool (JAMES), namely an agent model and template implemented using Java, which has been used to implement a prototype multipurpose telepathology application based on a federated agency architecture.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1189,
    "label": 2,
    "text": "The PageRank Citation Ranking: Bringing Order to the Web The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.",
    "neighbors": [
      149,
      219,
      247,
      482,
      819,
      867,
      1017,
      1247
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1190,
    "label": 2,
    "text": "Ontology-Based Integration of Information - A Survey of Existing Approaches We review the use on ontologies for the integration  of heterogeneous information sources. Based  on an in-depth evaluation of existing approaches to  this problem we discuss how ontologies are used to  support the integration task. We evaluate and compare  the languages used to represent the ontologies  and the use of mappings between ontologies as well  as to connect ontologies with information sources.  We also ask for ontology engineering methods and  tools used to develop ontologies for information integration.  Based on the results of our analysis we  summarize the state of the art in ontology-based information  integration and name areas of further research  activities.  1 Motivation  The so-called information society demands for complete access to available information, which is often heterogeneous and distributed. In order to establish efficient information sharing, many technical problems have to be solved. First, a suitable information source must be located that might conta...",
    "neighbors": [
      161,
      296
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1191,
    "label": 0,
    "text": "Animated Agents for Procedural Training in Virtual Reality: Perception, Cognition, and Motor Control This paper describes Steve, an animated agent that helps students learn to perform physical, procedural tasks. The student and Steve cohabit a three-dimensional, simulated mock-up of the student's work environment. Steve can demonstrate how to perform tasks and can also monitor students while they practice tasks, providing assistance when needed. This paper describes Steve's architecture in detail, including perception, cognition, and motor control. The perception module monitors the state of the virtual world, maintains a coherent representation of it, and provides this information to the cognition and motor control modules. The cognition module interprets its perceptual input, chooses appropriate goals, constructs and executes plans to achieve those goals, and sends out motor commands. The motor control module implements these motor commands, controlling Steve's voice, locomotion, gaze, and gestures, and allowing Steve to manipulate objects in the virtual world. 1 Introduction  To ma...",
    "neighbors": [
      49,
      265,
      723,
      1033,
      1267
    ],
    "mask": "Train"
  },
  {
    "node_id": 1192,
    "label": 4,
    "text": "The Shopping Jacket: Wearable Computing for the Consumer . As part of the Bristol Wearable Computing Initiative we are  exploring location sensing systems suitable for use with Wearable Computing.  In this paper we present our findings, and in particular a wearable  application - the 'Shopping Jacket' - which relies on a minimal infrastructure  to be effective. We use two positioning devices, 'Pingers' and  GPS. The Pinger is used to signal the presence of a shop, and to indicate  the type of shop and it's website. The GPS is used to disambiguate  which branch of a high street chain we are passing. The wearable uses  this information to determine whether the wearer needs to be alerted  that they are passing an interesting shop, or to direct the wearer around  a shopping mall.  The shopping jacket integrates a wearable CardPC; GPS and Pinger  receivers; a near-field radio link; hand-held display; GSM data telephone;  and a speech interface into a conventional sports blazer.  Keywords - wearable computer, location sensing, GPS, pinger, shoppin...",
    "neighbors": [
      12,
      32,
      75,
      215
    ],
    "mask": "Test"
  },
  {
    "node_id": 1193,
    "label": 0,
    "text": "Cyber-Atvs: Dynamic And Distributed Reconnaissance And Surveilllance Using All Terrain Ugvs This paper describes our current effort to develop robotic vehicles for tactical distributed surveillance. Our research is focused on multi-agent collaboration, reconfigurable systems, efficient perception and sensor fusion, distributed command and control, and task decomposition. In particular, this paper describes the main features and capabilities of our All Terrain Vehicles (ATVs), concentrating on their autonomous navigation capabilities.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1194,
    "label": 5,
    "text": "Map Learning and High-Speed Navigation in RHINO This chapter surveys basic methods for learning maps and high speed autonomous navigation for indoor mobile robots. The methods have been developed in our lab over the past few years, and most of them have been tested thoroughly in various indoor environments. The chapter is targeted towards researchers and engineers who attempt to build reliable mobile robot navigation software.",
    "neighbors": [
      4,
      369,
      789,
      1035
    ],
    "mask": "Train"
  },
  {
    "node_id": 1195,
    "label": 3,
    "text": "Program Comprehension in Multi-Language Systems This paper presents an approach to program comprehension in multi-language systems. Such systems are characterized by a high amount of source codes in various languages for programming, database definition and job control. Coping with those systems requires the references crossing the language boundaries to be analysed.  Using the EER/GRAL approach to graph-based conceptual modeling, models representing relevant aspects of single language are built and integrated into a common conceptual model. Since conceptual modeling focusses on specific problems, the integrated model presented here is especially tailored to multi-language aspects. Software systems are parsed and represented according to this conceptual model and queried by using a powerful graph query mechanism. This allows multi-language cross references to be easily retrieved.  The multi-language conceptual model and the query facilities have been developed in cooperation with the maintenance programmers at an insurance company w...",
    "neighbors": [
      1116
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1196,
    "label": 0,
    "text": "Agents for Process Coherence in Virtual Enterprises SoCom  #  1  \"on time\"  Abstract SoCom  #  2  \"cheap\"  Buyer Seller  Abstract SoCom  #  3  Buyer Seller  SoCom Manager  Hoosier Inc.  Register me as  buyer and  seller  Register me as  buyer and  seller  Play Seller in  AbstractSoCom  #1?  Yes  Valvano & Co. Hot Air Bros.  8 9  Concrete  SoCom  created  4  6  7  \"high quality\"  = Roles  = Agents  Directory  Agent_id Role derived  1  Figure 2. Instantiation of a concrete SoCom  68 March 1999/Vol. 42, No. 3 COMMUNICATIONS OF THE ACM  Adopt role  Need to  initiate  Ask SoCom  manager  Participate  No  No  No  No  No  No  No  Ye s  Yes  Ye s  Yes  Yes  Yes  Ye s  Register  SoCom  manager suggests  a socom  Request  to create  SoCom  Process  request  Stop  Stop  (undefined)  Stop  (Failure)  Instantiate  and  announce  Receipt of a  request  Request to  register  Condition  evaluation  OK?  Find  candidates  Ask  candidates  All  say yes?  Agents decision making SoCom manager's decision making  Because our agents are autonomous, we must  e...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1197,
    "label": 1,
    "text": "An Adaptive Penalty Approach for Constrained Genetic-Algorithm Optimization In this paper we describe a new adaptive penalty approach for handling constraints in genetic algorithm optimization problems. The idea is to start with a relatively small penalty coefficient and then increase it or decrease it on demand as the optimization progresses. Empirical results in several engineering design domains demonstrate the merit of the proposed approach.  1 Introduction  Genetic Algorithms (GAs) (Goldberg 1989) are search algorithms that mimic the behavior of natural selection. GAs attempt to find the best solution to some problem (e.g., the maximum of a function) by generating a collection (\"population \") of potential solutions (\"individuals\") to the problem. Through mutation and recombination (crossover) operations, better solutions are hopefully generated out of the current set of potential solutions. This process continues until an acceptably good solution is found. GAs have many advantages over other search techniques, including the ability to deal with qualitativ...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1198,
    "label": 2,
    "text": "The Web Graph: an Overview this paper, a study is made on a 200 millions vertices graph obtained from a crawl of the Web, and it appears that is is composed of four parts of equivalent sizes. See Figure 3. The first part is the largest strongly connected component of the graph (the second largest is much smaller), which composes the core of the well connected pages. The second part, called IN, is composed of those pages from which the core is reachable, but which are not reachable from the core. Conversly, the third part, called OUT, is the set of pages reachable from the core but from which the core is unreachable. Finally, the dendrites are the pages reachable from one of the three first parts, or from which one of the three first parts is reachable, but which belong to none of the previous parts. Only ten percent of the whole graph do not belong to one of these four parts which compose the bow-tie",
    "neighbors": [
      216
    ],
    "mask": "Train"
  },
  {
    "node_id": 1199,
    "label": 5,
    "text": "Spontaneous, Short-term Interaction with Mobile Robots major open research directions in mobile robotics. This paper considers a specific type of interaction: short-term and spontaneous interaction with crowds of people. Such patterns of interactions are found when service robots operate in public places, for example information kiosks, receptionists, tour-guide robots applications. We describe our approach to spontaneous short-term interaction: a robot designed to be a believable social agent. The approach has been implemented using a mobile robot with a motorized face as focal point for interaction, an architecture that suggests the robot has moods, and a method for learning how to interact with people. Our system was recently deployed at a Smithsonian museum in Washington, DC. During a two week period it interacted with thousands of people. The robot's interactive capabilities were essential for its high on-task performance, and thus its practical success.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1200,
    "label": 1,
    "text": "Evolving Detectors of 2D Patterns on a Simulated CAM-Brain Machine, an Evolvable Hardware Tool for Building a 75 Million Neuron Artificial Brain . This paper presents some simulation results of the evolution  of 2D visual pattern recognizers to be implemented very shortly on real  hardware, namely the \"CAM-Brain Machine\" (CBM), an FPGA based  piece of evolvable hardware which implements a genetic algorithm (GA)  to evolve a 3D cellular automata (CA) based neural network circuit module,  of approximately 1,000 neurons, in about a second, i.e. a complete  run of a GA, with 10,000s of circuit growths and performance evaluations.  Up to 65,000 of these modules, each of which is evolved with a  humanly specified function, can be downloaded into a large RAM space,  and interconnected according to humanly specified artificial brain architectures.  This RAM, containing an artificial brain with up to 75 million  neurons, is then updated by the CBM at a rate of 130 billion CA cells  per second. Such speeds will enable real time control of robots and hopefully  the birth of a new research field that we call \"brain building\". The  first su...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1201,
    "label": 3,
    "text": "Querying Network Directories Hierarchically structured directories have recently proliferated with the growth of the Internet, and are being used to store not only address books and contact information for people, but also personal pro les, network resource information, and network and service policies. These systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way far superior to what conventional relational or object-oriented databases o er. Yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated \\queries &quot; involve navigational access. In this paper, we develop the core of a formal data model for network directories, and propose a sequence of e ciently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the e ciency of each algorithm in terms of its I/O complexity. Our data model and query languages share the exibility and utility of the recent proposals for semi-structured data models, while at the same time e ectively addressing the speci c needs of network directory applications, which we demonstrate by means of a representative real-life example. This work was done when the authors were at AT&T Labs{",
    "neighbors": [
      304,
      711,
      1254
    ],
    "mask": "Train"
  },
  {
    "node_id": 1202,
    "label": 1,
    "text": "Learning Fuzzy Classification Rules from Data . Automatic design of fuzzy rule-based classification systems based on labeled data is considered. It is recognized that both classification performance and interpretability are of major importance and e#ort is made to keep the resulting rule bases small and comprehensible. An iterative approach for developing fuzzy classifiers is proposed. The initial model is derived from the data and subsequently, feature selection and rule base simplification are applied to reduce the model, and a GA is used for model tuning. An application to the Wine data classification problem is shown.  1 Introduction  Rule-based expert systems are often applied to classification problems in fault detection, biology, medicinem etc. Fuzzy logic improves classification and decision support systems by allowing the use of overlapping class definitions and improves the interpretability of the results by providing more insight into the classifier structure and decision making process [13]. The automatic determination...",
    "neighbors": [
      560,
      828
    ],
    "mask": "Train"
  },
  {
    "node_id": 1203,
    "label": 2,
    "text": "Visual Information Retrieval from Large Distributed On-line Repositories ion ---  VIR systems differ in the level of abstraction in  which content is indexed. For example,  images may be indexed at various levels, such  as at the feature-level (e.g., color, texture, and  shape), object-level (e.g., moving foreground  object), syntax-level (e.g., video shot), and  semantic-level (e.g., image subject), and so  forth. Most automatic VIR systems aim at lowlevel features, while the high-level indexes are  usually generated manually. Interaction  among different levels is an exciting but  unsolved issue.  . Generality ---  VIR systems differ in their specificity of the  domain of visual information. For example,  customized feature sets can be developed to  incorporate specific domain knowledge, such  as those in medical and remote-sensing  applications. Other, more general VIR  systems aim at indexing unconstrained visual  information such as that on the Internet.  . Content Collection ---  VIR systems differ in the methods in which new  visual information is ad...",
    "neighbors": [
      100,
      118,
      523,
      781,
      931
    ],
    "mask": "Train"
  },
  {
    "node_id": 1204,
    "label": 0,
    "text": "The Cooperative Problem-Solving Process We present a model of cooperative problem solving that describes the process from its beginning, with some agent recognizing the potential for cooperation with respect to one of its goals, through to team action. Our approach is to characterize the mental states of the agents that lead them to solicit, and take part in, cooperative action. The model is formalized by expressing it as a theory in a quantified multi-modal logic.  Keywords: Multi-agent systems, cooperation, modal logic, temporal logic.  1 Introduction  Agents --- both human and artificial --- can engage in many and varied types of social interaction, ranging from altruistic cooperation through to open conflict. However, perhaps the paradigm example of social interaction is cooperative problem solving (CPS), in which a group of autonomous agents choose to work together to achieve a common goal. For example, we might find a group of people working together to move a heavy object, play a symphony, build a house, or write a jo...",
    "neighbors": [
      557,
      724
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1205,
    "label": 3,
    "text": "Active Disks Several application and technology trends indicate that it might be both profitable and feasible to move computation closer to the data that it processes. In this paper, we evaluate Active Disk architectures which integrate significant processing power and memory into a disk drive and allow application-specific code to be downloaded and executed on the data that is being read from (written to) disk. The key idea is to offload bulk of the processing to the disk-resident processors and to use the host processor primarily for coordination, scheduling and combination of results from individual disks. To program Active Disks, we propose a stream-based programming model which allows disklets to be executed efficiently and safely. Simulation results for a suite of seven algorithms from three application domains (commercial data warehouses, image processing and satellite data processing) indicate that for these algorithms, Active Disks outperform conventional-disk architectures. 1 Introduction...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1206,
    "label": 2,
    "text": "Intelligent Information Access in the Web: ML based User Modeling for high precision Meta-Search It is a well known fact that high precision search for documents concerning a certain topic in the World Wide Web (Www) is a tough problem. Index based search engines vary in recall (with a coverage of at most 30% of the web) and offer a very bad precision by (simple) keyword search. Meta search engines provide a specialised monolithic architecture for information extraction, and integration of heterogenous information resources which ensures a bigger recall and may yield a better precision. Few search engines (as, e. g., HuskySearch) employ intelligent techniques in order to increase precision. On the other hand, many personalized agents for web browsing are currently developed. It is a straightforward idea to incorporate the idea of user modeling with machine learning (Ml) methods into web search services. We propose an abstract prototype, OySTER, which makes use of machine learning based user modeling and which for reasons of robustness, performance and quality is realised as a mult...",
    "neighbors": [
      347
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1207,
    "label": 2,
    "text": "Web Search -- Your Way We describe a metasearch engine architecture, in use at NEC Research Institute, that allows users to provide preferences in the form of an information need category. This extra information is used to direct the search process, providing more valuable results than by considering only the query. Using our architecture, identical keyword queries may be sent to different search engines, and results may be scored differently for different users.",
    "neighbors": [
      496,
      587,
      897
    ],
    "mask": "Train"
  },
  {
    "node_id": 1208,
    "label": 0,
    "text": "Mental States Recognition from Communication  Effective and useful communication requires the agents' being able to foresee the effects of their utterances over the addressee's mental state. However, referring to the classical Speech Act Theory, it seems to us that the idea of predicting such effects is rather optimistic since they are not really completely \"a priori\" foreseeable by the speaker. Along with some obvious main effects, there are other side effects which might be regarded as the result of some kind of plausible inference, particularly abduction, performed by the hearerhimself79  the received communication and over: - its own actual mental state (which can be differentfromen one expected by the speaker), - its image (may be incorrect and incomplete) ofthe81-24 's mental state. In this paper we explore the idea that if (and asfar17 it is possible: 1. to formalize in a declarative manner the mental state ofan502-22165 Agent [1], 2. to postulate a correlation between a speaker's mental state and his uttering a certainsent...",
    "neighbors": [
      557,
      964,
      1067
    ],
    "mask": "Train"
  },
  {
    "node_id": 1209,
    "label": 3,
    "text": "Scalable Maintenance in Distributed Data Warehousing Environment The maintenance of data warehouses is becoming an increasingly important topic due to the growing  use, derivation and integration of digital information. Most previous work has dealt with one centralized  data warehouse (DW) only. In this paper, we now focus on environments with multiple data warehouses  that are possibly derived from other data warehouses. In such a large-scale environment, data  updates from base sources may arrive in individual data warehouses in different orders, thus resulting  in inconsistent data warehouse extents. We propose a registry-based solution strategy that addresses  this problem by employing a registry agent responsible for establishing one unique order for the propagation  of updates from the base sources to the data warehouses. With this solution, individual data  warehouse managers can still maintain their respective extents autonomously and independently from  each other, thus allowing them to apply any of the existing incremental maintenance algo...",
    "neighbors": [
      455
    ],
    "mask": "Test"
  },
  {
    "node_id": 1210,
    "label": 4,
    "text": "Developing Adaptable User Interfaces for Component-based Systems Developing software components with user interfaces that can be adapted to diverse reuse situations is challenging. Examples of such adaptations include extending, composing and reconfiguring multiple component user interfaces, and adapting component user interfaces to particular user preferences, roles and subtasks. We describe our recent work in facilitating such adaptation via the concept of user interface aspects, which facilitate effective component user interface design and realisation using an extended, component-based software architecture.  1. Introduction  Component-based software applications are composed from diverse software components to form an application [1, 14, 16, 17]. Typically many of these components have been developed separately, with no knowledge of the user interfaces of other components they may be composed with. This can result in component-based applications with inappropriate, inconsistent interfaces.  For example, two components with user interfaces that ...",
    "neighbors": [
      1174
    ],
    "mask": "Train"
  },
  {
    "node_id": 1211,
    "label": 2,
    "text": "Experiments In Information Retrieval From Spoken Documents This paper describes the experiments performed as part of the TREC-97 Spoken Document Retrieval Track. The task was to pick the correct document from 35 hours of recognized speech documents, based on a text query describing exactly one document. Among the experiments we described here are: Vocabulary size experiments to assess the effect of words missing from the speech recognition vocabulary; experiments with speech recognition using a stemmed language model; using confidence annotations that estimate of the correctness of each recognized word; using multiple hypotheses from the recognizer. And finally we also measured the effects of corpus size on the SDR task. Despite fairly high word error rates, information retrieval performance was only slightly degraded for speech recognizer transcribed documents. 1. INTRODUCTION  For the first time, the 1997 Text REtrieval Conference (TREC97) included an evaluation track for information retrieval on spoken documents. In this paper, we describe ...",
    "neighbors": [
      1161
    ],
    "mask": "Train"
  },
  {
    "node_id": 1212,
    "label": 4,
    "text": "On the Expressiveness of Distributed Leasing in Linda-like Coordination Languages S. All local authors can be reached via e-mail at the address last-name@cs.unibo.it. Questions and comments should be addressed to tr-admin@cs.unibo.it.  Recent Titles from the UBLCS Technical Report Series  99-2 A Theory of Efficiency for Markovian Processes, M. Bernardo, W.R. Cleaveland, February 1999 (Revisied March 2000).  99-3 A Reliable Registry for the Jgroup Distributed Object Model, A. Montresor, March 1999.  99-4 Comparing the QoS of Internet Audio Mechanisms via Formal Methods, A. Aldini, M. Bernardo, R. Gorrieri, M. Roccetti, March 1999.  99-5 Group-Enhanced Remote Method Invocations, A. Montresor, R. Davoli, O. Babao glu, April 1999.  99-6 Managing Complex Documents Over the WWW: a Case Study for XML, P. Ciancarini, F. Vitali, C. Mascolo, April 1999.  99-7 Data-Flow Hard Real-Time Programs: Scheduling Processors and Communication Channels in a Distributed Environment, R. Davoli, F. Tamburini, April 1999.  99-8 The MPS Computer System Simulator, M. Morsiani, R. Davoli, Apri...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1213,
    "label": 2,
    "text": "Normalization and matching in the DORO system This paper is concerned with the use of linguistically motivated phrases as indexing terms in Information Retrieval applications. Apart from the conventional noun phrases, we propose to use verb phrases as index terms for text classification. Techniques for phrase matching through syntactic normalization and semantical matching are described. In particular, we show how to perform syntactic normalization of phrases in order to enhance recall. Semantical normalization is based on lexico-semantical relations, taking into account certain properties of the classification algorithms used. The ideas described here are being implemented in the Document Routing system DORO, in which statistical learning algorithms are applied to document profiles consisting of phrases. This paper describes the rationale behind work in progress, rather than presenting final results.",
    "neighbors": [
      770
    ],
    "mask": "Test"
  },
  {
    "node_id": 1214,
    "label": 4,
    "text": "Context-Aware Telephony over WAP . In this paper we introduce a novel approach to share context to enhance the social quality of remote mobile communication. We provide an analysis of how people start a conversation in situations where they meet physically, especially looking on the influence of the situation. Than this is compared to the way remote communication is initiated using mobile phones. The lack of knowledge about the situation on the other end leads to initiation of calls that are not appropriate in the situation. The solution we propose is to exchange context information before initiating the call. We implemented this concept using the Wireless Application Protocol (WAP). The WML-based Application Context-Call offers a phone interface that provides information about the receiver when setting up a call. The caller can than decide based on that information to place the call, to leave a message or to cancel the call. Privacy issues that arise from this technology are discussed, too. Keywords: Contex...",
    "neighbors": [
      80
    ],
    "mask": "Test"
  },
  {
    "node_id": 1215,
    "label": 1,
    "text": "A Hierarchical Approach to Wrapper Induction With the tremendous amount of information that becomes available on the Web on a daily basis, the ability to quickly develop information agents has become a crucial problem. A vital component of any Web-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex document into a series of easier extraction tasks. We introduce an inductive algorithm, stalker, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results show that stalker does significantly better then other approaches; on one hand, stalker requires up to two orders of magnitude fewer examples than other algorithms, while on the other hand...",
    "neighbors": [
      412,
      570,
      1232,
      1233
    ],
    "mask": "Test"
  },
  {
    "node_id": 1216,
    "label": 3,
    "text": "Results on Reasoning about Updates in Transaction Logic . Transaction Logic was designed as a general logic of state change for deductive databases and logic programs. It has a model theory, a proof theory, and its Horn subset can be given a procedural interpretation. Previous work has demonstrated that the combination of declarative semantics and procedural interpretation turns the Horn subset of Transaction Logic into a powerful language for logic programming with updates [BK98,BK94,BK93,BK95]. In this paper, we focus not on the Horn subset, but on the full logic, and we explore its potential as a formalism for reasoning about logic programs with updates. We first develop a methodology for specifying properties of such programs, and then provide a sound inference system for reasoning about them, and conjecture a completeness result. Finally, we illustrate the power of the inference system through a series of examples of increasing difficulty. 1 Introduction  Updates are a crucial component of any database programming language. Even the si...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1217,
    "label": 5,
    "text": "Expert System Technology in Observing Tools Over the past two years, the Scientist's Expert Assistant team from NASA's Goddard Space Flight Center and the Space Telescope Science Institute has been prototyping tools to support General Observer proposal development for the Hubble Space Telescope and the Next Generation Space Telescope. One aspect of this effort has been the exploration of the use of expert systems in guiding the user in preparing their observing program. The initial goal was to provide the user with a question-and-answer style of interaction where the software would \"interview\" the user for their science needs and recommend instrument settings. This design ultimately failed. The reasons for this failure, and the resulting evolution of our approach, are an interesting case study in the use of expert system technology for observing tools. Although the interview approach failed we felt that expert systems can still be used in the tools environment. This paper describes our current approach to the use of expert syste...",
    "neighbors": [
      1131
    ],
    "mask": "Train"
  },
  {
    "node_id": 1218,
    "label": 3,
    "text": "The FERET Evaluation Methodology for Face-Recognition Algorithms Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems. The Face Recognition Technology (FERET) program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests. To date, 14,126 images from 1199 individuals are included in the FERET database, which is divided into development and sequestered portions of the database. In September 1996, the FERET program administered the third in a series of FERET face-recognition tests. The primary objectives of the third test were to (1) assess the state of the art, (2) identify future areas of research, and (3) measure algorithm performance. 1 Introduction  Over the last decade, face recognition has become an active area of research in computer vision, neuroscience, and psychology. Progress has advanced to the point that face-recognition systems are being demonstrated in real-...",
    "neighbors": [
      71,
      393,
      777,
      928,
      1108
    ],
    "mask": "Train"
  },
  {
    "node_id": 1219,
    "label": 3,
    "text": "Joint Optimization of Cost and Coverage of Query Plans in Data Integration Existing approaches for optimizing queries in data integration use decoupled strategies--attempting to optimize coverage and cost in two separate phases. Since sources tend to have a variety of access limitations, such phased optimization of cost and coverage can unfortunately lead to expensive planning as well as highly inefficient plans. In this paper we present techniques for joint optimization of cost and coverage of the query plans. Our algorithms search in the space of parallel query plans that support multiple sources for each subgoal conjunct. The refinement of the partial plans takes into account the potential parallelism between source calls, and the binding compatibilities between the sources included in the plan. We start by introducing and motivating our query plan representation. We then briefly review how to compute the cost and coverage of a parallel plan. Next, we provide both a System-R style query optimization algorithm as well as a greedy local search algorithm for searching in the space of such query plans. Finally we present a simulation study that demonstrates that the plans generated by our approach will be significantly better, both in terms of planning cost, and in terms of plan execution cost, compared to the existing approaches.  1.",
    "neighbors": [
      218,
      876
    ],
    "mask": "Train"
  },
  {
    "node_id": 1220,
    "label": 1,
    "text": "Self Bounding Learning Algorithms Most of the work which attempts to give bounds on the generalization error of the hypothesis generated by a learning algorithm is based on methods from the theory of uniform convergence. These bounds are a-priori bounds that hold for any distribution of examples and are calculated before any data is observed. In this paper we propose a different approach for bounding the generalization error after the data has been observed. A self-bounding learning algorithm is an algorithm which, in addition to the hypothesis that it outputs, outputs a reliable upper bound on the generalization error of this hypothesis. We first explore the idea in the statistical query learning framework of Kearns [10]. After that we give an explicit self bounding algorithm for learning algorithms that are based on local search. 1 INTRODUCTION Most of the work on the sample complexity of learning is based on uniform convergence theory and attempts to give uniform a-priori bounds. A uniform a-priori bound is a guar...",
    "neighbors": [
      890
    ],
    "mask": "Train"
  },
  {
    "node_id": 1221,
    "label": 2,
    "text": "Integrating Community Services - A Common Infrastructure Proposal Computer based community systems can provide powerful support in knowledge transfer, both in the direct exchange of information and in finding people to exchange information with. Several such information exchange and expert finding services have already been implemented. The problem of current approaches is, that they are not able to exchange data with each other. User profile information and contributed information have to be entered separately for every community-based information system. So, what one system has learned is not available for other systems. In this paper we present our ideas for a community services infrastructure as an enabling technology for further integration of community related services.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1222,
    "label": 0,
    "text": "Specification and Simulation of Multi-Agent Systems in CaseLP Nowadays software applications are characterized by a great complexity. It arises from the need of reusing existing components and properly integrating them. The distribution of the involved entities and their heterogeneity makes it very useful the adoption of the agent-oriented technology. The paper presents the state-of-the-art of CaseLP, an experimental logic-based prototyping environment for multi-agent systems. CaseLP provides a prototyping method and a set of tools and languages which support the prototype realization. At the system specification level, an architectural description language can be adopted to describe the prototype in terms of agents classes, instances, their provided and required services and their communication links. At the agent specification level, a rule-based, not executable language can be used to easily define reactive and proactive agents. An executable, linear logic language can define more sophisticated agents and the system in which they operate. At t...",
    "neighbors": [
      106,
      485,
      884,
      1028
    ],
    "mask": "Test"
  },
  {
    "node_id": 1223,
    "label": 1,
    "text": "Analog Neural Nets with Gaussian or other Common Noise Distributions cannot Recognize Arbitrary Regular Languages We consider recurrent analog neural nets where the output of each gate is subject to Gaussian noise, or any other common noise distribution that is nonzero on a large set. We show that many regular languages cannot be recognized by networks of this type, and we give a precise characterization of those languages which can be recognized. This result implies severe constraints on possibilities for constructing recurrent analog neural nets that are robust against realistic types of analog noise. On the other hand we present a method for constructing feedforward analog neural nets that are robust with regard to analog noise of this type. 1 Introduction  A fairly large literature (see [Omlin, Giles, 1996] and the references therein) is devoted to the construction of analog neural nets that recognize regular languages. Any physical realization of the analog computational units of an analog neural net in technological or biological systems is bound to encounter some form of \"imprecision\" or an...",
    "neighbors": [
      91
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1224,
    "label": 3,
    "text": "Sequence Mining in Categorical Domains: Incorporating Constraints We present cSPADE, an efficient algorithm for mining frequent sequences considering a variety of syntactic constraints. These take the form of length or width limitations on the sequences, minimum or maximum gap constraints on consecutive sequence elements, applying a time window on allowable sequences, incorporating item constraints, and finding sequences predictive of one or more classes, even rare ones. Our method is efficient and scalable. Experiments on a number of synthetic and real databases show the utility and performance of considering such constraints on the set of mined sequences. 1.",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1225,
    "label": 4,
    "text": "Embedding Robots into the Internet With the explosive growth of embedded computing hardware it is possible to conceive many new networked robotic applications for diverse domains ranging from urban search and rescue to house cleaning. Designing reliable software for such systems is a challenging problem. However, communication can facilitate robotics by reducing uncertainty, and robotics can facilitate communication by providing physical mobility. In this article we focus on methods for control and coordination of embedded mobile systems (robots) which interact with other computers on a wireless network situated in human environments.  1 Introduction  Ubiquitous embedded computing [12] is here to stay. Information appliances, laptops, palmtops, and wearable computers are examples of the first wave of this new era. Two factors have contributed to the phenomenal increase in the number of computers in our environment: Moore's law and improved network connectivity. It is now increasingly accepted that appliances of the futu...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1226,
    "label": 4,
    "text": "MediaCups: Experience with Design and Use of Computer-Augmented Everyday Artefacts Our view of ubiquitous computing is artefact-centred: in this view, computers are considered as secondary artefacts that enable items of everyday use as networked digital artefacts. This view is expressed in an artefact computing model and investigated in the Mediacup project, an evolving artefact computing environment. The Mediacup project provides insights into the augmentation of artefacts with sensing, processing, and communication capabilities, and into the provision of an open infrastructure for information exchange among artefacts. One of the artefacts studied is the Mediacup itself, an ordinary coffee cup invisibly augmented with computing and context-awareness. The Mediacup and other computeraugmented everyday artefacts are connected through a network infrastructure supporting loosely-coupled spatially-defined communication. Keywords Ubiquitous computing, digital artefacts, context-awareness, networking, embedded systems, Mediacup 1. INTRODUCTION Computers are becoming ubi...",
    "neighbors": [
      520,
      701,
      1177
    ],
    "mask": "Train"
  },
  {
    "node_id": 1227,
    "label": 0,
    "text": "D'Agents: Applications and Performance of a Mobile-Agent System D'Agents is a general-purpose mobile-agent system that has been used in several informationretrieval  applications. In this paper, we  rst examine one such application, operational support  for military  eld personnel, where D'Agents greatly simpli  es the task of providing ecient,  application-speci  c access to remote information resources. After describing the application, we  discuss the key dierences between D'Agents and most other mobile-agent systems, notably its  support for strong mobility and multiple agent languages. Finally, we derive a small, simple  application that is representative of many information-retrieval tasks, including those in the  example application, and use this application to compare the scalability of mobile agents and  traditional client/server approaches. The results con  rm and quantify the usefulness of mobile  code, and perhaps more importantly, con  rm that intuition about when to use mobile code is  usually correct. Although signi  cant additional experiments are needed to fully characterize the  complex mobile-agent performance space, the results here help answer the basic question of when  mobile agents should be considered at all, particularly for information-retrieval applications.",
    "neighbors": [
      333,
      942,
      1038,
      1054,
      1065
    ],
    "mask": "Test"
  },
  {
    "node_id": 1228,
    "label": 2,
    "text": "C4-1: Building a community hierarchy for the Web based on Bipartite Graphs In this paper we propose an approach to extract and relate the communities by considering a community signature as a group of content creators that manifests itself as a set of interlinked pages. We abstract a community signature as a group of pages that form a dense bipartite graph (DBG), and proposed an algorithm to extract the DBGs from the given data set. Also, using the proposed approach, the extracted communities can be grouped to form a high-level communities. We apply the proposed algorithm on 10 GB TREC (Text REtrieval Conference) data set and extract a three-level community hierarchy. The extracted community hierarchy facilitates an easy analysis of low-level communities and provides a way to understand the sociology of the Web.",
    "neighbors": [
      536,
      774,
      1000,
      1017
    ],
    "mask": "Train"
  },
  {
    "node_id": 1229,
    "label": 0,
    "text": "Macroscopic Observation of Multi-Robot Behavior : This paper presents a new approach to the observation and the control of the behavior of multiple autonomous robots. It is the microscopic observation expressed by dynamic equations that has been commonly employed to observe the multi-robot behavior. However, the approach has the difficulties in estimating the behavior and the mutual interactions of robots. Furthermore, it is hard to realize the system by checking up all the factors of the system. It seems that a macroscopic observation defined by state equations is efficient for recognizing the multiple robots behavior. Therefore, we would like to propose a quantitative observation approach. This attempt means the application of the thermodynamic macroscopic state values to the multi-robot systems. The advantage of this approach is that it enables us to observe the behavior of autonomous robots in real world by mapping the characteristic values of them in another conceptual state space. First, we discuss the implication of applying ...",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1230,
    "label": 1,
    "text": "Evolving Neural Networks through Augmenting Topologies An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1231,
    "label": 0,
    "text": "Communication in Reactive Multiagent Robotic Systems Abstract. Multiple cooperating robots are able to complete many tasks more quickly and reliably than one robot alone. Communication between the robots can multiply their capabilities and e ectiveness, but to what extent? In this research, the importance of communication in robotic societies is investigated through experiments on both simulated and real robots. Performance was measured for three di erent types of communication for three di erent tasks. The levels of communication are progressively more complex and potentially more expensive to implement. For some tasks, communication can signi cantly improve performance, but for others inter-agent communication is apparently unnecessary. In cases where communication helps, the lowest level of communication is almost as e ective as the more complex type. The bulk of these results are derived from thousands of simulations run with randomly generated initial conditions. The simulation results help determine appropriate parameters for the reactive control system which was ported for tests on Denning mobile robots.",
    "neighbors": [
      123
    ],
    "mask": "Train"
  },
  {
    "node_id": 1232,
    "label": 2,
    "text": "Adaptive information extraction: Core technologies for information agents Introduction  For the purposes of this chapter, an information agent can be described as a distributed system that receives a goal through its user interface, gathers information relevant to this goal from a variety of sources, processes this content as appropriate, and delivers the results to the users. We focus on the second stage in this generic architecture. We survey a variety of information extraction techniques that enable information agents to automatically gather information from heterogeneous sources.  For example, consider an agent that mediates package-delivery requests. To satisfy such requests, the agent might need to retrieve address information from geographic services, ask an advertising service for freight forwarders that serve the destination, request quotes from the relevant freight forwarders, retrieve duties and legal constraints from government sites, get weather information to estimate transportation delays, etc.  Information extraction (IE) is a form of sh",
    "neighbors": [
      201,
      279,
      412,
      670,
      855,
      1215,
      1233
    ],
    "mask": "Test"
  },
  {
    "node_id": 1233,
    "label": 2,
    "text": "Wrapper Induction: Efficiency and Expressiveness The Internet presents numerous sources of useful information---telephone directories, product catalogs, stock quotes, event listings, etc. Recently, many systems have been built that automatically gather and manipulate such information on a user's behalf. However, these resources are usually formatted for use by people (e.g., the relevant content is embedded in HTML pages), so extracting their content is difficult. Most systems use customized wrapper procedures to perform this extraction task. Unfortunately, writing wrappers is tedious and error-prone. As an alternative, we advocate wrapper induction, a technique for automatically constructing wrappers. In this article, we describe six wrapper classes, and use a combination of empirical and analytical techniques to evaluate the computational tradeoffs among them. We first consider expressiveness: how well the classes can handle actual Internet resources, and the extent to which wrappers in one class can mimic those in another. We then...",
    "neighbors": [
      161,
      169,
      188,
      496,
      570,
      855,
      1144,
      1178,
      1215,
      1232
    ],
    "mask": "Train"
  },
  {
    "node_id": 1234,
    "label": 3,
    "text": "Beyond Schema Versioning: A Flexible Model for Spatio-Temporal Schema Selection Schema versioning provides a mechanism for handling change in the structure of database systems and has been investigated widely, both in the context of static and temporal databases. With the growing interest in spatial and spatio-temporal data as well as the mechanisms for holding such data, the spatial context within which data items are formatted also becomes an issue. This paper presents a generalised model that accommodates temporal, spatial and spatio-temporal schema versioning within databases.",
    "neighbors": [
      766,
      1171
    ],
    "mask": "Train"
  },
  {
    "node_id": 1235,
    "label": 4,
    "text": "Using Semantic Networks for Knowledge Representation in an Intelligent Environment Introduction  For many years now, research in intelligent spaces has grown, exploring different ways that a room can react to one or more users and their actions. As usage of these intelligent environments (IEs) grows, however, they will by necessity collect ever-increasing amounts of data about their users, in order to adapt to the user's desires. Information will be collected on the users' interests, who they communicate with, their location, web pages they visit, and numerous other details that we may not even notice. All this information needs to be collected and organized, so that the IE can make quick, correct assumptions about what the user would like to do next.  At the Intelligent Room project (Hanssens et al., 2002), we are beginning to define one such knowledge representation  (KR), using semantic networks as the basis for the representation. This creates inherent advantages, both in ease of adding and changing information as well as inference generation.  2. Knowledge Repre",
    "neighbors": [
      688
    ],
    "mask": "Test"
  },
  {
    "node_id": 1236,
    "label": 0,
    "text": "Meta-Agent Programs There are numerous applications where an agent a needs to reason about the beliefs of another agent, as well as about the actions that other agents may take. In (21) the concept of an agent program is introduced, and a language within which the operating principles of an agent can be declaratively encoded on top of imperative data structures is dened. In this paper we rst introduce certain belief data structures that an agent needs to maintain. Then we introduce the concept of a Meta Agent Program (map), that extends the framework of (21; 19), so as to allow agents to perform metareasoning. We build a formal semantics for maps, and show how this semantics supports not just beliefs agent a may have about agent b's state, but also beliefs about agents b's beliefs about agent c's actions, beliefs about b's beliefs about agent c's state, and so on. Finally, we provide a translation that takes any map as input and converts it into an agent program such that there is a one-one correspondence between the semantics of the map and the semantics of the resulting agent program. This correspondence allows an implementation of maps to be built on top of an implementation of agent programs.",
    "neighbors": [
      342,
      532,
      663,
      905
    ],
    "mask": "Train"
  },
  {
    "node_id": 1237,
    "label": 3,
    "text": "Temporal Dependencies Generalized for Spatial and Other Dimensions . Recently, there has been a lot of interest in temporal granularity  , and its applications in temporal dependency theory and data mining. Generalization hierarchies used in multi-dimensional databases and OLAP serve a role similar to that of time granularity in temporal databases, but they also apply to non-temporal dimensions, like space. In this paper, we first generalize temporal functional dependencies for non-temporal dimensions, which leads to the notion of roll-up dependency (RUD). We show the applicability of RUDs in conceptual modeling and data mining. We then indicate that the notion of time granularity used in temporal databases is generally more expressive than the generalization hierarchies in multi-dimensional databases, and show how this surplus expressiveness can be introduced in non-temporal dimensions, which leads to the formalism of RUD with negation (RUD  :  ). A complete axiomatization for reasoning about RUD  :  is given. 1 Introduction  Generalization hierarchi...",
    "neighbors": [
      484,
      1011
    ],
    "mask": "Train"
  },
  {
    "node_id": 1238,
    "label": 1,
    "text": "Off-policy temporal-difference learning with function approximation We introduce the first algorithm for off-policy temporal-difference learning that is stable with linear function approximation. Off-policy learning is of interest because it forms the basis for popular reinforcement learning methods such as Q-learning, which has been known to diverge with linear function approximation, and because it is critical to the practical utility of multi-scale, multi-goal, learning frameworks such as options, HAMs, and MAXQ. Our new algorithm combines TD(\u03bb) over state\u2013action pairs with importance sampling ideas from our previous work. We prove that, given training under any \u025b-soft policy, the algorithm converges w.p.1 to a close approximation (as in Tsitsiklis and Van Roy, 1997; Tadic, 2001) to the action-value function for an arbitrary target policy. Variations of the algorithm designed to reduce variance introduce additional bias but are also guaranteed convergent. We also illustrate our method empirically on a small policy evaluation problem. Our current results are limited to episodic tasks with episodes of bounded length. 1 Although Q-learning remains the most popular of all reinforcement learning algorithms, it has been known since about 1996 that it is unsound with linear function approximation (see Gordon, 1995; Bertsekas and Tsitsiklis, 1996). The most telling counterexample, due to Baird (1995) is a seven-state Markov decision process with linearly independent feature vectors, for which an exact solution exists, yet 1 This is a re-typeset version of an article published in the Proceedings",
    "neighbors": [
      97,
      820
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1239,
    "label": 5,
    "text": "Experiments on human-robot communication with Robota, an imitative learning and communicating doll robot. Imitation 1 and communication behaviours are important means of interaction between humans and robots. In experiments on robot teaching by demonstration, imitation and communication behaviours can be used by the demonstrator to drive the robot's attention to the demonstrated task. In a children game, they play an important role to engage the interaction between the child and the robot and to stimulate the child's interest. In this work, we study how imitation skills can be used for teaching a robot a symbolic communication system to describe its actions and perceptions. We report on experiments in which we study human-robot interactions using a doll robot. Robota is a robot, whose shape is similar to that of a doll, and which has the capacity to learn, imitate and communicate. Through simple phototaxis behaviour, the robot can imitate (mirror) the arms and head's movements of a demonstrator. The robot is controlled by a Dynamical Recurrent Associative memory Architecture (DRAMA), wh...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1240,
    "label": 2,
    "text": "Analyzing the Effectiveness and Applicability Of Co-Training Recently there has been significant interest in supervised learning algorithms that combine labeled and unlabeled data for text learning tasks. The co-training setting [1] applies to datasets that have a natural separation of their features into two disjoint sets. We demonstrate that when learning from labeled and unlabeled data, algorithms explicitly leveraging a natural independent split of the features outperform algorithms that do not. When a natural split does not exist, co-training algorithms that manufacture a feature split may out-perform algorithms not using a split. These results help explain why co-training algorithms are both discriminative in nature and robust to the assumptions of their embedded classifiers.  Categories and Subject Descriptors  I.2.6 [Artificial Intelligence]: Learning; H.3.3 [Information  Storage and Retrieval]: Information Search and Retrieval---  Information Filtering  Keywords  co-training, expectation-maximization, learning with labeled and unlabeled...",
    "neighbors": [
      609,
      643,
      891,
      1133
    ],
    "mask": "Train"
  },
  {
    "node_id": 1241,
    "label": 5,
    "text": "Distributed Repositories of Highly Expressive Reusable Ontologies We describe an ongoing project to develop technology that will support collaborative construction and effective use of distributed large-scale repositories of highly expressive reusable ontologies. We are focusing on developing a distributed server architecture for ontology construction and use, representation formalisms that remove key barriers to expressing essential knowledge in and about ontologies, ontology construction tools, and tools for obtaining domain models for use in applications from large-scale ontology repositories. We are building on the results of the DARPA Knowledge Sharing Effort, specifically by using the Knowledge Interchange Format (KIF) as a core representation language and the Ontolingua system as a core ontology development environment. In order to enable distributed ontology repositories and services, we are developing a distributed server architecture for ontology construction and use based on ontology servers which provide access via a network API to the contents of ontologies and to information derivable from the contents by a general purpose reasoner. Ontology servers will be analogous to data base servers and will provide services including configuration",
    "neighbors": [
      765
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1242,
    "label": 5,
    "text": "Using Ontological Engineering to Overcome Common AI-ED Problems This paper discusses long-term prospects of AI-ED research with the aim of giving a clear view of what we need for further promotion of the research from both the AI and ED points of view. An analysis of the current status of AI-ED research is done in the light of intelligence, conceptualization, standardization and theory-awareness. Following this, an ontology-based architecture with appropriate ontologies is proposed. Ontological engineering of IS/ID is next discussed followed by a road map towards an ontology-aware authoring system. Heuristic design patterns and XML-based documentation are also discussed.  1. INTRODUCTION  Among AI-ED research done to date, several paradigms such as CAI, ICAI, Micro-world, ITS, ILE, and CSCL have been proposed and many systems have been built within each paradigm. Additionally, innovative computer technologies such as hyper-media, virtual reality, internet, WWW have significantly affected the AI-ED community in general. We really have learned a lot ...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1243,
    "label": 3,
    "text": "Multidimensional Data Modeling for Complex Data Systems for On-Line Analytical Processing (OLAP) considerably ease the process of analyzing business  data and have become widely used in industry. OLAP systems primarily employ multidimensional  data models to structure their data. However, current multidimensional data models fall short in their  ability to model the complex data found in some real-world application domains. The paper presents  nine requirements to multidimensional data models, each of which is exemplified by a real-world, clinical  case study. A survey of the existing models reveals that the requirements not currently met include  support for many-to-many relationships between facts and dimensions, built-in support for handling  change and time, and support for uncertainty as well as different levels of granularity in the data. The  paper defines an extended multidimensional data model, which addresses all nine requirements. Along  with the model, we present an associated algebra, and outline how to implement the model using relational  databases.",
    "neighbors": [
      422
    ],
    "mask": "Train"
  },
  {
    "node_id": 1244,
    "label": 0,
    "text": "Principles of Intention Reconsideration We present a framework that enables a belief-desire-intention (BDI) agent to dynamically choose its intention reconsideration policy in order to perform optimally in accordance with the current state of the environment. Our framework integrates an abstract BDI agent architecture with the decision theoretic model for discrete deliberation scheduling of Russell and Wefald. As intention reconsideration determines an agent's commitment to its plans, this work increases the level of autonomy in agents, as it pushes the choice of commitment level from design-time to run-time. This makes it possible for an agent to operate effectively in dynamic and open environments, whose behaviour is not known at design time. Following a precise formal definition of the framework, we present an empirical analysis that evaluates the run-time policy in comparison with design-time policies. We show that an agent utilising our framework outperforms agents with fixed policies.",
    "neighbors": [
      707
    ],
    "mask": "Train"
  },
  {
    "node_id": 1245,
    "label": 0,
    "text": "Flexible Process Planning And Production Control Using Co-Operative Agent Systems Nowadays, one of the greatest challenges companies have to face is the change towards flexible and demand-driven production. More information has to be handled and a considerable speed-up of development and manufacturing processes is needed. However, the actual situation is characterized by strong borderlines between process planning, production control and scheduling systems, caused by extreme specialisation and independent historical paths of system evolution. This gap implies loss of time and of information. Thus, there is a strong need for innovative concepts for management and control of integrated information logistics, production scheduling and process planning.  Agent-based information technologies like the innovative concept of co-operative agent systems are promising approaches for more flexible and distributed production networks. Agents are autonomously, co-operatively and goal-oriented acting intelligent software units. The use of agents for managing information within production control and process planning makes short term and flexible reaction to unexpected events and disturbances in manufacturing (e.g. break down of machines or lack of other resources as well as unexpected change of market situation) possible. Thus, enterprises will react to changing requirements in a more flexible way and will be able to face the challenges of international competition successfully.  KEYWORDS: Agile Manufacturing, Intelligent Manufacturing, CAPP/CAM, PPC, Operations Management, Distributed Artificial Intelligence, Multiagent Systems",
    "neighbors": [
      1263
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1246,
    "label": 5,
    "text": "Beyond Predictive Accuracy: What? This paper presents  a number of such criteria and discusses the impact they have on meta-level  approaches to model selection",
    "neighbors": [
      442
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1247,
    "label": 2,
    "text": "A Comparison of Techniques to Find Mirrored Hosts on the WWW We compare several algorithms for identifying mirrored hosts on the World Wide Web. The algorithms operate on the basis of URL strings and linkage data: the type of information easily available from web proxies and crawlers. Identification of mirrored hosts can improve web-based information retrieval in several ways: First, by identifying mirrored hosts, search engines can avoid storing and returning duplicate documents. Second, several new information retrieval techniques for the Web make inferences based on the explicit links among hypertext documents -- mirroring perturbs their graph model and degrades performance. Third, mirroring information can be used to redirect users to alternate mirror sites to compensate for various failures, and can thus improve the performance of web browsers and proxies.  # This work was presented at the Workshop on Organizing Web Space at the Fourth ACM Conference on Digital Libraries 1999.  We evaluated 4 classes of \"top-down\" algorithms for detecting ...",
    "neighbors": [
      410,
      774,
      1000,
      1189
    ],
    "mask": "Train"
  },
  {
    "node_id": 1248,
    "label": 0,
    "text": "Architecture-Centric Object-Oriented Design Method for Multi-Agent Systems This paper introduces an architecture-centric object-oriented design method for MAS  (Multi-Agent Systems) using the extended UML (Unified Modeling Language). The  UML extension is based on design principles that are derived from characteristics of  MAS and concept of software architecture which helps to design reusable and wellstructured  multi-agent architecture. The extension allows one to use original objectoriented  method without syntactic or semantic changes which implies the preservation  of OO productivity, i.e., the availability of developers and tools, the utilization of past  experiences and knowledge, and the seamless integration with other systems.  Keywords: multi-agent systems, architecture, object-oriented development methods  1. Introduction  Software agents provide a new way of analyzing, designing, and implementing complex software systems. Currently, agent technology is used in wide variety of applications with range from comparatively small systems such as",
    "neighbors": [
      573,
      941
    ],
    "mask": "Train"
  },
  {
    "node_id": 1249,
    "label": 0,
    "text": "Agents with Complex Plans: Design and Implementation of CASA We describe the design of CASA, an agent specification language that builds on the formal agent specification approach AgentSpeak (L) and extends it by concepts from concurrent logic programming. With CASA it is possible to design agents with complex behavior patterns like speculative computations and parallel executed strategies. The design of multi agent systems composed of CASA agents is supported by providing predefined message structures and integrating an existing agent communication framework.",
    "neighbors": [
      563
    ],
    "mask": "Train"
  },
  {
    "node_id": 1250,
    "label": 3,
    "text": "Bottom-up Partial Deduction of Logic Programs In this paper, we develop a solid theoretical foundation for a bottom-up program transformation, capable of specialising a logic program with respect to a set of unit clauses. Extending a well-known operator, we define a bottom-up partial deduction operator and prove correctness of the transformation with respect to the S-semantics. We also show how, within this framework, a concrete control strategy can be designed. The transformation can be used as a stand-alone specialisation technique, useful when a program needs to be specialised with respect to its internal structure (e.g., a library of predicates with respect to an abstract data type) instead of a single goal. Moreover, the bottom-up transformation can be usefully combined with a more traditional top-down partial deduction strategy.",
    "neighbors": [
      310
    ],
    "mask": "Train"
  },
  {
    "node_id": 1251,
    "label": 0,
    "text": "Conflict Resolution in Collaborative Planning Dialogues In a collaborative planning environment in which the agents are autonomous and heterogeneous, it is  inevitable that discrepancies in the agents' beliefs result in conflicts during the planning process. In such  cases, it is important that the agents engage in collaborative negotiation to resolve the detected conflicts  in order to determine what should constitute their shared plan of actions and shared beliefs. This paper  presents a plan-based model for conflict detection and resolution in collaborative planning dialogues.  Our model specifies how a collaborative system should detect conflicts that arise between the system  and its user during the planning process. If the detected conflicts warrant resolution, our model initiates  collaborative negotiation in an attempt to resolve the conflicts in the agent's beliefs. In addition, when  multiple conflicts arise, our model identifies and addresses the most effective aspect in its pursuit of  conflict resolution. Furthermore, by captur...",
    "neighbors": [
      110
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1252,
    "label": 4,
    "text": "The Augurscope: A Mixed Reality Interface for Outdoors The augurscope is a portable mixed reality interface for outdoors. A tripod-mounted display is wheeled to different locations and rotated and tilted to view a virtual environment that is aligned with the physical background. Video from an onboard camera is embedded into this virtual environment. Our design encompasses physical form, interaction and the combination of a GPS receiver, electronic compass, accelerometer and rotary encoder for tracking. An initial application involves the public exploring a medieval castle from the site of its modern replacement. Analysis of use reveals problems with lighting, movement and relating virtual and physical viewpoints, and shows how environmental factors and physical form affect interaction. We suggest that problems might be accommodated by carefully constructing virtual and physical content.",
    "neighbors": [
      130
    ],
    "mask": "Test"
  },
  {
    "node_id": 1253,
    "label": 2,
    "text": "Towards a Highly-Scalable and Effective Metasearch Engine A metasearch engine is a system that supports unified access to multiple local search engines. Database selection is one of the main challenges in building a large-scale metasearch engine. The problem is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query. In order to enable accurate selection, metadata that reect the contents of each search engine need to be collected and used. In this paper, we propose a highly scalable and accurate database selection method. This method has several novel features. First, the metadata for representing the contents of all search engines are organized into a single integrated representative. Such a representative yields both computation efficiency and storage efficiency. Second, our selection method is based on a theory for ranking search engines optimally. Experimental results indicate that this new method is very effective. An operational prototype system has been built based on the proposed approach.",
    "neighbors": [
      224,
      241,
      271,
      510,
      696,
      792,
      931,
      1003,
      1134
    ],
    "mask": "Train"
  },
  {
    "node_id": 1254,
    "label": 2,
    "text": "Algorithmics and Applications of Tree and Graph Searching Modern search engines answer keyword-based queries extremely efficiently. The impressive speed is due to clever inverted index structures, caching, a domain-independent knowledge of strings, and thousands of machines. Several research efforts have attempted to generalize keyword search to keytree and keygraph searching, because trees and graphs have many applications in next-generation database systems. This paper surveys both algorithms and applications, giving some emphasis to our own work.",
    "neighbors": [
      585,
      634,
      681,
      752,
      1069,
      1201
    ],
    "mask": "Train"
  },
  {
    "node_id": 1255,
    "label": 3,
    "text": "A Sequence-Based Object-Oriented Model for Video Databases Structuration, annotation and composition, are amidst the most crucial modeling issues that video editing and querying in the context of a database entail. In this paper, we propose a sequence-based, object-oriented data model that addresses them in an unified, yet orthogonal way. This orthogonality allows to capture the interactions between these three aspects, i.e. annotations may be attached to any level of video structuration, and the composition operators preserve the structurations and annotations of the argument videos. The proposed model reuses concepts stemming from temporal databases, so that operators defined in this latter setting may be used to query it. Accordingly, the query language for video databases proposed in this paper, is a variant of a temporal extension of ODMG's OQL. The main components of the proposal have been formalized and implemented on top of an object-oriented DBMS.  Keywords: video databases, sequence databases, object-oriented databases, ODMG.  R'esum...",
    "neighbors": [
      1058
    ],
    "mask": "Train"
  },
  {
    "node_id": 1256,
    "label": 5,
    "text": "Multi-scale EM-ICP: A Fast and Robust Approach for Surface Registration We investigate in this article the rigid registration of large sets of points, generally sampled from surfaces. We formulate this problem as a general Maximum-Likelihood (ML) estimation of the transformation and the matches. We show that, in the specific case of a Gaussian noise, it corresponds to the Iterative Closest Point algorithm (ICP) with the Mahalanobis distance.",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1257,
    "label": 5,
    "text": "Neurules: Improving the Performance of Symbolic Rules In this paper, we present a method for improving the performance of classical symbolic rules.",
    "neighbors": [
      66,
      1110
    ],
    "mask": "Test"
  },
  {
    "node_id": 1258,
    "label": 5,
    "text": "Reformulating Temporal Plans For Efficient Execution The Simple Temporal Network formalism permits significant flexibility in specifying the occurrence time of events in temporal plans. However, to retain this flexibility during execution, there is a need to propagate the actual execution times of past events so that the occurrence windows of future events are adjusted appropriately. Unfortunately, this may run afoul of tight real-time control requirements that dictate extreme efficiency. The performance may be improved by restricting the propagation. However, a fast, locally propagating, execution controller may incorrectly execute a consistent plan. To resolve this dilemma, we identify a class of dispatchable networks that are guaranteed to execute correctly under local propagation. We show that every consistent temporal plan can be reformulated as an equivalent dispatchable network, and we present an algorithm that constructs such a network. Moreover, the constructed network is shown to have a minimum number of edges among all such n...",
    "neighbors": [
      871
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1259,
    "label": 1,
    "text": "Case-Based Representation and Learning of Pattern Languages Pattern languages seem to suit case-based reasoning particularly well. Therefore, the problem of inductively learning pattern languages is paraphrased in a case-based manner. A careful investigation requires a formal semantics for case bases together with similarity measures in terms of formal languages. Two basic semantics are introduced and investigated. It turns out that representability problems are major obstacles for case-based learnability. Restricting the attention to so-called proper patterns avoids these representability problems. A couple of learnability results for proper pattern languages are derived both for case-based learning from only positive data and for case-based learning from positive and negative data. Under the so-called competing semantics, we show that the learnability result for positive and negative data can be lifted to the general case of arbitrary patterns. Learning under the standard semantics from positive data is closely related to monotonic language l...",
    "neighbors": [
      292,
      566,
      776,
      935,
      975,
      1154,
      1268
    ],
    "mask": "Train"
  },
  {
    "node_id": 1260,
    "label": 5,
    "text": "Combining Evolutionary Algorithms With Oblique Decision Trees to Detect Bent-Double Galaxies Decision trees have long been popular in classification as they use simple and easy-to-understand tests at each node. Most variants of decision trees test a single attribute at a node, leading to axis-parallel trees, where the test results in a hyperplane which is parallel to one of the dimensions in the attribute space. These trees can be rather large and inaccurate in cases where the concept to be learned is best approximated by oblique hyperplanes. In such cases, it may be more appropriate to use an oblique decision tree, where the decision at each node is a linear combination of the attributes. Oblique decision trees have not gained wide popularity in part due to the complexity of constructing good oblique splits and the tendency of existing splitting algorithms to get stuck in local minima. Several alternatives have been proposed to handle these problems including randomization in conjunction with deterministic hill-climbing and the use of simulated annealing. In this paper, we use...",
    "neighbors": [
      1262
    ],
    "mask": "Train"
  },
  {
    "node_id": 1261,
    "label": 3,
    "text": "Top-Down Induction Of First Order Logical Decision Trees this paper. For instance, the Progol system (Muggleton, 1995) has recently been extended with caching and other efficiency improvements (Cussens, 1997). Another direction of work is the use of sampling techniques, see e.g. (Srinivasan, 1998; Sebag, 1998).  168 CHAPTER 7. SCALING UP Tilde",
    "neighbors": [
      158
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1262,
    "label": 5,
    "text": "Using Evolutionary Algorithms to Induce Oblique Decision Trees This paper illustrates the application of evolutionary algorithms (EAs) to the problem of oblique decision tree induction. The objectives are to demonstrate that EAs can find classifiers whose accuracy is competitive with other oblique tree construction methods, and that at least in some cases this can be accomplished in a shorter time. Experiments were performed with a (1+1) evolution strategy and a simple genetic algorithm on public domain and artificial data sets. The empirical results suggest that the EAs quickly find competitive classifiers, and that EAs scale up better than traditional methods to the dimensionality of the domain and the number of instances used in training.",
    "neighbors": [
      1260
    ],
    "mask": "Train"
  },
  {
    "node_id": 1263,
    "label": 0,
    "text": "Synthesis And Adaptation Of Multiagent Communication Protocols In The Production Engineering Domain The application of multiagent systems is often based on the claim that there will be an emergent behavior within these systems. To reach the emergent behavior many researcher propagate to plan it within design and analysis of specific systems, as it will not occur by chance.  We are presenting a new way to adaptive agent communication protocols with respect to a possible gain of emergent behavior. Communication protocols can be generated, refined or adapted by the agents autonomously. The approach uses basic concepts of machine learning.",
    "neighbors": [
      1245
    ],
    "mask": "Validation"
  },
  {
    "node_id": 1264,
    "label": 2,
    "text": "MySpiders : Evolve your own intelligent Web crawlers Abstract. The dynamic nature of the World Wide Web makes it a challenge to find information that is bothrelevant and recent. Intelligent agents can complement the power of searchengines to meet this challenge. We present a Web tool called MySpiders, which implements an evolutionary algorithm managing a population of adaptive crawlers who browse the Web autonomously. Each agent acts as an intelligent client on behalf of the user, driven by a user query and by textual and linkage clues in the crawled pages. Agents autonomously decide which links to follow, which clues to internalize, when to spawn offspring to focus the search near a relevant source, and when to starve. The tool is available to the public as a threaded Java applet. We discuss the development and deployment of such a system. Keywords: web informational retrieval, topic-driver crawlers, online search, InfoSpiders, MySpiders, applet",
    "neighbors": [
      1,
      281,
      314,
      587,
      649,
      662,
      1005
    ],
    "mask": "Train"
  },
  {
    "node_id": 1265,
    "label": 1,
    "text": "Evolving More Efficient Digital Circuits By Allowing Circuit Layout Evolution and Multi-Objective Fitness We use evolutionary search to design combinational logic circuits. The technique is based on evolving the functionality and connectivity of a rectangular array of logic cells whose dimension is defined by the circuit layout. The main idea of this approach is to improve quality of the circuits evolved by the genetic algorithm (GA) by reducing the number of active gates used. We accomplish this by combining two ideas: 1) using multiobjective fitness function; 2) evolving circuit layout. It will be shown that using these two approaches allows us to increase the quality of evolved circuits. The circuits are evolved in two phases. Initially the genome fitness in given by the percentage of output bits that are correct. Once 100% functional circuits have been evolved, the number of gates actually used in the circuit is taken into account in the fitness function. This allows us to evolve circuits with 100% functionality and minimise the number of active gates in circuit structure. The populati...",
    "neighbors": [
      702
    ],
    "mask": "Train"
  },
  {
    "node_id": 1266,
    "label": 0,
    "text": "The RoboCup Synthetic Agent Challenge 97 RoboCup Challenge offers a set of challenges for intelligent agent researchers using a friendly competition in a dynamic, real-time, multiagent domain. While RoboCup in general envisions longer range challenges over the next few decades, RoboCup Challenge presents three specific challenges for the next two years: (i) learning of individual agents and teams; (ii) multi-agent team planning and plan-execution in service of teamwork; and (iii) opponent modeling. RoboCup Challenge provides a novel opportunity for machine learning, planning, and multi-agent researchers --- it not only supplies a concrete domain to evalute their techniques, but also challenges researchers to evolve these techniques to face key constraints fundamental to this domain: real-time, uncertainty, and teamwork.  1 Introduction  RoboCup (The World Cup Robot Soccer) is an attempt to promote AI and robotics research by providing a common task, Soccer, for evaluation of various theories, algorithms, and agent architectur...",
    "neighbors": [
      127,
      155,
      250,
      356,
      394,
      724,
      839,
      927
    ],
    "mask": "Train"
  },
  {
    "node_id": 1267,
    "label": 0,
    "text": "Eye communication in a conversational 3D synthetic agent this paper, we concentrate on the study and generation of coordinated linguistic and gaze communicative acts. In this view we analyse gaze signals according to their functional meaning rather than to their physical actions. We propose a formalism where a communicative act is represented by two elements: a meaning (that corresponds to a set of goals and beliefs that the agent has the purpose to transmit to the interlocutor) and a signal, that is the nonverbal expression of that meaning. We also outline a methodology to generate messages that coordinate verbal with nonverbal signals.",
    "neighbors": [
      265,
      1191
    ],
    "mask": "Train"
  },
  {
    "node_id": 1268,
    "label": 1,
    "text": "On Case-Based Learnability of Languages Case-based reasoning is deemed an important technology to alleviate the bottleneck  of knowledge acquisition in Artificial Intelligence (AI). In case-based reasoning,  knowledge is represented in the form of particular cases with an appropriate similarity  measure rather than any form of rules. The case-based reasoning paradigm adopts the  view that an AI system is dynamically changing during its life-cycle which immediately  leads to learning considerations.  Within the present paper, we investigate the problem of case-based learning of  indexable classes of formal languages. Prior to learning considerations, we study the  problem of case-based representability and show that every indexable class is case-based  representable with respect to a fixed similarity measure. Next, we investigate several  models of case-based learning and systematically analyze their strengths as well as  their limitations. Finally, the general approach to case-based learnability of indexable  classes of form...",
    "neighbors": [
      473,
      935,
      975,
      1154,
      1259
    ],
    "mask": "Train"
  },
  {
    "node_id": 1269,
    "label": 0,
    "text": "Towards socially sophisticated BDI agents We present an approach to social reasoning that integrates prior work on norms and obligations with the BDI approach to agent architectures. Norms and obligations can be used to increase the eficiency of agent reasoning, and their explicit representation supports reasoning about a wide range of behaviour types in a single framework. We propose a modified BDI interpreter loop that takes norms and obligations into account in an agent's deliberation.",
    "neighbors": [
      636,
      703,
      964
    ],
    "mask": "Test"
  },
  {
    "node_id": 1270,
    "label": 3,
    "text": "VideoGraph: A Graphical Object-based Model for Representing and Querying Video Data . Modeling video data poses a great challenge since they do  not have as clear an underlying structure as traditional databases do.  We propose a graphical object-based model, called VideoGraph, in this  paper. This scheme has the following advantages: (1) In addition to semantics  of video individual events, we capture their temporal relationships  as well. (2) The inter-event relationships allow us to deduce implicit  video information. (3) Uncertainty can also be handled by associating the  video event with a temporal Boolean-like expression. This also allows us  to exploit incomplete information. The above features make VideoGraph  very flexible in representing various metadata types extracted from diverse  information sources. To facilitate video retrieval, we also introduce  a formalism for the query language based on path expressions. Query  processing involves only simple traversal of the video graphs.  1 Introduction  We deal with the modeling aspect of video database manageme...",
    "neighbors": [],
    "mask": "Train"
  },
  {
    "node_id": 1271,
    "label": 2,
    "text": "Management of XML Documents in an Integrated Digital Library We describe a generalized toolset developed by the Perseus Project to  manage XML documents in the context of a large, heterogeneous digital  library. The system manages multiple DTDs through mappings from elements  in the DTD to abstract document structures. The abstraction of  document metadata, both structural and descriptive, facilitates the development  of application-level tools for knowledge management and document  presentation. We discuss the implementation of the XML back  end and describe applications for cross citation retrieval, toponym extraction  and plotting, automatic hypertext generation, morphology, and word  co-occurrence.  1",
    "neighbors": [],
    "mask": "Test"
  },
  {
    "node_id": 1272,
    "label": 3,
    "text": "Capacity-Augmenting Schema Changes on Object-Oriented Databases: Towards Increased Interoperability The realization of capacity-augmenting schema changes on a shared database while providing continued interoperability to active applications has been recognized as a hard open problem. A novel three-pronged process, called transparent object schema evolution (TOSE), is presented that successfully addresses this problem. TOSE uses the combination of views and versioning to simulate schema changes requested by one application without affecting other applications interoperating on a shared OODB. The approach is of high practical relevance as it builds upon schema evolution support offered by commercial OODBMSs. Keywords: Transparent schema evolution, object-oriented views, object-oriented databases, application migration. 1 Introduction  Current schema evolution technology suffers from the problem that schema updates on a database shared by interoperating applications often have catastrophic consequences [BKKK87, KC88, MS93, PS87, TS93, Zic91]. In such a multi-user environment, a schema c...",
    "neighbors": [],
    "mask": "Validation"
  },
  {
    "node_id": 1273,
    "label": 4,
    "text": "Patterns as Tools for User Interface Design . Designing usable systems is difficult and designers need effective  tools that are usable themselves. Effective design tools should be based on  proven knowledge of design. Capturing knowledge about the successful design  of usable systems is important for both novice and experienced designers and  traditionally, this knowledge has largely been described in guidelines. However,  guidelines have shown to have problems concerning selection, validity and applicability.  Patterns have emerged as a possible solution to some of the problems  from which guidelines suffer. Patterns focus on the context of a problem and solution  thereby guiding the designer in using the design knowledge. Patterns for  architecture or software engineering are not identical in structure and user interface  design also requires its own structure for patterns, focusing on usability.  This paper explores how patterns for user interface design must be structured in  order to be effective and usable tools for desig...",
    "neighbors": [
      287
    ],
    "mask": "Train"
  }
]