import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline

# =========================
# 1. Set the model environment variables and model name
# =========================
cache_dir = "/home/ai/EnjunDu/model"  # Model cache directory
os.environ["HF_HOME"] = cache_dir
os.environ["CUDA_VISIBLE_DEVICES"] = "2"

model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"  # target model name

# =========================
# 2. Read cora.json
# =========================
with open('./data/cora.json', 'r', encoding='utf-8') as file:
    dataset = json.load(file)  # Read JSON data and store all nodes in list form

# =========================
# 3. Initialize the model and Tokenizer
# =========================
tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    cache_dir=cache_dir,
    device_map="auto",       # Automatically assign to GPU
    torch_dtype=torch.float16 # Half-precision calculations for large models
)
text_generation = TextGenerationPipeline(model=model, tokenizer=tokenizer)

# =========================
# 4. Generate Text Compression Prompt (English)
# =========================
def generate_text_summarization_prompt(text):
    """
    Generate the prompt required by LLM, requiring the model to compress the text to less than 35 words.
    """
    prompt = f"""
You are an AI assistant specializing in text summarization.
Your task is to compress the following academic text while preserving its core meaning.
Ensure that the summary is concise and contains no more than 30 words.
Please think step by step.

Original text:
{text}

Summarized text (30 words or fewer):
"""
    return prompt

# =========================
# 5. Process a single node text (return only the summary of the model output, excluding the prompt part)
# =========================
def compress_text(text):
    """
    Use DeepSeek-R1 32B to compress the text and limit it to 35 words.
    Extract the content after the "Summarized text (35 words or fewer):" tag in the generated text as the summary.
    """
    prompt = generate_text_summarization_prompt(text)

    # Call LLM to generate compressed text
    output = text_generation(
        prompt,
        max_new_tokens=45,  # Generate up to 50 tokens (about 35 words)
        do_sample=False,     # Turn off random sampling to ensure consistency
        temperature=0.0      # Reduce creativity and ensure the abstract is precise
    )

    # Get the full text generated
    generated_text = output[0]["generated_text"].strip()

    # Extract the summary part of the model output: look for the content after "Summarized text (35 words or fewer):"
    marker = "Summarized text (30 words or fewer):"
    if marker in generated_text:
        # After splitting, take the last part as the summary
        summary = generated_text.split(marker)[-1].strip()
    else:
        # If the marker is not found, the entire content is returned directly (not recommended)
        summary = generated_text

    # Filter out the "</think>" tag that may be generated by the model and the whitespace characters before and after it
    summary = summary.replace("</think>", "").strip()

    return summary

# =========================
# 6. Process the entire data set
# =========================
enhanced_dataset = []  # Storing processed data

for i, node in enumerate(dataset):
    print(f"Processing node {i + 1}/{len(dataset)} (ID: {node['node_id']})...")

    # Generate compressed text (only keep the summary part of the model output)
    compressed_text = compress_text(node["text"])

    # Construct an enhanced node that only saves the summary generated by the model
    enhanced_node = {
        "node_id": node["node_id"],
        "label": node["label"],
        "text": compressed_text,  # Save only the summary generated by the model
        "neighbors": node["neighbors"],
        "mask": node["mask"]
    }

    enhanced_dataset.append(enhanced_node)

# =========================
# 7. Save to cora_enhanced.json
# =========================
output_path = "./data/cora_enhanced_30.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(enhanced_dataset, f, ensure_ascii=False, indent=2)

print(f"Processing complete! Enhanced dataset saved to {output_path}")